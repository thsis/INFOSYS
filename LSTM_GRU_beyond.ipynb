{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Crime Prediction with Recurrent Neural Networks #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    " - Problem Setting and the Data\n",
    " - RNN\n",
    " - LSTM\n",
    " - GRU\n",
    " - Implementation of RNN, LSTM and GRU\n",
    " - Parameter Tuning and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- data table: features (viele features, nehmen nur crime, district, time)\n",
    "- Chicago Map interactively\n",
    "- Chicago whole and Districts\n",
    "- problemstellung\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN, LSTM and GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Starting off with Artificial Neural Networks #\n",
    "<img src=\"presentation_pics/neural_network1.png\" alt=\"NN\" style=\"width: 400px;\"/>\n",
    "[Source:  Alisa's Presentation on NN Primer]\n",
    "\n",
    "# Why are Neural Networks maybe not that good for predicting time series e.g. determine later actions in a movie or sentence?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about Long-Term Dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "# RNN - Loop Structure #\n",
    "<img src=\"presentation_pics/RNN_loop.png\" alt=\"RNN\" style=\"width: 300px;\"/>\n",
    "\n",
    "# RNN - Unroll Loop #\n",
    "<img src=\"presentation_pics/RNN.png\" alt=\"RNN\" style=\"width: 800px;\"/>\n",
    "\n",
    "[Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why not use RNN for further implementation?\n",
    "- issue for long-term dependencies: back to example \n",
    "# “In France, I had a great time and I learnt some of the _____ language.”\n",
    " \n",
    "- cannot connect information anymore\n",
    "- \"vanishing gradient problem\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing Gradient problem\n",
    "- backpropagation through time\n",
    "- as gap between timesteps becomes bigger, product longer and we are multiplying very small numbers (small gradients)\n",
    "- due to activation function (tanh)\n",
    "- some crucial previous timesteps do not influence anymore in later timesteps: gradient vanishes...\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial J_2}{\\partial W}=\\frac{\\partial J_2}{\\partial y_2}...\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"presentation_pics/vanishing_gradient.png\" alt=\"vanishing_gradient\" style=\"width: 600px;\"/>\n",
    "[Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LSTM Network - Long Short-Term Memory Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LSTM\n",
    "<img src=\"presentation_pics/LSTM.png\" alt=\"LSTM\" style=\"width: 800px;\"/>\n",
    "[Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GRU Network - Gated Recurrent Unit##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gated Cells (LSTM/GRU) can keep track of information throughout the timeseries\n",
    "- consider output of previous timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GRU\n",
    "<img src=\"presentation_pics/GRU.png\" alt=\"GRU\" style=\"width: 800px;\"/>\n",
    "\n",
    "[Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
