{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation on LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define activation functions\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(x):\n",
    "    \"\"\"Derivative of sigmoid function.\"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "def dtanh(x):\n",
    "    \"\"\"Derivative of tanh function.\"\"\"\n",
    "    return 1 - np.tanh(x)**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward pass\n",
    "$loss = ||y-\\hat{y}||^2$\n",
    "\n",
    "$\\hat{y}_t = w^\\prime h_t$, where $w$ is an $n$-dimensional vector of weights\n",
    "\n",
    "$h_t = o_t * g(c_t)$, where $*$ denotes element-wise multiplication\n",
    "\n",
    "$c_t = f_t * c_{t-1} + i_t * \\tilde{c_t}$\n",
    "\n",
    "$\\tilde{c}_t = g(W_c x_t + U_c h_{t-1})$\n",
    "\n",
    "and for the gates:\n",
    "\\begin{align}\n",
    "o_t &= \\sigma\\left( W_{o} x_t + U_{o} h_{t-1} \\right)\\\\\n",
    "f_t &= \\sigma\\left( W_{f} x_t + U_{f} h_{t-1} \\right)\\\\\n",
    "i_t &= \\sigma\\left( W_{i} x_t + U_{i} h_{t-1} \\right)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sample\n",
    "x = np.array([0.2, 0.3, 0.4])\n",
    "y = 7.0\n",
    "\n",
    "# Initialize Weights\n",
    "Wc = np.array([0.2, 0.4])\n",
    "\n",
    "Wo = np.array([0.1, 3.1])\n",
    "Wf = np.array([2.3, 0.2])\n",
    "Wi = np.array([3.1, 0.1])\n",
    "\n",
    "Uc = np.array([[1.8, 3.6], [4.7, 2.9]])\n",
    "Uo = np.array([[0.1, 0.9], [0.7, 4.3]])\n",
    "Uf = np.array([[3.6, 4.1], [1.0, 0.9]])\n",
    "Ui = np.array([[1.5, 2.6], [2.1, 0.2]])\n",
    "\n",
    "w = np.array([2.0, 4.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "h = [np.zeros_like(Wc)]\n",
    "o = []\n",
    "f = []\n",
    "i = []\n",
    "c_ = []\n",
    "c = [np.zeros_like(Wc)]\n",
    "y_ = []\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    # Calculate values of gates\n",
    "    ot = sigmoid(Wo.dot(xt) + Uo.dot(h[t]))\n",
    "    ft = sigmoid(Wf.dot(xt) + Uf.dot(h[t]))\n",
    "    it = sigmoid(Wi.dot(xt) + Ui.dot(h[t]))\n",
    "\n",
    "    # Calculate candidate update\n",
    "    c_t = tanh(Wc.dot(xt) + Uc.dot(h[t]))\n",
    "    ct = ft * c[t] + it * c_t\n",
    "\n",
    "    # Calculate cell state\n",
    "    ht = ot * tanh(ct)\n",
    "\n",
    "    # Prediction at step t\n",
    "    y_t = w.dot(ht)\n",
    "\n",
    "    # Save variables to container\n",
    "    h.append(ht)\n",
    "    o.append(ot)\n",
    "    f.append(ft)\n",
    "    i.append(it)\n",
    "    c_.append(c_t)\n",
    "    c.append(ct)\n",
    "    y_.append(y_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backward pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the following gradients\n",
    "\n",
    "\n",
    "* candidate update: $\\frac{\\partial loss}{\\partial W_c}$ & $\\frac{\\partial loss}{\\partial U_c}$\n",
    "* output gate update: $\\frac{\\partial loss}{\\partial W_o}$ & $\\frac{\\partial loss}{\\partial U_o}$\n",
    "* forget gate update:$ \\frac{\\partial loss}{\\partial W_f} $ & $ \\frac{\\partial loss}{\\partial U_f}$\n",
    "* input gate update:$ \\frac{\\partial loss}{\\partial W_i}$ & $\\frac{\\partial loss}{\\partial U_i}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for the output gate:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial {loss}}{\\partial W_o} &= \\sum\\frac{\\partial {loss}_t}{\\partial \\hat{y}_t} \\frac{\\partial \\hat{y}_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial o_t} \\frac{\\partial o_t}{\\partial W_o}\\\\\n",
    "                                   &= \\sum(y - \\hat{y}_t)\\ w * g(c_t) * \\sigma^\\prime \\left( W_o x_t + U_o h_{t-1} \\right)\\ x_t\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdWo = np.zeros_like(Wo)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dotdWo = dsigmoid(Wo.dot(xt) + Uo.dot(h[t])) * xt\n",
    "    # Note that c = (c0, c1, c2, c3), thus we need to index it differently\n",
    "    dLossdWo += (y-y_[t]) * w * tanh(c[t+1]) * dotdWo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the gradient of the loss function with respect to $U_o$ we get\n",
    "\n",
    "$$\n",
    "\\frac{\\partial loss}{\\partial U_o} = \\sum\\frac{\\partial {loss}_t}{\\partial \\hat{y}_t} \\frac{\\partial \\hat{y}_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial o_t} \\frac{\\partial o_t}{\\partial U_o}.\n",
    "$$\n",
    "\n",
    "Here we need to make a distiction: the first part is the same as before $\\frac{\\partial {loss}_t}{\\partial \\hat{y}_t} \\frac{\\partial \\hat{y}_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial o_t}$ which in our example is of dimension $2 \\times 1$. However, the last part is of dimension $2 \\times 2$ since it is the Jacobian of \n",
    "\n",
    "$$\n",
    "U_o h_{t-1} = \\begin{bmatrix} u_{11} h_1 + u_{12} h_2 \\\\ u_{21} h_1 + u_{22} h_2 \\end{bmatrix} $$\n",
    "which is given by \n",
    "$$\n",
    "\\frac{\\partial U_o h_{t-1}}{\\partial U_o} = \\begin{bmatrix} h_1 & h_2 \\\\ h_1 & h_2 \\end{bmatrix} = \\begin{bmatrix} h^T_{t-1} \\\\ h^T_{t-1} \\end{bmatrix}, \\text{ where } h_{t-1} = \\begin{bmatrix}h_1 \\\\ h_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In `numpy` this is not a problem, just a simple broadcasting operation. In mathematical notation we need to insert a fitting diagonal matrix in order to multiply the first row of $\\frac{\\partial {loss}_t}{\\partial \\hat{y}_t} \\frac{\\partial \\hat{y}_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial o_t}$ with all entries of $\\frac{\\partial U_o h_{t-1}}{\\partial U_o}$'s first row and the second row of the vector with all elements in the matrix' second row.\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial U_o} &= \\sum\\frac{\\partial {loss}_t}{\\partial \\hat{y}_t} \\frac{\\partial \\hat{y}_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial o_t} \\frac{\\partial o_t} {\\partial U_o} \\\\\n",
    "                                   &= \\sum(y_t-\\hat{y}_t)\\ \\textbf{diag}\\left[ w * g(c_t) * \\sigma^\\prime \\left(W_o x_t + U_o h_{t-1} \\right) \\right] \\cdot \\begin{bmatrix} h^T_{t-1} \\\\ h^T_{t-1} \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "where $*$ denotes element-wise multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdUo = np.zeros_like(Uo)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dy_dot = w * tanh(c[t+1]) * dsigmoid(Wo.dot(xt) + Uo.dot(h[t]))\n",
    "    # Expand so that the above multiplication can be performed element-wise\n",
    "    dLossdUo += (y-y_[t]) * dy_dot.reshape(-1, 1) * h[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for the input gate\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial W_i} &= \\sum\\frac{\\partial loss_t}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial c_t} \\frac{\\partial c_t}{\\partial i_t}\\frac{\\partial i_t}{\\partial W_i} \\\\\n",
    "                                   &= \\sum(y-\\hat{y}_t)\\ w * \\left[o_t * g^\\prime(c_t)\\right] * \\tilde{c}_t* \\sigma^\\prime \\left( W_i x_t + U_i h_{t-1} \\right)\\ x_t\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdWi = np.zeros_like(Wi)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    ditdWi = dsigmoid(Wi.dot(xt) + Ui.dot(h[t])) * xt\n",
    "    dLossdWi += (y-y_[t]) * w * (o[t] * dtanh(c[t+1])) * c_[t] * ditdWi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial U_i} &= \\sum\\frac{\\partial loss_t}{\\partial \\hat{y}_t} \\frac{\\partial \\hat{y}_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial c_t} \\frac{\\partial c_t}{\\partial i_t} \\frac{\\partial i_t}{\\partial U_i} \\\\\n",
    "                                   &= \\sum(y-\\hat{y}_t)\\ \\textbf{diag} \\left[w * [ o_t * g^\\prime (c_t)] * \\tilde{c}_t * \\sigma^\\prime \\left( W_i x_t + U_i h_{t-1} \\right)\\right] \\cdot \\begin{bmatrix}h^T_{t-1} \\\\ h^T_{t-1}\\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdUi = np.zeros_like(Ui)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dy_dit = w * (o[t]*dtanh(c[t])) * c_[t] * dsigmoid(Wi.dot(xt)+Ui.dot(h[t]))\n",
    "    dLossdUi += (y-y_[t]) * dy_dit.reshape(-1, 1) * h[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for the forget gate\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial W_f} &= \\sum\\frac{\\partial loss_t}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial c_t} \\frac{\\partial c_t}{\\partial f_t}\\frac{\\partial f_t}{\\partial W_f} \\\\\n",
    "                                   &= \\sum(y-\\hat{y}_t) w * \\left[o_t * g^\\prime(c_t)\\right] * c_{t-1} * \\sigma^\\prime \\left( W_f x_t + U_f h_{t-1} \\right) x_t\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdWf = np.zeros_like(Wf)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dftdWf = dsigmoid(Wf.dot(xt) + Uf.dot(h[t])) * xt\n",
    "    dLossdWf += (y-y_[t]) * w * (o[t] * dtanh(c[t+1])) * c[t] * dftdWf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial U_f} &= \\sum\\frac{\\partial loss_t}{\\partial \\hat{y}_t} \\frac{\\partial \\hat{y}_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial c_t} \\frac{\\partial c_t}{\\partial f_t} \\frac{\\partial f_t}{\\partial U_f} \\\\\n",
    "                                   &= \\sum(y-\\hat{y}_t)\\ \\textbf{diag} \\left[w * [ o_t * g^\\prime (c_t)] * c_{t-1} * \\sigma^\\prime \\left( W_f x_t + U_f h_{t-1} \\right)\\right] \\cdot \\begin{bmatrix}h^T_{t-1} \\\\ h^T_{t-1}\\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdUf = np.zeros_like(Uf)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dy_dft = w*(o[t]*dtanh(c[t+1])) * c[t] * dsigmoid(Wf.dot(xt)+Uf.dot(h[t]))\n",
    "    dLossdUf += (y-y_[t]) * dy_dft.reshape(-1, 1) * h[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for the cell state\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial W_c} &= \\sum\\frac{\\partial loss_t}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial c_t}\\frac{\\partial c_t}{\\partial \\tilde{c}_t} \\frac{\\partial \\tilde{c}_t}{\\partial W_c} \\\\\n",
    "                                   &= \\sum(y - \\hat{y}_t)\\ w * [o_t * g^\\prime(c_t)] * i_t * g^\\prime(W_c x_t + U_c h_{t-1})\\ x_t\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdWc = np.zeros_like(Wc)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dc_tdWc = dtanh(Wc.dot(xt) + Uc.dot(h[t])) * xt\n",
    "    dLossdWc += (y-y_[t]) * w * (o[t] * dtanh(c[t+1])) * i[t] * dc_tdWc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial U_c} &= \\sum\\frac{\\partial loss_t}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial c_t}\\frac{\\partial c_t}{\\partial \\tilde{c}_t}\\frac{\\partial \\tilde{c}_t}{\\partial U_c} \\\\\n",
    "                          &= \\sum(y_t - \\hat{y}_t)\\ \\textbf{diag} \\left[w * [o_t * g^\\prime(c_t)] * i_t * g^\\prime(W_c x_t + U_c h_{t-1})\\right] \\begin{bmatrix} h^T_{t-1} \\\\ h^T_{t-1} \\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdUc = np.zeros_like(Uc)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dc_tdf = w * (o[t] * dtanh(c[t+1])) * i[t] * dtanh(Wc.dot(xt)+Uc.dot(h[t]))\n",
    "    dLossdUc += (y-y_[t]) * dc_tdf.reshape(-1, 1) * h[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights of the outer layer\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial w} &= \\sum\\frac{\\partial loss_t}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial w} \\\\\n",
    "                                 &= \\sum (y-\\hat{y}_t) h_t \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdw = np.zeros_like(w)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dLossdw += (y - y_[t]) * h[t+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.01\n",
    "\n",
    "Wc -= eta * dLossdWc\n",
    "Wo -= eta * dLossdWo\n",
    "Wf -= eta * dLossdWf\n",
    "Wi -= eta * dLossdWi\n",
    "\n",
    "Uc -= eta * dLossdUc\n",
    "Uo -= eta * dLossdUo\n",
    "Uf -= eta * dLossdUf\n",
    "Ui -= eta * dLossdUi\n",
    "\n",
    "w -= eta * dLossdw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
