{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation on GRU\n",
    "\n",
    "$\n",
    "% Vector shortcuts\n",
    "\\newcommand{\\Htil}{\\color{green}{\\tilde{h}_t}}\n",
    "\\newcommand{\\H}{\\color{green}{h_{t-1}}}\n",
    "\\newcommand{\\R}{\\color{blue}{r_t}}\n",
    "\\newcommand{\\Z}{\\color{red}{z_t}}\n",
    "% Gates Matrix Notation\n",
    "\\newcommand{\\Hfull}[1][]{\\color{green}{g^{#1}\\left( W_h x_t + U_h \\cdot ( \\R * \\H)\\right)}}\n",
    "\\newcommand{\\Zfull}[1][]{\\color{red}{\\sigma^{#1} \\left( W_z x_t + U_z \\cdot h_{t-1} \\right)}}\n",
    "\\newcommand{\\Rfull}[1][]{\\color{blue}{\\sigma^{#1} \\left( W_r x_t + U_r \\cdot h_{t-1} \\right)}}\n",
    "% Vector Elements\n",
    "\\newcommand{\\htil}[1]{\\color{green}{\\tilde{h}^t_{#1}}}\n",
    "\\newcommand{\\h}[1]{\\color{green}{h_{#1}^{t-1}}}\n",
    "\\newcommand{\\z}[1]{\\color{red}{z_{#1}^{t}}}\n",
    "\\newcommand{\\r}[1]{\\color{blue}{r_{#1}^{t}}}\n",
    "% Explicit Vector Elements\n",
    "\\newcommand{\\gfull}[2][]{\\color{green}{g^{#1} \\left( w^h_{21} x_t + u^h_{#2 1} \\r{1} \\h{1} + u^h_{#2 2} \\r{2} \\h{2} \\right)}}\n",
    "\\newcommand{\\zfull}[2][]{\\color{red}{\\sigma^{#1} \\left(w^z_{#2} x_t + u^z_{#2 1}h_1^{t-1}+u^z_{#2 2} h_2^{t-1}\\right)}}\n",
    "\\newcommand{\\rfull}[2][]{\\color{blue}{\\sigma^{#1} \\left(w^r_{#2} x_t + u^r_{#2 1}h_1^{t-1}+u^r_{#2 2} h_2^{t-1}\\right)}}\n",
    "\\newcommand{\\gfullfull}[2][]{\\color{green}{g^{#1} \\left( w^h_{21} x_t + u^h_{#2 1} \\rfull{1} \\h{1} + u^h_{#2 2} \\rfull{2} \\h{2} \\right)}}\n",
    "% Weight Matrix Elements\n",
    "\\newcommand{\\wh}[1]{\\color{green}{w^h_{#1}}}\n",
    "\\newcommand{\\wz}[1]{\\color{red}{w^z_{#1}}}\n",
    "\\newcommand{\\wr}[1]{\\color{blue}{w^r_{#1}}}\n",
    "\\newcommand{\\uh}[2]{\\color{green}{u^h_{#1 #2}}}\n",
    "\\newcommand{\\uz}[2]{\\color{red}{u^z_{#1 #2}}}\n",
    "\\newcommand{\\ur}[2]{\\color{blue}{u^r_{#1 #2}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define activation functions\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(x):\n",
    "    \"\"\"Derivative of sigmoid function.\"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "def dtanh(x):\n",
    "    \"\"\"Derivative of tanh function.\"\"\"\n",
    "    return 1 - np.tanh(x)**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward pass\n",
    "\n",
    "Consider a low-dimensional numerical example with only one case with features $x = \\begin{pmatrix}x_1, & x_2, & x_3 \\end{pmatrix}^T$ and label $y$ additionally let's assume we only have two neurons in the GRU-cell. This means for the weight matrices \n",
    "\n",
    "\\begin{equation}\n",
    "\\color{green}{W_h} = \\begin{bmatrix} \\wh{1} \\\\ \\wh{2} \\end{bmatrix}  \\quad\n",
    "\\color{green}{U_h} = \\begin{bmatrix} \\uh{1}{1} & \\uh{1}{2} \\\\ \\uh{2}{1} & \\uh{2}{2}\\end{bmatrix}\\\\ \n",
    "\\color{red}{W_z} = \\begin{bmatrix} \\wz{1} \\\\ \\wz{2} \\end{bmatrix}  \\quad\n",
    "\\color{red}{U_z} = \\begin{bmatrix} \\uz{1}{1} & \\uz{1}{2} \\\\ \\uz{2}{1} & \\uz{2}{2} \\end{bmatrix}\\\\\n",
    "\\color{blue}{W_r} = \\begin{bmatrix} \\wr{1} \\\\ \\wr{2} \\end{bmatrix} \\quad\n",
    "\\color{blue}{U_r} = \\begin{bmatrix} \\ur{1}{1} & \\ur{1}{2} \\\\ \\ur{2}{1} & \\ur{2}{2} \\end{bmatrix} \n",
    "\\end{equation}\n",
    "\n",
    "Define the candidate as\n",
    "\\begin{align}\n",
    "\\Htil &= \\Hfull \\\\ &= \\begin{pmatrix} \\gfull{1} \\\\ \\gfull{2} \\end{pmatrix} \\\\ &= \\begin{pmatrix} \\gfullfull{1} \\\\ \\gfullfull{2} \\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Define the gates as\n",
    "\\begin{equation}\n",
    "\\Z = \\Zfull = \\begin{pmatrix} \\z{1} \\\\ \\z{2} \\end{pmatrix} = \\begin{pmatrix} \\zfull{1} \\\\ \\zfull{2} \\end{pmatrix}  \\\\\n",
    "\\R = \\Rfull = \\begin{pmatrix} \\r{1} \\\\ \\r{2} \\end{pmatrix} = \\begin{pmatrix} \\rfull{1} \\\\ \\rfull{2} \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "And the final update of the cell state as\n",
    "\n",
    "\\begin{align}\n",
    "h_t &= \\color{red}{1-\\Z} * h_{t-1} + \\Z * \\Htil \\\\ \n",
    "    &= \n",
    "\\begin{pmatrix}\n",
    "\\color{red}{(1-\\z{1})} h^{t-1}_1 + \\z{1} \\gfull{1} \\\\\n",
    "\\color{red}{(1-\\z{2})} h^{t-1}_2 + \\z{2} \\gfull{2}\n",
    "\\end{pmatrix} \\\\\n",
    "    &= \n",
    "\\begin{pmatrix}\n",
    "\\color{red}{(1-\\zfull{1})} h^{t-1}_1 + \\zfull{1} \\gfullfull{1} \\\\\n",
    "\\color{red}{(1-\\zfull{2})} h^{t-1}_2 + \\zfull{2} \\gfullfull{2}\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "For the actual prediction at a step $t$, we connect every output of our cell in a dense layer. Which means that we are taking a weighted sum of all two of them.\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y}_t &= W^T h_t = w_1 h^t_1 + w_2 h^t_2 \\\\ &= w_1 \\color{red}{(1-\\zfull{1})} h^{t-1}_1 + \\zfull{1} \\gfullfull{1} \\\\ &+ w_2\n",
    "\\color{red}{(1-\\zfull{2})} h^{t-1}_2 + \\zfull{2} \\gfullfull{2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a simple example containing just one sample $x \\in \\mathbb{R}^2$, suppose that the the weights at a certain point look like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sample\n",
    "x = np.array([0.2, 0.3, 0.4])\n",
    "y = 7.0\n",
    "\n",
    "# Initialize Weights\n",
    "Wh = np.array([0.2, 0.9])\n",
    "\n",
    "Wz = np.array([0.1, 3.1])\n",
    "Wr = np.array([2.3, 0.5])\n",
    "\n",
    "Uh = np.array([[1.5, 2.6], [1.8, 3.6]])\n",
    "Uz = np.array([[0.1, 4.1], [0.2, 1.0]])\n",
    "Ur = np.array([[1.3, 7.1], [9.1, 4.5]])\n",
    "\n",
    "w = np.array([2.0, 4.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement the forward pass like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [np.zeros_like(Wh)]\n",
    "h_ = []\n",
    "z = []\n",
    "r = []\n",
    "y_ = []\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    # Calculate values of the gates\n",
    "    zt = sigmoid(Wz.dot(xt) + Uh.dot(h[t]))\n",
    "    rt = sigmoid(Wr.dot(xt) + Ur.dot(h[t]))\n",
    "\n",
    "    # Calculate candidate update\n",
    "    h_t = tanh(Wh.dot(xt) + Uh.dot(rt * h[t]))\n",
    "\n",
    "    # Calculate cell state\n",
    "    ht = (1-zt) * h[t] + zt * h_t\n",
    "\n",
    "    # Calculate prediction at step t\n",
    "    y_t = w.dot(ht)\n",
    "\n",
    "    # Save variables to container\n",
    "    h.append(ht)\n",
    "    h_.append(h_t)\n",
    "    z.append(zt)\n",
    "    r.append(rt)\n",
    "    y_.append(y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backward pass\n",
    "\n",
    "We need the following gradients\n",
    "\n",
    "* the candidate update $\\frac{\\partial loss}{\\partial W_h}$ and $\\frac{\\partial loss}{\\partial U_h}$\n",
    "* the update for the update gate $\\frac{\\partial loss}{\\partial W_z}$ and $\\frac{\\partial loss}{\\partial U_z}$\n",
    "* the update for the recurrent gate $\\frac{\\partial loss}{\\partial W_r}$ and $\\frac{\\partial loss}{\\partial U_r}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights for the candidate $\\tilde{h}_t$\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial {loss}_t}{\\partial W_h} &= \\frac{\\partial {loss}_t}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial \\tilde{h}_t}\\frac{\\partial \\tilde{h}_t}{\\partial W_h} \\\\\n",
    "                                   &= (y-\\hat{y}_t) w * z_t * g^\\prime \\left( W_h x_t + U_h \\cdot (r_t * h_{t-1}) \\right) x_t\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdWh = np.zeros_like(Wh)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    # Note that `h` has an entry at start, so indexing at t accesses h_{t-1}\n",
    "    dh_tdWh = dtanh(Wh.dot(xt) + Uh.dot(r[t] * h[t])) * xt\n",
    "    dLossdWh += (y-y_[t]) * w * z[t] * dh_tdWh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial {loss}_t}{\\partial U_h} &= \\frac{\\partial {loss}_t}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial \\tilde{h}_t}\\frac{\\partial \\tilde{h}_t}{\\partial U_h} \\\\\n",
    "                                    &= (y-\\hat{y}_t)\\ \\textbf{diag} \\left[ w * z_t * g^\\prime \\left( W_h x_t + U_h \\cdot (r_t * h_{t-1}) \\right)\\right] \\begin{bmatrix} (r * h_{t-1})^T \\\\ (r * h_{t-1})^T \\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdUh = np.zeros_like(Uh)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dy_dh_t = w * z[t] * dtanh(Wh.dot(xt) + Uh.dot(r[t] * h[t]))\n",
    "    dLossdUh += (y-y_[t]) * dy_dh_t.reshape(-1, 1) * h[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights for the update gate $z_t$\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial {loss}_t}{\\partial W_z} &= \\frac{\\partial {loss}_t}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial z_t}\\frac{\\partial z_t}{\\partial W_z} \\\\\n",
    "                                   &= (y-\\hat{y}_t)\\ w * [-h_t + \\tilde{h}_t] * \\sigma^\\prime \\left( W_z x_t + U_z h_{t-1} \\right) x_t\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdWz = np.zeros_like(Wz)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dztdWz = dsigmoid(Wz.dot(xt) + Uz.dot(h[t])) * xt\n",
    "    dLossdWz += (y-y_t) * w * (-h[t+1] - h_[t]) * dztdWz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial {loss}_t}{\\partial U_z} &= \\frac{\\partial loss}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial z_t}\\frac{\\partial z_t}{\\partial U_z} \\\\\n",
    "                                   &= (y-\\hat{y}_t)\\ \\textbf{diag} \\left[ w * [-h_t + \\tilde{h}_t] * \\sigma^\\prime \\left( W_z x_t + U_z h_{t-1} \\right)\\right] \\begin{bmatrix} h_{t-1}^T \\\\ h_{t-1}^T \\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdUz = np.zeros_like(Uz)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dy_dzt = w * [-h[t+1] + h_[t] * dsigmoid(Wz.dot(xt) + Uz.dot(h[t]))]\n",
    "    dLossdUz += (y-y_[t]) * dy_dzt.reshape(-1, 1) * h[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights for the recurrent gate $r_t$\n",
    "\\begin{align}\n",
    "\\frac{\\partial {loss}_t}{\\partial W_r} &= \\frac{\\partial {loss}_t}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial \\tilde{h}_t}\\frac{\\partial \\tilde{h}_t}{\\partial W_r}\n",
    "\\end{align}\n",
    "\n",
    "However, due to the entangled nature of $U_h$, $r_t$ and $h_{t-1}$ and the element-wise multiplication inside the parentheses it is not immediately obvious how to obtain the derivative with respect to any of the inner weight matrices. Thus, we will rewrite $\\tilde{h}_t$ as a system of linear equations. For ease of notation denote\n",
    "\n",
    "\\begin{equation}\n",
    "W_h = \\begin{bmatrix} w_1^h \\\\ w_2^h \\end{bmatrix} \\quad U_h = \\begin{bmatrix} u_{11}^h & u_{12}^h \\\\ u_{21}^h &\n",
    "u_{22}^h \\end{bmatrix} \\quad W_r = \\begin{bmatrix} w_{1}^r \\\\ w_{2}^r \\end{bmatrix} \\quad U_r = \\begin{bmatrix} u_{11}^r & u_{12}^r \\\\ u_{21}^r &\n",
    "u_{22}^r \\end{bmatrix} \\quad \\tilde{h}_t = g\\left( W_h x_t + U_h (r_t * h_{t-1}) \\right) = \\begin{pmatrix} g(\\cdot)_1 \\\\ g(\\cdot)_2  \\end{pmatrix} \\quad h_{t-1} = \\begin{pmatrix} h^{t-1}_1 \\\\ h^{t-1}_2\\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "This way we get\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{h}_t &= g\\left( W_h x_t + U_h \\cdot (r_t * h_{t-1}) \\right) \\\\\n",
    "            &= g \\begin{pmatrix} w_{1}^h x_t + u_{11}^h r_1 h^{t-1}_1 + u_{12}^h r_2 h^{t-1}_2 \\\\\n",
    "                                 w_{2}^h x_t + u_{21}^h r_1 h^{t-1}_1 + u_{22}^h r_2 h^{t-1}_2\n",
    "                                 \\end{pmatrix} \\\\\n",
    "            &= \\begin{pmatrix} g(w_{1}^h x_t + u_{11}^h \\sigma(w_{1}^r x_t + u_{11}^r h^{t-1}_1 + u_{12}^r h^{t-1}_2) h^{t-1}_1 + u^h_{12} \\sigma(w_{2}^r x_t + u_{21}^r h^{t-1}_1 + u^r_{22} h^{t-1}_2) h^{t-1}_2) \\\\\n",
    "                               g(w^h_{2} x_t + u^h_{21} \\sigma(w^r_{1}x_t + u^r_{11} h^{t-1}_1 + u^r_{12} h^{t-1}_2) h^{t-1}_1 + u^h_{22} \\sigma(w^r_{2}x_t + u^r_{21} h^{t-1}_1 + u^r_{22} h^{t-1}_2) h^{t-1}_2)\n",
    "                                 \\end{pmatrix} \\\\\n",
    "\\end{align}\n",
    "\n",
    "This way we get the derivatives for\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\tilde{h}_t}{\\partial w_1^r} = \\begin{pmatrix} g^\\prime(\\cdot)_1 u^h_{11} h_1^{t-1} \\sigma^\\prime(\\cdot)_1 x_t \\\\\n",
    "                                                              g^\\prime(\\cdot)_2 u^h_{21} h_1^{t-1} \\sigma^\\prime(\\cdot)_1 x_t\n",
    "                                              \\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "and\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\tilde{h}_t}{\\partial w_2^r} = \\begin{pmatrix} g^\\prime(\\cdot)_1 u^h_{12} h_2^{t-1} \\sigma^\\prime(\\cdot)_2 x_t \\\\\n",
    "                                                              g^\\prime(\\cdot)_2 u^h_{22} h_2^{t-1} \\sigma^\\prime(\\cdot)_2 x_t\n",
    "                                              \\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "which we can combine and reformulate as\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\tilde{h}_t}{\\partial W_r} &= \\begin{bmatrix} g_1^\\prime(\\cdot) u^h_{11} h_1^{t-1} \\sigma^\\prime(\\cdot)_1 x_t &                                                                        g_1^\\prime(\\cdot) u^h_{12} h_2^{t-1} \\sigma^\\prime(\\cdot)_2 x_t \\\\\n",
    "                                                             g_2^\\prime(\\cdot) u^h_{21} h_1^{t-1} \\sigma^\\prime(\\cdot)_1 x_t &\n",
    "                                                             g_2^\\prime(\\cdot) u^h_{22} h_2^{t-1} \\sigma^\\prime(\\cdot)_2 x_t\n",
    "                                                             \\end{bmatrix} \\\\\n",
    "                                          &= \\begin{bmatrix} g^\\prime(\\cdot)_1 & 0 \\\\\n",
    "                                                             0 & g^\\prime(\\cdot)_2\n",
    "                                             \\end{bmatrix} * U_h * \\begin{bmatrix} h_{t-1}^T \\\\ h_{t-1}^T \\end{bmatrix}\\ x_t \\\\\n",
    "                                          &= \\textbf{diag}\\left[ \\tilde{h}_t \\right] \\cdot U_h * \\begin{bmatrix} h_{t-1}^T \\\\ h_{t-1}^T \\end{bmatrix}\\ x_t\n",
    "\\end{align}\n",
    "\n",
    "which in turn provides us with\n",
    "\\begin{align}\n",
    "\\frac{\\partial {loss}_t}{\\partial W_r} &= \\frac{\\partial {loss}_t}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial \\tilde{h}_t}\\frac{\\partial \\tilde{h}_t}{\\partial W_r} \\\\\n",
    "                                   &= (y-\\hat{y}_t)\\ \\textbf{diag} \\left[ w * z_t * \\tilde{h}_t \\right] \\cdot U_h * \\begin{bmatrix} h_{t-1}^T \\\\ h_{t-1}^T \\end{bmatrix}\\ x_t.\n",
    "\\end{align}\n",
    "\n",
    "In our example this is a $2 \\times 2$ Jacobian matrix which we need to transform in order to update our $2 \\times 1$ vector $W_z$. As it stands, we have ordered the gradients column-wise. The first column of $\\frac{\\partial {loss}_t }{\\partial W_r}$ contains the partial derivative with respect to the first element of $W_r$ and the second column of $\\frac{\\partial {loss}_t}{\\partial W_r}$ contains the partial derivative with respect to the second element of $W_r$. So in order to obtain the total gradients we can compute the column-sums. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdWr = np.zeros_like(Wr)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    jacobian = (y-y_[t]) * (w * z[t] * h_[t]).reshape(-1, 1) * Uh * h[t] * xt\n",
    "    dLossdWr += jacobian.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we get for the gradients with respect to $U_r$\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\tilde{h}_t}{\\partial u^r_{11}} &= \\begin{bmatrix} \n",
    "    g^\\prime (\\cdot)_1 u_{11}^r h^{t-1}_1 \\sigma^\\prime (\\cdot)_1 h^{t-1}_1 \\\\ \n",
    "    g^\\prime (\\cdot)_2 u_{21}^r h^{t-1}_1 \\sigma^\\prime (\\cdot)_2 h^{t-1}_1\n",
    "\\end{bmatrix}  \\quad \n",
    "\\frac{\\partial \\tilde{h}_t}{\\partial u^r_{12}} = \\begin{bmatrix} \n",
    "    g^\\prime (\\cdot)_1 u_{11}^r h^{t-1}_1 \\sigma^\\prime (\\cdot)_1 h^{t-1}_2 \\\\ \n",
    "    g^\\prime (\\cdot)_2 u_{21}^r h^{t-1}_1 \\sigma^\\prime (\\cdot)_1 h^{t-1}_2\n",
    "\\end{bmatrix}\\\\\n",
    "\\frac{\\partial \\tilde{h}_t}{\\partial u^r_{21}} &= \\begin{bmatrix} \n",
    "    g^\\prime (\\cdot)_1 u_{12}^r h^{t-1}_2 \\sigma^\\prime (\\cdot)_1 h^{t-1}_1 \\\\ \n",
    "    g^\\prime (\\cdot)_2 u_{22}^r h^{t-1}_2 \\sigma^\\prime (\\cdot)_2 h^{t-1}_1\n",
    "\\end{bmatrix} \\quad \n",
    "\\frac{\\partial \\tilde{h}_t}{\\partial u^r_{22}} = \\begin{bmatrix} \n",
    "    g^\\prime (\\cdot)_1 u_{12}^r h^{t-1}_2 \\sigma^\\prime (\\cdot)_1 h^{t-1}_2 \\\\ \n",
    "    g^\\prime (\\cdot)_2 u_{22}^r h^{t-1}_2 \\sigma^\\prime (\\cdot)_2 h^{t-1}_2\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Where, again, we will sum all fitting elements of each gradient in order to get a $2 \\times 2$ matrix. Therefore we get\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\tilde{h}_t}{\\partial U_r} &= \\begin{bmatrix} g^\\prime (\\cdot)_1 u_{11}^r h^{t-1}_1 \\sigma^\\prime (\\cdot)_1 h^{t-1}_1 + \n",
    "    g^\\prime (\\cdot)_2 u_{21}^r h^{t-1}_1 \\sigma^\\prime (\\cdot)_2 h^{t-1}_1 & g^\\prime (\\cdot)_1 u_{11}^r h^{t-1}_1 \\sigma^\\prime (\\cdot)_1 h^{t-1}_2 + g^\\prime (\\cdot)_2 u_{21}^r h^{t-1}_1 \\sigma^\\prime (\\cdot)_1 h^{t-1}_2 \\\\\n",
    "    g^\\prime (\\cdot)_1 u_{12}^r h^{t-1}_2 \\sigma^\\prime (\\cdot)_1 h^{t-1}_1 + \n",
    "    g^\\prime (\\cdot)_2 u_{22}^r h^{t-1}_2 \\sigma^\\prime (\\cdot)_2 h^{t-1}_1 & g^\\prime (\\cdot)_1 u_{12}^r h^{t-1}_2 \\sigma^\\prime (\\cdot)_1 h^{t-1}_2 + \n",
    "    g^\\prime (\\cdot)_2 u_{22}^r h^{t-1}_2 \\sigma^\\prime (\\cdot)_2 h^{t-1}_2\\end{bmatrix} \\\\\n",
    "    &= \\left(\\begin{bmatrix} g^\\prime (\\cdot)_1 & g^\\prime (\\cdot)_2 \\\\ g^\\prime (\\cdot)_1 & g^\\prime (\\cdot)_2 \\end{bmatrix} \\cdot U_r\\right) * \n",
    "    \\begin{bmatrix} \\sigma^\\prime (\\cdot)_1 & \\sigma^\\prime (\\cdot)_1 \\\\\n",
    "                    \\sigma^\\prime (\\cdot)_2 & \\sigma^\\prime (\\cdot)_2 \\end{bmatrix} * \\begin{bmatrix}\n",
    "                    h^{t-1}_1 h^{t-1}_1 & h^{t-1}_1 h^{t-1}_2 \\\\ h^{t-1}_2 h^{t-1}_1 & h^{t-1}_2 h^{t-1}_2\n",
    "                    \\end{bmatrix} \\\\\n",
    "    &= \\begin{bmatrix} g^\\prime (\\cdot)^T \\\\ g^\\prime (\\cdot)^T \\end{bmatrix} \\cdot U_r \\cdot \\textbf{diag}\\left[ \\sigma^\\prime (\\cdot)\\right] * h_{t-1} h_{t-1}^T\n",
    "\\end{align}\n",
    "\n",
    "which leaves us with\n",
    "\\begin{align}\n",
    "\\frac{\\partial {loss}_t}{\\partial U_r} &= \\frac{\\partial {loss}_t}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial \\tilde{h}_t}\\frac{\\partial \\tilde{h}_t}{\\partial U_r} \\\\\n",
    "                                       &= \n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdUr = np.zeros_like(Ur)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dtanh(Wh.dot(xt) + Uh.dot(r[t] * h[t])).reshape(-1, 1)\n",
    "    dsigmoid(Wr.dot(xt) + Ur.dot(h[t])).reshape(-1, 1)\n",
    "    np.outer(h[t], h[t])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
