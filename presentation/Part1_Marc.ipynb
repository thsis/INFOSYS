{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Crime Prediction with Recurrent Neural Networks (RNNs)#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    " - Why do we need Recurrent Neural Networks?\n",
    " - Introduction to RNNs?\n",
    " - Long Short Term Memory Cell\n",
    " - Gated Recurrent Unit\n",
    " - Introduction to the Crime Dataset \n",
    " - Implementation of RNN, LSTM and GRU\n",
    " - Parameter Tuning and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why do we need Recurrent Neural Networks? \n",
    "<br>\n",
    "### Recap Feed Forward Network\n",
    "<br>\n",
    "<img src=\"presentation_pics/neural_network1.png\" alt=\"NN\" style=\"width: 400px;\"/>\n",
    "<br>\n",
    "[Source:  Alisa's Presentation on NN Primer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lets simplify\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"presentation_pics/NN_simplified.png\" alt=\"NN\" style=\"width: 500px;\"/>\n",
    "<br>\n",
    "\n",
    "### Input data:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __“In France, I had a great time”__\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "[Source: https://course.fast.ai/lessons/lesson6.html ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Changed from color to shape coding \n",
    "- got rid of dimensionality --> eachs symbol represents multiple activations\n",
    "- each arrow represents a Layer operation e.g. a matrix multiplication\n",
    "<br>\n",
    "<br>\n",
    "- Theory on Textanalysis example, bc intuitve understanding of sequential dependencies in Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use Case sentiment analysis\n",
    "### Is this a __positive__ or a __negative__ statement?\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __“In France, I had a great time”__\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"presentation_pics/NN_simplified.png\" alt=\"NN\" style=\"width: 500px;\"/>\n",
    "<br>\n",
    "__One Solution:__\n",
    "\n",
    "- Turn sentence into vector (Bag-of-Words)\n",
    "- Use NN to predict the sentiment for each sentence independently\n",
    "<br>\n",
    "\n",
    "[Source: https://course.fast.ai/lessons/lesson6.html ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### One Solution:\n",
    "- Use Bag-of_words to transfer sentence into a vector (Order doesnt matter)\n",
    "- Use a Feed Forward Network to predict the class of the given sentence\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Notice:\n",
    "- The order of the words is not taken into account\n",
    "- Each sentence / document is a single observation\n",
    "- The classification of the next sentencen is independent from the last sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's change our classification task:\n",
    "\n",
    "### What will be the next word?\n",
    "<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __“In France, I had a great ...?”__\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"presentation_pics/NN_simplified.png\" alt=\"NN\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "__How can we include multiple observation vectors while preserving the order?__\n",
    "\n",
    "<br>\n",
    "\n",
    "[Source: https://course.fast.ai/lessons/lesson6.html ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### Whats new:\n",
    "- Each word is an observation at a given point of time\n",
    "- The next word depends mainly on the previous words -> the order is important\n",
    "- Each word might be represented as a vector \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to add annother preceding timestep\n",
    "\n",
    "### What will be the next word?\n",
    "<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __“a great ...?”__\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"presentation_pics/RNN2.png\" alt=\"NN\" style=\"width: 600px;\"/>\n",
    "<br>\n",
    "<br>\n",
    "Prediction at time t:  $$y_t = softmax(W_{out}h_t)$$\n",
    "Activations for step t: $$h_t = tanh(W_{in}x_t + W_hh_{t-1})$$\n",
    "<br>\n",
    "[Source: https://course.fast.ai/lessons/lesson6.html ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "W_in, W_h, W_out are equal across all steps --> reduces the number of parameters to learn\n",
    "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to add annother preceding timestep\n",
    "\n",
    "### What will be the next word?\n",
    "<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __“had a great ...?”__\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"presentation_pics/RNN3.png\" alt=\"NN\" style=\"width: 650px;\"/>\n",
    "<br>\n",
    "[Source: https://course.fast.ai/lessons/lesson6.html ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adding an abitrary number of preceding words\n",
    "\n",
    "### What will be the next word?\n",
    "<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __“In France, I had a great ...?”__\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"presentation_pics/RNN_loop.png\" alt=\"NN\" style=\"width: 600px;\"/>\n",
    "<br>\n",
    "[Source: https://course.fast.ai/lessons/lesson6.html ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to train a Recurrent Neural Net\n",
    "\n",
    "### Backpropagation through time (bptt)\n",
    "<br>\n",
    "\n",
    "\n",
    "Total Loss: $$J(\\theta) = \\sum_t{J_t(\\theta)}$$\n",
    "Loss at time t: $$J_t(\\theta) = f(y_{t,true}, y_{t})$$\n",
    "\n",
    "<br>\n",
    "Gradient: $$\\frac{\\partial J_t}{\\partial W_{in}} =\n",
    "\\frac{\\partial J_t}{\\partial y_{t}}\\frac{\\partial y_t}{\\partial W_{in}} = \n",
    "\\frac{\\partial J_t}{\\partial y_{t}}\\frac{\\partial y_t}{\\partial h_{t}}\\frac{\\partial h_t}{\\partial W_{in}}=\n",
    "\\frac{\\partial J_t}{\\partial y_{t}}\\frac{\\partial y_t}{\\partial h_{t}}\\frac{\\partial h_t}{\\partial h_{t-1}}\n",
    "\\frac{\\partial h_{t-1}}{\\partial W_{in}} = \n",
    "\\frac{\\partial J_t}{\\partial y_{t}}\\frac{\\partial y_t}{\\partial h_{t}}\\frac{\\partial h_t}{\\partial h_{t-1}}\n",
    "\\frac{\\partial h_{t-1}}{\\partial h_{t-2}} ...\\frac{\\partial h_{0}}{\\partial W_{in}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"presentation_pics/RNN_loop.png\" alt=\"NN\" style=\"width: 600px;\"/>\n",
    "<br>\n",
    "[Source: http://introtodeeplearning.com/materials/2018_6S191_Lecture2.pdf ]<br>\n",
    "[Source: http://neuralnetworksanddeeplearning.com/chap5.html ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Yet annother representation\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"presentation_pics/RNN_alt.png\" alt=\"NN\" style=\"width: 600px;\"/>\n",
    "<br>\n",
    "[Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Drawback of a simple Recurrent Network\n",
    "<br>\n",
    "\n",
    "### Forgetting long-term dependencies:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __“In France, I had a great time and I learnt some of the ...? [language]\"__ \n",
    " \n",
    "<img src=\"presentation_pics/vanishing_gradient.png\" alt=\"vanishing_gradient\" style=\"width: 600px;\"/>\n",
    "<br>\n",
    "[Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- cannot connect information anymore\n",
    "- \"vanishing gradient problem\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vanishing Gradient problem\n",
    "\n",
    "Gradient: $$\\frac{\\partial J_t}{\\partial W_{in}} =\\frac{\\partial J_t}{\\partial y_{t}}\\frac{\\partial y_t}{\\partial h_{t}}\\frac{\\partial h_t}{\\partial h_{t-1}}\n",
    "\\frac{\\partial h_{t-1}}{\\partial h_{t-2}} ...\\frac{\\partial h_{0}}{\\partial W_{in}}$$\n",
    "<br><br>\n",
    "It can be shown: $$\\frac{\\partial h_t}{\\partial h_{t-1}}\n",
    "= W_{in}^T diag[tanh^{ `}(W_{in}+W_h x_j)]$$\n",
    "<br>\n",
    "$tanh^{ `} \\in [0,1] $ <br><br>\n",
    "$W_{in}$ = sampled from standard normal distribution = mostly < 1\n",
    "<br>\n",
    "<br>\n",
    "[Source: http://introtodeeplearning.com/materials/2018_6S191_Lecture2.pdf]<br>\n",
    "[Source: http://neuralnetworksanddeeplearning.com/chap5.html ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- backpropagation through time\n",
    "- as gap between timesteps becomes bigger, product longer and we are multiplying very small numbers (small gradients)\n",
    "- due to activation function (tanh)\n",
    "- some crucial previous timesteps do not influence anymore in later timesteps: gradient vanishes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application / Use Case\n",
    "### Where should the head of the Chicago Police Force sent his patrols?\n",
    "<img src=\"presentation_pics/crime_intro.png\" alt=\"GRU\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning and Ethics\n",
    "### Should the police apply our Model?\n",
    "#### Pro:\n",
    "- Reduce Crime\n",
    "- Use public resources more effectivly\n",
    "<br>\n",
    "\n",
    "#### Contra:\n",
    "- Data bias towards certain crime types and neighborhoods\n",
    "- Confirmation bias\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "[Source: Cathy O’Neil, Weapons of Math Destruction, Chapter 5]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
