{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Crime Prediction with Recurrent Neural Networks #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    " - Why do we need Recurrent Neural Networks?\n",
    " - Introduction to RNNs\n",
    " - Long Short Term Memory Cell\n",
    " - Gated Recurrent Unit\n",
    " - Introduction to the Crime Dataset \n",
    " - Implementation of RNN, LSTM and GRU\n",
    " - Parameter Tuning and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why do we need Recurrent Neural Networks? - An Example from Natural Language Processing\n",
    "\n",
    "### Is this a __positive__ or a __negative__ statement?\n",
    "<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __“In France, I had a great time”__\n",
    "\n",
    "#### One Solution:\n",
    "- Use Bag-of_words to transfer sentence into a vector\n",
    "- Use a Feed Forward Network to predict the class of the given sentence\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Notice:\n",
    "- The order of the words is not taken into account\n",
    "- Each sentence / document is a single observation\n",
    "- The classification of the next sentencen is independent from the last sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's change our prediction task:\n",
    "\n",
    "### What will be the next word?\n",
    "<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __“In France, I had a great ...?”__\n",
    "<br>\n",
    "#### Whats different?\n",
    "- Each word is an observation at a given point of time\n",
    "- The next word depends mainly on the previous words -> the order is important\n",
    "- Each word might be representet as a single vector\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Notice:\n",
    "- Hard to solve with a Feed Forward Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we caputure these properties in a Neural Network?\n",
    "\n",
    "### What will be the next word?\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __“In France, I had a great ...?”__\n",
    "<br>\n",
    "### Recap Feed Forward Network\n",
    "<img src=\"presentation_pics/neural_network1.png\" alt=\"NN\" style=\"width: 400px;\"/>\n",
    "<br>\n",
    "[Source:  Alisa's Presentation on NN Primer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's simplify our representation and the input size\n",
    "\n",
    "### What will be the next word?\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __“great ...?”__\n",
    "<br>\n",
    "<img src=\"presentation_pics/NN_simplified.png\" alt=\"NN\" style=\"width: 400px;\"/>\n",
    "<br>\n",
    "[Source:  fastai Practical Deep Learning for Coders, Lesson 6: RNNs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to add annother preceding timestep\n",
    "\n",
    "### What will be the next word?\n",
    "<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __“a great ...?”__\n",
    "<br>\n",
    "<img src=\"presentation_pics/RNN2.png\" alt=\"NN\" style=\"width: 400px;\"/>\n",
    "<br>\n",
    "[Source:  fastai Practical Deep Learning for Coders, Lesson 6: RNNs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to add annother preceding timestep\n",
    "\n",
    "### What will be the next word?\n",
    "<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __“had a great ...?”__\n",
    "<br>\n",
    "<img src=\"presentation_pics/RNN3.png\" alt=\"NN\" style=\"width: 600px;\"/>\n",
    "<br>\n",
    "[Source:  fastai Practical Deep Learning for Coders, Lesson 6: RNNs]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding an abitrary number of preceding words\n",
    "\n",
    "### What will be the next word?\n",
    "<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __“In France, I had a great ...?”__<br>\n",
    "<img src=\"presentation_pics/RNN_loop.png\" alt=\"NN\" style=\"width: 600px;\"/>\n",
    "<br>\n",
    "[Source:  fastai Practical Deep Learning for Coders, Lesson 6: RNNs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yet annother representation\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"presentation_pics/RNN_alt.png\" alt=\"NN\" style=\"width: 600px;\"/>\n",
    "<br>\n",
    "[Source:  fastai Practical Deep Learning for Coders, Lesson 6: RNNs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Placeholder Math forward]  \n",
    "<br>\n",
    "h_t-1 = sigmoid(0 + X_t-1 x W_in) <br>\n",
    "h_t = sigmoid(h_t-1 x W_h + X_t x W_in) <br>\n",
    "h_t+1 = sigmoid(h_t x W_h + X_t+1 x W_in) <br>\n",
    "Y = X_t+1 = softmax(h_t+1 x W_out)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Placeholder Math derivation]  \n",
    "\n",
    "dY / dx_t-1 = dY / dX_t-1 * dh_t+1 / dX_t-1\n",
    "\n",
    "h_t-1 = sigmoid(X_t-1 x W_in) <br>\n",
    "h_t = sigmoid(h_t-1 x W_h + X_t x W_in) <br>\n",
    "h_t+1 = sigmoid(h_2 x W_h + X_t x W_in) <br>\n",
    "Y = X_t+1 = sigmoid(h_2 x W_h + X_t x W_in) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why not use RNN for further implementation?\n",
    "- issue for long-term dependencies: back to example \n",
    "# “In France, I had a great time and I learnt some of the _____ language.”\n",
    " \n",
    "- cannot connect information anymore\n",
    "- \"vanishing gradient problem\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing Gradient problem\n",
    "- backpropagation through time\n",
    "- as gap between timesteps becomes bigger, product longer and we are multiplying very small numbers (small gradients)\n",
    "- due to activation function (tanh)\n",
    "- some crucial previous timesteps do not influence anymore in later timesteps: gradient vanishes...\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial J_2}{\\partial W}=\\frac{\\partial J_2}{\\partial y_2}...\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"presentation_pics/vanishing_gradient.png\" alt=\"vanishing_gradient\" style=\"width: 600px;\"/>\n",
    "[Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LSTM Network - Long Short-Term Memory Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LSTM\n",
    "<img src=\"presentation_pics/LSTM.png\" alt=\"LSTM\" style=\"width: 800px;\"/>\n",
    "[Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GRU Network - Gated Recurrent Unit##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gated Cells (LSTM/GRU) can keep track of information throughout the timeseries\n",
    "- consider output of previous timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GRU\n",
    "<img src=\"presentation_pics/GRU.png\" alt=\"GRU\" style=\"width: 800px;\"/>\n",
    "\n",
    "[Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application / Use Case\n",
    "### Where should the head of the Chicago Police Force sent his patrols?\n",
    "<img src=\"presentation_pics/crime_intro.png\" alt=\"GRU\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Placeholder Interactive Crime Map]\n",
    "Could be used to argue crime changes over time, thus this is a time series problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Implementation of RNN, LSTM and GRU\n",
    "Parameter Tuning and Evaluation"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
