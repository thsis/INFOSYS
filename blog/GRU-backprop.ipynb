{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation functions\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(x):\n",
    "    \"\"\"Derivative of sigmoid function.\"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "def dtanh(x):\n",
    "    \"\"\"Derivative of tanh function.\"\"\"\n",
    "    return 1 - np.tanh(x)**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "% Vector shortcuts\n",
    "\\newcommand{\\Htil}{\\color{green}{\\tilde{h}_t}}\n",
    "\\newcommand{\\H}{\\color{green}{h_{t-1}}}\n",
    "\\newcommand{\\R}{\\color{blue}{r_t}}\n",
    "\\newcommand{\\Z}{\\color{red}{z_t}}\n",
    "% Gates Matrix Notation\n",
    "\\newcommand{\\Gfull}[1][]{\\color{green}{g^{#1}\\left( W_h x_t + U_h \\cdot ( \\R * \\H)\\right)}}\n",
    "\\newcommand{\\Zfull}[1][]{\\color{red}{\\sigma^{#1} \\left( W_z x_t + U_z \\cdot h_{t-1} \\right)}}\n",
    "\\newcommand{\\Rfull}[1][]{\\color{blue}{\\sigma^{#1} \\left( W_r x_t + U_r \\cdot h_{t-1} \\right)}}\n",
    "% Vector Elements\n",
    "\\newcommand{\\htil}[1]{\\color{green}{\\tilde{h}^t_{#1}}}\n",
    "\\newcommand{\\h}[1]{\\color{green}{h_{#1}^{t-1}}}\n",
    "\\newcommand{\\z}[1]{\\color{red}{z_{#1}^{t}}}\n",
    "\\newcommand{\\r}[1]{\\color{blue}{r_{#1}^{t}}}\n",
    "% Explicit Vector Elements\n",
    "\\newcommand{\\gfull}[2][]{\\color{green}{g^{#1} \\left( w^h_{#2} x_t + u^h_{#2 1} \\r{1} \\h{1} + u^h_{#2 2} \\r{2} \\h{2} \\right)}}\n",
    "\\newcommand{\\zfull}[2][]{\\color{red}{\\sigma^{#1} \\left(w^z_{#2} x_t + u^z_{#2 1}h_1^{t-1}+u^z_{#2 2} h_2^{t-1}\\right)}}\n",
    "\\newcommand{\\rfull}[2][]{\\color{blue}{\\sigma^{#1} \\left(w^r_{#2} x_t + u^r_{#2 1}h_1^{t-1}+u^r_{#2 2} h_2^{t-1}\\right)}}\n",
    "\\newcommand{\\gfullfull}[2][]{\\color{green}{g^{#1} \\left( w^h_{#2} x_t + u^h_{#2 1} \\rfull{1} \\h{1} + u^h_{#2 2} \\rfull{2} \\h{2} \\right)}}\n",
    "% Weight Matrices\n",
    "\\newcommand{\\Wh}{\\color{green}{W_h}}\n",
    "\\newcommand{\\Wz}{\\color{red}{W_z}}\n",
    "\\newcommand{\\Wr}{\\color{blue}{W_r}}\n",
    "\\newcommand{\\Uh}{\\color{green}{U_h}}\n",
    "\\newcommand{\\Uz}{\\color{red}{U_z}}\n",
    "\\newcommand{\\Ur}{\\color{blue}{U_r}}\n",
    "% Weight Matrix Elements\n",
    "\\newcommand{\\wh}[1]{\\color{green}{w^h_{#1}}}\n",
    "\\newcommand{\\wz}[1]{\\color{red}{w^z_{#1}}}\n",
    "\\newcommand{\\wr}[1]{\\color{blue}{w^r_{#1}}}\n",
    "\\newcommand{\\uh}[2]{\\color{green}{u^h_{#1 #2}}}\n",
    "\\newcommand{\\uz}[2]{\\color{red}{u^z_{#1 #2}}}\n",
    "\\newcommand{\\ur}[2]{\\color{blue}{u^r_{#1 #2}}}\n",
    "% Miscellaneous\n",
    "\\newcommand{\\dxt}[1]{\\color{#1}{x_t}}\n",
    "\\newcommand{\\yhat}{\\hat{y}_t}\n",
    "\\newcommand{\\deriv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\DeclareMathOperator{\\diag}{diag}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again, a low-dimensional numerical example with only one case with features $x = \\begin{pmatrix}x_1, & x_2, & x_3 \\end{pmatrix}^T$ and label $y$. Additionally let's assume we only have two neurons in our network. This means for the weight matrices \n",
    "\n",
    "\\begin{alignat}{2}\n",
    "\\color{green}{W_h} &= \\begin{bmatrix} \\wh{1} \\\\ \\wh{2} \\end{bmatrix}  \\quad\n",
    "\\color{green}{U_h} &&= \\begin{bmatrix} \\uh{1}{1} & \\uh{1}{2} \\\\ \\uh{2}{1} & \\uh{2}{2}\\end{bmatrix}\\\\ \n",
    "\\color{red}{W_z} &= \\begin{bmatrix} \\wz{1} \\\\ \\wz{2} \\end{bmatrix}  \\quad\n",
    "\\color{red}{U_z} &&= \\begin{bmatrix} \\uz{1}{1} & \\uz{1}{2} \\\\ \\uz{2}{1} & \\uz{2}{2} \\end{bmatrix}\\\\\n",
    "\\color{blue}{W_r} &= \\begin{bmatrix} \\wr{1} \\\\ \\wr{2} \\end{bmatrix} \\quad\n",
    "\\color{blue}{U_r} &&= \\begin{bmatrix} \\ur{1}{1} & \\ur{1}{2} \\\\ \\ur{2}{1} & \\ur{2}{2} \\end{bmatrix} \n",
    "\\end{alignat}\n",
    "\n",
    "Define the candidate as\n",
    "\\begin{align}\n",
    "\\Htil &= \\Gfull \\\\ &= \\begin{pmatrix} \\gfull{1} \\\\ \\gfull{2} \\end{pmatrix} \\\\ &= \\begin{pmatrix} \\gfullfull{1} \\\\ \\gfullfull{2} \\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Define the gates as\n",
    "\\begin{equation}\n",
    "\\Z = \\Zfull = \\begin{pmatrix} \\z{1} \\\\ \\z{2} \\end{pmatrix} = \\begin{pmatrix} \\zfull{1} \\\\ \\zfull{2} \\end{pmatrix}  \\\\\n",
    "\\R = \\Rfull = \\begin{pmatrix} \\r{1} \\\\ \\r{2} \\end{pmatrix} = \\begin{pmatrix} \\rfull{1} \\\\ \\rfull{2} \\end{pmatrix},\n",
    "\\end{equation}\n",
    "\n",
    "where $g$ is the $tanh$-function and $\\sigma$ the sigmoid function.\n",
    "\n",
    "And the final update of the hidden state as\n",
    "\n",
    "\\begin{align}\n",
    "h_t &= \\color{red}{1-\\Z} * h_{t-1} + \\Z * \\Htil \\\\ \n",
    "    &= \n",
    "\\begin{pmatrix}\n",
    "\\color{red}{(1-\\z{1})} h^{t-1}_1 + \\z{1} \\gfull{1} \\\\\n",
    "\\color{red}{(1-\\z{2})} h^{t-1}_2 + \\z{2} \\gfull{2}\n",
    "\\end{pmatrix} \\\\\n",
    "    &= \n",
    "\\begin{pmatrix}\n",
    "\\color{red}{(1-\\zfull{1})} h^{t-1}_1 + \\zfull{1} \\gfullfull{1} \\\\\n",
    "\\color{red}{(1-\\zfull{2})} h^{t-1}_2 + \\zfull{2} \\gfullfull{2}\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "For the actual prediction at a step $t$, we connect every output of our hidden state in a dense layer. Which means that we are taking a weighted sum of all two of them.\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y}_t &= W^T h_t = w_1 h^t_1 + w_2 h^t_2 \\\\ &= w_1 \\color{red}{(1-\\zfull{1})} h^{t-1}_1 \\\\&\\quad + \\zfull{1} \\gfullfull{1} \\\\ &+ w_2\n",
    "\\color{red}{(1-\\zfull{2})} h^{t-1}_2 \\\\&\\quad+ \\zfull{2} \\gfullfull{2},\n",
    "\\end{align}\n",
    "\n",
    "Where $W = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}$ is the matrix that contains the weights for the outer layer. In our case this is just a $2\\times 1$ vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a simple example containing just one sample $x \\in \\mathbb{R}^2$, suppose that the the weights at a certain point look like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sample\n",
    "x = np.array([0.2, 0.3, 0.4])\n",
    "y = 7.0\n",
    "\n",
    "# Initialize Weights\n",
    "Wh = np.array([0.2, 0.9])\n",
    "\n",
    "Wz = np.array([0.1, 3.1])\n",
    "Wr = np.array([2.3, 0.5])\n",
    "\n",
    "Uh = np.array([[1.5, 2.6], [1.8, 3.6]])\n",
    "Uz = np.array([[0.1, 4.1], [0.2, 1.0]])\n",
    "Ur = np.array([[1.3, 7.1], [9.1, 4.5]])\n",
    "\n",
    "w = np.array([2.0, 4.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement the forward pass like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = []\n",
    "h_ = []\n",
    "z = []\n",
    "r = []\n",
    "y_ = []\n",
    "\n",
    "def GRU_forward():\n",
    "    \"\"\"Perform forward pass.\"\"\"\n",
    "    h.append(np.zeros_like(Wh))\n",
    "    \n",
    "    for t, xt in enumerate(x, start=len(y_)):\n",
    "        # Calculate values of the gates\n",
    "        zt = sigmoid(Wz.dot(xt) + Uz.dot(h[t]))\n",
    "        rt = sigmoid(Wr.dot(xt) + Ur.dot(h[t]))\n",
    "\n",
    "        # Calculate candidate update\n",
    "        h_t = tanh(Wh.dot(xt) + Uh.dot(rt * h[t]))\n",
    "\n",
    "        # Calculate hidden state\n",
    "        ht = (1-zt) * h[t] + zt * h_t\n",
    "\n",
    "        # Calculate prediction at step t\n",
    "        y_t = w.dot(ht)\n",
    "\n",
    "        # Save variables to container\n",
    "        h.append(ht)\n",
    "        h_.append(h_t)\n",
    "        z.append(zt)\n",
    "        r.append(rt)\n",
    "        y_.append(y_t)\n",
    "    \n",
    "    return y_[-1]\n",
    "\n",
    "yhat_before = GRU_forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we want to keep track of the prediction before updating the weights, such that we can compare it with the prediction we obtain after we update our weight matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights for the candidate $\\tilde{h}_t$\n",
    "\n",
    "Let's start with taking partial derivatives of $\\hat{y}_t$ with respect to the elemens of $\\Wh$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\hat{y}_t}{\\partial \\wh{1}} = w_1 \\z{1} \\gfull[\\prime]{1} \\color{green}{x_t}\n",
    "\\end{equation}\n",
    "\n",
    "Which is just an application of the chain-rule for derivatives. Similarly we get\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\hat{y}_t}{\\partial \\wh{2}} = w_2 \\z{2} \\gfull[\\prime]{2} \\color{green}{x_t}.\n",
    "\\end{equation}\n",
    "\n",
    "We can obtain the gradient by combining the two partial derivatives in the following manner\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\hat{y}_t}{\\partial \\Wh} &= \n",
    "    \\begin{bmatrix} \n",
    "        \\frac{\\partial \\hat{y}_t}{\\partial \\wh{1}} \\\\ \\frac{\\partial \\hat{y}_t}{\\partial \\wh{2}}  \n",
    "    \\end{bmatrix}                       =\n",
    "    \\begin{bmatrix} \n",
    "        w_1 \\z{1} \\gfull{1} \\color{green}{x_t} \\\\\n",
    "        w_2 \\z{2} \\gfull{2} \\color{green}{x_t}\n",
    "    \\end{bmatrix} \\\\\n",
    "                                        &= W * \\Z * \\Gfull[\\prime] \\color{green}{x_t}.\n",
    "\\end{align}\n",
    "\n",
    "Additionally, we can make use of the element-wise product in order to obtain a formulation that has a straightforward `numpy` implementation. We get the same result if we deploy the logic of the backpropagation algorithm.\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial {loss}_t}{\\partial W_h} &= \\frac{\\partial {loss}_t}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial \\tilde{h}_t}\\frac{\\partial \\tilde{h}_t}{\\partial \\Wh} \\\\\n",
    "                                   &= -(y-\\hat{y}_t) W * \\Z * \\Gfull[\\prime] \\color{green}{x_t}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdWh = np.zeros_like(Wh)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    # Note that `h` has an entry at start, so indexing at t accesses h_{t-1}\n",
    "    dh_tdWh = dtanh(Wh.dot(xt) + Uh.dot(r[t] * h[t])) * xt\n",
    "    dLossdWh += -(y-y_[t]) * w * z[t] * dh_tdWh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with the partial derivatives of $\\hat{y}_t$ with respect to the elements of $\\Uh$\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\hat{y}_t}{\\partial \\uh{1}{1}} &= w_1 \\z{1} \\gfull[\\prime]{1} \\r{1} \\h{1} \\\\\n",
    "\\frac{\\partial \\hat{y}_t}{\\partial \\uh{1}{2}} &= w_1 \\z{1} \\gfull[\\prime]{1} \\r{2} \\h{2} \\\\\n",
    "\\frac{\\partial \\hat{y}_t}{\\partial \\uh{2}{1}} &= w_2 \\z{2} \\gfull[\\prime]{2} \\r{1} \\h{1} \\\\\n",
    "\\frac{\\partial \\hat{y}_t}{\\partial \\uh{2}{2}} &= w_2 \\z{2} \\gfull[\\prime]{2} \\r{2} \\h{2}.\n",
    "\\end{align}\n",
    "\n",
    "These we can combine to form the jacobian of of $\\yhat$ with respect to $\\Uh$\n",
    "\n",
    "\\begin{align}\n",
    "\\deriv{\\yhat}{\\Uh} &= \n",
    "\\begin{bmatrix} \n",
    "    \\deriv{\\yhat}{\\uh{1}{1}} & \\deriv{\\yhat}{\\uh{1}{2}} \\\\\n",
    "    \\deriv{\\yhat}{\\uh{2}{1}} & \\deriv{\\yhat}{\\uh{2}{2}}\n",
    "\\end{bmatrix}      = \n",
    "\\begin{bmatrix}\n",
    "w_1 \\z{1} \\gfull[\\prime]{1} \\r{1} \\h{1} &\n",
    "w_1 \\z{1} \\gfull[\\prime]{1} \\r{2} \\h{2} \\\\\n",
    "w_2 \\z{2} \\gfull[\\prime]{2} \\r{1} \\h{1} &\n",
    "w_2 \\z{2} \\gfull[\\prime]{2} \\r{2} \\h{2}.\n",
    "\\end{bmatrix} \\\\\n",
    "                  &=\n",
    "\\begin{bmatrix} w_1 & w_1 \\\\ w_2 & w_2 \\end{bmatrix} * \n",
    "\\begin{bmatrix} \\z{1} & \\z{1} \\\\ \\z{2} & \\z{2} \\end{bmatrix} * \n",
    "\\begin{bmatrix} \\gfull[\\prime]{1} & \\gfull[\\prime]{1} \\\\ \\gfull[\\prime]{2} & \\gfull[\\prime]{2} \\end{bmatrix} * \n",
    "\\begin{bmatrix} \\r{1} & \\r{2} \\\\ \\r{1} & \\r{2} \\end{bmatrix} * \n",
    "\\begin{bmatrix} \\h{1} & \\h{2} \\\\ \\h{1} & \\h{2} \\end{bmatrix} \\\\\n",
    "                  &=\n",
    "\\begin{bmatrix} w & w \\end{bmatrix} * \\begin{bmatrix} \\Z & \\Z \\end{bmatrix} * \\begin{bmatrix} \\Gfull[\\prime] & \\Gfull[\\prime] \\end{bmatrix} * \\begin{bmatrix} \\R^T \\\\ \\R^T \\end{bmatrix} * \\begin{bmatrix} \\H^T \\\\ \\H^T \\end{bmatrix} \\\\\n",
    "&= \\diag(w) \\cdot \\diag(\\Z) \\cdot \\diag(\\Gfull[\\prime]) \\cdot \\mathbb{1} \\mathbb{1}^T \\cdot \\diag(\\R) \\cdot \\diag(\\H)\n",
    "\\end{align}\n",
    "\n",
    "Again, following the logic of the backpropagation algorithm, we obtain the same result\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial {loss}_t}{\\partial \\Uh} &= \\frac{\\partial {loss}_t}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial \\tilde{h}_t}\\frac{\\partial \\tilde{h}_t}{\\partial \\Uh} \\\\\n",
    "                                    &= -(y-\\hat{y}_t)\\ \\diag \\left[ w * \\Z * \\Gfull[\\prime]\\right] \\cdot  \\begin{bmatrix} (\\R * \\H)^T \\\\ (\\R * \\H)^T \\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdUh = np.zeros_like(Uh)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dy_dh_t = w * z[t] * dtanh(Wh.dot(xt) + Uh.dot(r[t] * h[t]))\n",
    "    dLossdUh += -(y-y_[t]) * dy_dh_t.reshape(-1, 1) * h[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights for the update gate $z_t$\n",
    "\n",
    "Using the same logic, we calculate the partial derivatives of $\\yhat$ with respect to the elements of $\\Wz$, which gives \n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\wz{1}} = w_1 \\left(\\color{red}{-} \\zfull[\\prime]{1}\\right) \\h{1} + \\htil{1} \\zfull[\\prime]{1} \\dxt{red},\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\wz{2}} = w_2 \\left(\\color{red}{-} \\zfull[\\prime]{2}\\right) \\h{1} + \\htil{1} \\zfull[\\prime]{2} \\dxt{red},\n",
    "\\end{equation}\n",
    "\n",
    "and when combined leaves us the gradient of $\\yhat$ with respect to $\\Wz$ as\n",
    "\n",
    "\\begin{align}\n",
    "\\deriv{\\yhat}{\\Wz} &= \\begin{bmatrix}\n",
    "\\deriv{\\yhat}{\\wz{1}} \\\\ \\deriv{\\yhat}{\\wz{2}}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "    w_1 \\left(\\color{red}{-} \\zfull[\\prime]{1}\\right) \\h{1} + \\htil{1} \\zfull[\\prime]{1} \\dxt{red} \\\\\n",
    "    w_2 \\left(\\color{red}{-} \\zfull[\\prime]{2}\\right) \\h{1} + \\htil{1} \\zfull[\\prime]{2} \\dxt{red}\n",
    "\\end{bmatrix} \\\\\n",
    "&= w * \\begin{bmatrix} -\\H + \\Htil \\end{bmatrix} * \\Zfull[\\prime] \\dxt{red}\n",
    "\\end{align}\n",
    "\n",
    "With that knowledge we can implement the gradient of the loss function with respect to $\\Wz$ as \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial {loss}_t}{\\partial W_z} &= \\frac{\\partial {loss}_t}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial z_t}\\frac{\\partial z_t}{\\partial W_z} \\\\\n",
    "                                   &= -(y-\\hat{y}_t)\\ w * \\begin{bmatrix} -\\H + \\Htil \\end{bmatrix} * \\Zfull[\\prime] \\dxt{red}.\n",
    "\\end{align}\n",
    "\n",
    "Which in turn we can translate into `Python` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdWz = np.zeros_like(Wz)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dztdWz = dsigmoid(Wz.dot(xt) + Uz.dot(h[t])) * xt\n",
    "    dLossdWz += -(y-y_[t]) * w * (-h[t+1] - h_[t]) * dztdWz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now this should be getting old. We take partial derivatives of $\\yhat$ with respect to each element of $\\Uz$. This leaves us with\n",
    "\n",
    "\\begin{align}\n",
    "\\deriv{\\yhat}{\\uz{1}{1}} &= w_1 \\left[\\color{red}{(- \\zfull[\\prime]{1} \\h{1} )} \\h{1} + \\htil{1} \\zfull[\\prime]{1} \\h{1} \\right], \\\\\n",
    "\\deriv{\\yhat}{\\uz{1}{2}} &= w_1 \\left[\\color{red}{(- \\zfull[\\prime]{1} \\h{2} )} \\h{1} + \\htil{1} \\zfull[\\prime]{1} \\h{2} \\right], \\\\\n",
    "\\deriv{\\yhat}{\\uz{2}{1}} &= w_2 \\left[\\color{red}{(- \\zfull[\\prime]{2} \\h{1} )} \\h{2} + \\htil{2} \\zfull[\\prime]{2} \\h{1} \\right] \\text{and} \\\\\n",
    "\\deriv{\\yhat}{\\uz{2}{2}} &= w_2 \\left[\\color{red}{(- \\zfull[\\prime]{2} \\h{2} )} \\h{2} + \\htil{2} \\zfull[\\prime]{2} \\h{2} \\right]. \n",
    "\\end{align}\n",
    "\n",
    "We can combine these partial derivatives in order to form the jacobian of $\\yhat$ with respect to $\\Uz$ as\n",
    "\n",
    "\\begin{align}\n",
    "\\deriv{\\yhat}{\\Uz} &= \n",
    "\\begin{bmatrix} \n",
    "    \\deriv{\\yhat}{\\uz{1}{1}} & \\deriv{\\yhat}{\\uz{1}{2}} \\\\\n",
    "    \\deriv{\\yhat}{\\uz{2}{1}} & \\deriv{\\yhat}{\\uz{2}{2}}\n",
    "\\end{bmatrix} \\\\ &=\n",
    "\\begin{bmatrix}  \n",
    "     w_1 \\left[\\color{red}{(- \\zfull[\\prime]{1} \\h{1} )} \\h{1} + \\htil{1} \\zfull[\\prime]{1} \\h{1} \\right] &\n",
    "     w_1 \\left[\\color{red}{(- \\zfull[\\prime]{1} \\h{2} )} \\h{1} + \\htil{1} \\zfull[\\prime]{1} \\h{2} \\right] \\\\\n",
    "     w_2 \\left[\\color{red}{(- \\zfull[\\prime]{2} \\h{1} )} \\h{2} + \\htil{2} \\zfull[\\prime]{2} \\h{1} \\right] &\n",
    "     w_2 \\left[\\color{red}{(- \\zfull[\\prime]{2} \\h{2} )} \\h{2} + \\htil{2} \\zfull[\\prime]{2} \\h{2} \\right] \n",
    "\\end{bmatrix} \\\\ &=\n",
    "\\begin{bmatrix} w & w \\end{bmatrix} * \n",
    "\\begin{bmatrix} \\Htil - \\H & \\Htil - \\H \\end{bmatrix} * \n",
    "\\begin{bmatrix} \\Zfull[\\prime] & \\Zfull[\\prime] \\end{bmatrix} * \n",
    "\\begin{bmatrix} \\H^T \\\\ \\H^T \\end{bmatrix} \\\\\n",
    "&= \\diag(w) \\cdot \\diag(\\Htil - \\H) \\cdot \\Zfull[\\prime] \\cdot \\H^T.\n",
    "\\end{align}\n",
    "\n",
    "And by the same token, we need to use the previous derivations in order to find the derivative of the loss function with respect to $\\Uz$, where now one can easily see that:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial {loss}_t}{\\partial \\Uz} &= \\frac{\\partial loss}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial z_t}\\frac{\\partial z_t}{\\partial \\Uz} \\\\\n",
    "                                   &= -(y-\\hat{y}_t)\\ \\diag \\left[ w * [-\\H + \\Htil] * \\Zfull[\\prime]\\right] \\cdot \\begin{bmatrix} \\H^T \\\\ \\H^T \\end{bmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "Having done that, we can implement it in the now too familiar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdUz = np.zeros_like(Uz)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dy_dzt = w * [-h[t+1] + h_[t] * dsigmoid(Wz.dot(xt) + Uz.dot(h[t]))]\n",
    "    dLossdUz += -(y-y_[t]) * dy_dzt.reshape(-1, 1) * h[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights for the reset gate $r_t$\n",
    "\n",
    "Similarly, we can proceed with obtaining the gradients of $\\yhat$ with respect to the elements of the reset gate.\n",
    "\n",
    "\\begin{align}\n",
    "\\deriv{\\yhat}{\\wr{1}} &= w_1 \\z{1} \\gfull[\\prime]{1} \\uh{1}{1} \\h{1} \\rfull[\\prime]{1} \\dxt{blue} \\\\ \n",
    "&\\quad + w_2 \\z{2} \\gfull[\\prime]{2} \\uh{2}{1} \\h{1} \\rfull[\\prime]{1} \\dxt{blue}\n",
    "\\end{align}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align}\n",
    "\\deriv{\\yhat}{\\wr{2}} &= w_1 \\z{1} \\gfull[\\prime]{1} \\uh{1}{2} \\h{2} \\rfull[\\prime]{2} \\dxt{blue} \\\\ \n",
    "&\\quad + w_2 \\z{2} \\gfull[\\prime]{2} \\uh{2}{2} \\h{2} \\rfull[\\prime]{2} \\dxt{blue}.\n",
    "\\end{align}\n",
    "\n",
    "By collecting the partial derivatives into a vector we form the gradient of $\\yhat$ with respect to $\\Wr$\n",
    "\n",
    "\\begin{align}\n",
    "\\deriv{\\yhat}{\\Wr} &= \\begin{bmatrix}\\deriv{\\yhat}{\\wr{1}} \\\\ \\deriv{\\yhat}{\\wr{2}} \\end{bmatrix} \\\\ \n",
    "&= w^T \\cdot \\left(\n",
    "\\begin{bmatrix} \\Z & \\Z \\end{bmatrix} * \n",
    "\\begin{bmatrix} \\Gfull[\\prime] & \\Gfull[\\prime] \\end{bmatrix} * \n",
    "\\Uh *\n",
    "\\begin{bmatrix} \\H^T \\\\ \\H^T \\end{bmatrix} *\n",
    "\\begin{bmatrix} \\Rfull[\\prime]^T \\\\ \\Rfull[\\prime]^T \\end{bmatrix}\n",
    "\\right) \\dxt{blue} \\\\\n",
    "&= w^T \\cdot \\diag(\\Z) \\cdot \\diag(\\Gfull[\\prime]) \\cdot \\Uh \\cdot \\diag(\\H)\\cdot \\diag(\\Rfull[\\prime]) \\dxt{blue}.\n",
    "\\end{align}\n",
    "\n",
    "Subsequently, when looking at the loss function we see that it's gradient with respect to $\\Wr$ can be computed as\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial {loss}_t}{\\partial \\Wr} &= \\frac{\\partial {loss}_t}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial \\tilde{h}_t}\\frac{\\partial \\tilde{h}_t}{\\partial \\Wr} \\\\\n",
    "                                   &= -(y-\\hat{y}_t)\\ w^T \\cdot \\left(\n",
    "\\begin{bmatrix} \\Z & \\Z \\end{bmatrix} * \n",
    "\\begin{bmatrix} \\Gfull[\\prime] & \\Gfull[\\prime] \\end{bmatrix} * \n",
    "\\Uh *\n",
    "\\begin{bmatrix} \\H^T \\\\ \\H^T \\end{bmatrix} *\n",
    "\\begin{bmatrix} \\Rfull[\\prime]^T \\\\ \\Rfull[\\prime]^T \\end{bmatrix}\n",
    "\\right) \\dxt{blue}.\n",
    "\\end{align}\n",
    "\n",
    "Which we also implement in `Python`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdWr = np.zeros_like(Wr)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    z_dg = z[t] * dtanh(Wh.dot(xt) + Uh.dot(r[t] * h[t]))\n",
    "    d_rt = dsigmoid(Wr.dot(xt) + Ur.dot(h[t]))\n",
    "    dLossdWr += -(y-y_[t]) * w.dot(z_dg.reshape(-1, 1) * Uh * h[t] * d_rt) * xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the partial derivatives of $\\yhat$ with respect to $\\Ur$\n",
    "\n",
    "\\begin{align}\n",
    "\\deriv{\\yhat}{\\ur{1}{1}} &= w_1 \\z{1} \\gfull[\\prime]{1} \\uh{1}{1} \\h{1} \\rfull[\\prime]{1} \\h{1} \\\\ \n",
    "                         & \\quad + w_2 \\z{2} \\gfull[\\prime]{2} \\uh{2}{1} \\h{1} \\rfull[\\prime]{1} \\h{1} \\\\\n",
    "\\deriv{\\yhat}{\\ur{1}{2}} &= w_1 \\z{1} \\gfull[\\prime]{1} \\uh{1}{1} \\h{1} \\rfull[\\prime]{1} \\h{2} \\\\\n",
    "                         & \\quad + w_2 \\z{2} \\gfull[\\prime]{2} \\uh{2}{1} \\h{1} \\rfull[\\prime]{1} \\h{2} \\\\\n",
    "\\deriv{\\yhat}{\\ur{2}{1}} &= w_1 \\z{1} \\gfull[\\prime]{1} \\uh{1}{2} \\h{2} \\rfull[\\prime]{2} \\h{1} \\\\\n",
    "                         & \\quad + w_2 \\z{2} \\gfull[\\prime]{2} \\uh{2}{2} \\h{2} \\rfull[\\prime]{2} \\h{1} \\\\\n",
    "\\deriv{\\yhat}{\\ur{2}{2}} &= w_1 \\z{1} \\gfull[\\prime]{1} \\uh{1}{2} \\h{2} \\rfull[\\prime]{2} \\h{2} \\\\\n",
    "                         & \\quad + w_2 \\z{2} \\gfull[\\prime]{2} \\uh{2}{2} \\h{2} \\rfull[\\prime]{2} \\h{2}.\n",
    "\\end{align}\n",
    "\n",
    "And by combining all partial derivatives, we obtain the jacobian of $\\yhat$ with respect to $\\Ur$\n",
    "\n",
    "\\begin{align}\n",
    "\\deriv{\\yhat}{\\Ur} &=\n",
    "\\begin{bmatrix} \n",
    "    \\deriv{\\yhat}{\\ur{1}{1}} & \\deriv{\\yhat}{\\ur{1}{2}} \\\\\n",
    "    \\deriv{\\yhat}{\\ur{2}{1}} & \\deriv{\\yhat}{\\ur{2}{2}}\n",
    "\\end{bmatrix} \\\\ &= \n",
    "\\begin{bmatrix}(w * \\Z * \\Gfull[\\prime])^T \\\\ (w * \\Z * \\Gfull[\\prime])^T \\end{bmatrix} \\cdot \\Uh^T  * \n",
    "\\begin{bmatrix} \\H \\cdot \\H^T \\end{bmatrix} *\n",
    "\\begin{bmatrix} \\Rfull[\\prime] & \\Rfull[\\prime] \\end{bmatrix} \\\\ &=\n",
    "\\diag(w) \\cdot \\diag(\\Z) \\cdot \\diag(\\Gfull[\\prime]) \\cdot \\Uh^T \\cdot \\diag(\\H) \\cdot \\diag(\\Rfull[\\prime]) \\cdot \\mathbb{1}\\mathbb{1}^T \\cdot \\diag(\\H).\n",
    "\\end{align}\n",
    "\n",
    "We implement this gradient in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdUr = np.zeros_like(Ur)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    w_z_dg = w * z[t] * dtanh(Wh.dot(xt) + Uh.dot(r[t] * h[t]))\n",
    "    outer_h = np.outer(h[t], h[t])\n",
    "    dr = dsigmoid(Wr.dot(xt)+Ur.dot(h[t])).reshape(-1, 1)\n",
    "\n",
    "    dLossdUr += -(y - y_[t]) * w_z_dg.dot(Uh.T)*outer_h * dr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update the weights\n",
    "\n",
    "Now that we have computed the gradients of the loss function with respect to any of the weight matrices, we can update the weights and then see if this did indeed improve the prediction of our toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1\n",
    "Wr -= eta * dLossdWr\n",
    "Wz -= eta * dLossdWz\n",
    "Wh -= eta * dLossdWh\n",
    "\n",
    "Ur -= eta * dLossdUr\n",
    "Uz -= eta * dLossdUz\n",
    "Uh -= eta * dLossdUh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform the forward pass again with the updated weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_after = GRU_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$\\hat{y}_{before}$</th>\n",
       "      <th>$\\hat{y}_{after}$</th>\n",
       "      <th>$y$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.14414</td>\n",
       "      <td>5.986334</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   $\\hat{y}_{before}$  $\\hat{y}_{after}$  $y$\n",
       "1             5.14414           5.986334  7.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({r\"$\\hat{y}_{before}$\": yhat_before, r\"$\\hat{y}_{after}$\": yhat_after, r\"$y$\": y}, index=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed, we are better than before. If we were to reiterate these steps multiple times we could get arbitrarily close to the true value of $7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infosys",
   "language": "python",
   "name": "infosys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
