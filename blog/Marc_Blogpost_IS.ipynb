{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Recurrent Neural Networks with Long-Short-Term Memory and Gated Recurrent Unit to predict reported Crime Incidents\n",
    "\n",
    "<img align=\"right\" src=\"pics/crime_intro.png\" alt=\"NN\" style=\"width: 500px;\"/>\n",
    "Several police departments across the Unites States have been experimenting with software for crime prdiction. This  started a controversial debate: Critics are questioning the predictiv power of the underlying machine learning models and point out biases towards certain crime typs and neighborhoods. We took this as occacion to look into the publicly available <a href=\"https://catalog.data.gov/dataset/crimes-2001-to-present-398a4\">crime records of the city of chicago</a>. The data set contains close to 7 milliom reported crime incidences starting from 2001 until the precceding week of the data download. It includes 22 columns with detailed information, such as type of the offence,  geo location,  district and the time of crime occurence. We will focus on temporal patterns in the data by predicting crime frequencies per day on the district level. Such a model may be used in practice to faciliate personel planning across the districts for the chigaco police force. We generate the input data for model building by counting the reported crimes by day and district. This results in a table with three columns containing date, district and the respective counts. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>District</th>\n",
       "      <th>Date</th>\n",
       "      <th>Incidents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2001-01-02</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2001-01-03</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2001-01-04</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2001-01-05</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   District        Date  Incidents\n",
       "0       1.0  2001-01-01       37.0\n",
       "1       1.0  2001-01-02       44.0\n",
       "2       1.0  2001-01-03       44.0\n",
       "3       1.0  2001-01-04       41.0\n",
       "4       1.0  2001-01-05       67.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../data/crimes_district.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing this aggregation on the time or the locational level a similar model for more granular predictions might be build. This is under the assumption of enough representative data on a more detailed aggregation level. We will take the described use case as an example to introduce Recurrent Neural Networks (RNNs) with Long-Short-Term memory (LSTM) and Gated Recurrent Unit (GRU). First we explain the general concept behind RNNs and train a simple network using different input formats. We then introduce RNNs with LSTM and GRU to overcome the vanishing gradient problem. After explaining the forward pass and the derivation of LSTM and GRU we tune the hyperparameters for the three introduced RNN variants and compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Recurrent Neural Networks\n",
    "\n",
    "Many machine learning tasks are based on the assumption of identically independent distributed (i.i.d.) samples. Thus a prediction for a label $y_i$ only depends on the corresponding feature vector $\\vec{x_i}$, where $i$ denotes the index of the sample. When working with sequential data, one typically wants to predict a future outcome $\\vec{y_{t+1}}$, which often equals the following observable vector $\\vec{x_{t+1}}$, from a sequence of previously observed vectors $\\vec{x_t}$, …, $\\vec{x_{t-T}}$. $t$ denotes the last observed point of time and $T$  the number of observations since the start of the sequence.\n",
    "While the vectors within a sequence are assumed to be dependent, we might still work with multiple mutually independent time series in a dataset. In this case one could use de notation $\\vec{x_{ti}}$, corresponding to the observed vector at time $t$ from the sample sequence $i$. For simplicity we omit the sample index below. Note that many sequential prediction tasks are based on one dimensional time series, in that case one may also omit the vector notation. However, we stick to the more general form. <br>\n",
    "A simple feedforward network may be used to make predictions under the i.i.d assumption by passing one feature vector $\\vec{x_i}$ at a time through the network. However, if one wants to account for temporal dependencies between the observed vectors $\\vec{x_t}$, …, $\\vec{x_{t-T}}$ the network needs some kind of memory. That’s the concept behind recurrent neural networks: They use hidden activations, referred to as hidden state, to carry information within the network from one observation to the following one.\n",
    "\n",
    "\n",
    "<img align=\"center\" src=\"pics/RNN_t2.png\" alt=\"NN\" style=\"width: 500px;\"/>\n",
    "<p style='text-align: center;'>Image 1: Recurrent Neural Network with a sequence length of two.</p>\n",
    "\n",
    "Image 1 illustrates how a simple RNN may be used to predict $\\vec{y_{t+1}} (:= \\vec{x_{t+1})} $ given a sequence of two observed vectors $\\vec{x_t}$ and $\\vec{x_{t-1}}$. The rectangles represent the input vectors. Each circle represents a vector of hidden activations and the triangle represents the output i.e. the predicted label. The arrows illustrate layer operations, i.e. the multiplication of the previous vector with one of the weight matrices $W_{in}$, $W_h$, or $W_{out}$, denoted by different colours. To make a prediction for $\\vec{y_{t+1}}$, the first vector in the sequence $\\vec{x_{t-1}}$ gets multiplied with the input weights $W_{in}$ and the activation function is applied to produce the first hidden state $\\vec{h_{t-1}}$. Since $\\vec{x_{t-1}}$ is the first observation in the sequence, $\\vec{h_{t-1}}$ represents all available information at the time step $t-1$. To carry this information to the next hidden state $\\vec{h_t}$, $\\vec{h_{t-1}}$ gets multiplied with the hidden weight matrix $W_h$. Consecutively the new input $\\vec{x_t}$ is multiplied with the input matrix $W_{in}$. The resulting vector carries the new information extracted from $\\vec{x_t}$. This newly available information needs to be combined with the knowledge about previous observations to create $\\vec{h_t}$. This can be done by summing up the two outputs from the matrix products and the bias $b_h$ and applying the  hyperbolic tangents as activation function to the vector sum (Equation 1). This is illustrated by the intersect of the green and the orange arrow. Conceptually the weights in $W_h$ represent the importance of previous observations and the weights in $W_{in}$ represent the importance of new information for the prediction task.\n",
    "All available information at the time step $t$ is now aggregated in $\\vec{h_t}$. To make a prediction for $\\vec{y_{t+1}}$ it gets multiplied with the output weight matrix $W_{out}$ (Eqation 2). The size of $W_{out}$ and the corresponding activation function are, as usually, defined by the prediction task (regression or classification). This steps cover the forward pass in a simple RNN.\n",
    "<br>\n",
    "<br>\n",
    "$\\vec{h_t} = tanh(W_{in} * \\vec{h_{t-1}} +  \\vec{x_{t}} +\\vec{b_{h}} )$  (Eqation 1)\n",
    "<br>\n",
    "$\\vec{y_{t+1}} = ReLu(W_{out} * \\vec{h_t} + \\vec{b_{out}} )$ (Eqation 2)\n",
    "<br>\n",
    "<br>\n",
    "If we want to look back more time steps when making a prediction for $\\vec{y_{t+1}}$, we just need to add another hidden state and another input for every additional time step. Image 3 illustrates how this may look for a sequence of length three. Interestingly, even though RNNs can get quite deep along the time dimension, they have relatively few parameters. This is the case because all inputs or hidden states represent the same thing, another time step in the series or another aggregation of past observations. Thus, we only have to train three different weight matrices, one for the input, one for the hidden state and one for the output.<br>\n",
    "\n",
    "<img  src=\"pics/RNN_t3.png\" alt=\"NN\" style=\"width: 550px;\"/>\n",
    "<p style='text-align: center;'>Image 2: Recurrent neural network with a sequence length of three.</p>\n",
    "\n",
    "Drawing RNNs with more time steps becomes cumbersome. We have to keep adding inputs and hidden states. The same holds true if we would like to construct them in in python. A more elegant and flexible representation is given in Image 4. Compared to Image 3 the repetitive elements have been put into a loop. For every time step in the sequence length $T$, the new input is feed into the network and combined with the previous hidden state. The new hidden state is passed back to the start of the loop as input to the next iteration. The loop stops at the end of the sequence and the last hidden state is put through the output layer. To include the first timestep within the loop representation, an initial hidden state $\\vec{h_0}$ is defined as a vector of zeros.\n",
    "\n",
    "\n",
    "<img  src=\"pics/RNN_Loop.png\" alt=\"NN\" style=\"width: 350px;\"/>\n",
    "<p style='text-align: center;'>Image 3: Recurrent neural network with a loop and a sequence length of $T$.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a predefined model layer that constructs the described RNN behind the scenes. However, to emphasise the described operations, we demonstrate how to construct it by yourself using fully connected layers and the keras functional API. In keras a layer refers to a set of layer operations, typically a matrix multiplication and the application of an activation function, that output a new vector of activations. We follow this notation below.<br>\n",
    "The functional  API allows to define a list of inputs and put them through several layers to produce a (list of) output(s). It then connects inputs and output(s) so the keras backend can be used to derive the gradients and train the model. We start implementing the RNN by initializing the `input_list`. The basic properties of a simple RNN are defined by the number of the hidden activations `H_SZ` , the sequence length `SEQ_LEN` and the number of features `FEAT_SZ` per time step. By choosing `H_SZ`, the shape of the three weight matrices is sufficiently defined: The second and first dimension of $W_{in}$ and $W_{out}$ are respectively defined by the shape of the input data and the labels. Moreover, the first dimension of $W_{in}$ must match the first dimension of $W_h$, so their outputs can be added together. $W_h$ must be squared because the result of the matrix product $W_h$ x $h_t$ gets again multiplied with $W_h$. Since $h_t$ gets also multiplied by $W_{out}$ the second dimension of $W_{out}$ must also be `H_SZ`. We set `H_SZ` to five and `SEQ_LEN` to ten after little manual tuning. In general, the parameters did not seem to have a great impact on the model performance. Since the crime data consists of one-dimensional time series `FEAT_SZ` is given as one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras import metrics\n",
    "\n",
    "from keras.layers import Input, Dense, add, Activation, SimpleRNN, TimeDistributed\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "\n",
    "# create list to keep track of inputs\n",
    "input_list = []\n",
    "\n",
    "# initialize RNN properties\n",
    "H_SZ = 5\n",
    "SEQ_LEN = 10\n",
    "FEAT_SZ = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dense()` layer in keras holds the weights of a fully connected layer. It is initialized by passing the output dimension to the `Dense()` class. The input dimension will be interfered from the first input. After initialization, it can be called on a tensor which will return the resulting output activations. The three layers of the network are initialized by passing the respective number of output activations `H_SZ`, `H_SZ` and `FEAT_SZ` to the `Dense()` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilize fully connected layers and the activation function of the hidden state\n",
    "first_layer = Dense(H_SZ, name = 'first_layer')\n",
    "hidden_layer = Dense(H_SZ , name = 'hidden_layer')\n",
    "tanh = Activation('tanh')\n",
    "output_layer = Dense(FEAT_SZ, activation='relu', name = 'output_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  loop through the sequence length and create for every time step a new input `x` and append it to the input list. The input activations `act_in` are calculated for every time step by putting the input variable `x` through the first layer. We additionally initialize `h0` as a vector of zeros with the same (variable) shape as the input activations at the first time step. The new hidden state `ht` gets updated implementing Equation 1. After the loop the final hidden state is put through the output layer to make a prediction. Finally the functional API `Model()` connects input and output variables to create `myRNN`. Since keras typically works with tensors a function that splits a three-dimensional tensor along the second dimension, which corresponds to time dimension in keras, is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 567,
     "status": "ok",
     "timestamp": 1549013725328,
     "user": {
      "displayName": "Marc Scheu",
      "photoUrl": "",
      "userId": "04250369743767764432"
     },
     "user_tz": -60
    },
    "id": "rrej41PP2IyI",
    "outputId": "7d3eea3a-6d1c-43ed-8e5e-2bea1b7ac509",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(SEQ_LEN):\n",
    "  \n",
    "  # get input vector and append it to input list\n",
    "  x = Input(shape = (FEAT_SZ,))\n",
    "  input_list.append(x)\n",
    "  \n",
    "  # calculate input actiavtions\n",
    "  act_in = first_layer(x)\n",
    "  \n",
    "  if(i == 0):\n",
    "  # initialize h0 with zeros and append it to input list\n",
    "    h0 = K.zeros_like(act_in)\n",
    "    input_list.append(h0)  \n",
    "    ht = Input(tensor = h0, name = 'h0')\n",
    "  \n",
    "  # calculate hidden activations\n",
    "  ht = tanh(add([hidden_layer(ht), act_in])) \n",
    "  \n",
    "# calculate output\n",
    "out = output_layer(ht)\n",
    "\n",
    "myRNN = Model(inputs=input_list, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a_FPjJwKfAiK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_input(X):\n",
    "  # slice 3-dim tensor along 2nd axis into list of inputs\n",
    "  return([X[:,i,:] for i in range(X.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1NalVgUG_tCe"
   },
   "source": [
    "### Formatting sequential data\n",
    "\n",
    "We load the reported crimes, counte by district and day and change the long-format in wide-format to get a matrix of 24 districts by 6513 timsteps.\n",
    "By training one model on the time series from all districts, we assume each series to be a different realization of the same underlying sequential pattern. An obvious alternative would be to train a separate model for each district. \n",
    "Before further formatting we split the data into a train and a validation set. The validation set should resemble the data, the model would see in production. We therefore use the data from 2015 - 2017 as validation set. We will later use the most resent crime counts from 2017 as test set.\n",
    "One can not simply train a neural network on 24 long time series.\n",
    "It is common practice to split long time series into sub-sequences, treating them as independent samples. The simplest way to do this is, to slide a window of length `SEQ_LEN` through each series and treat each subseries contained in the window as one sample. The direct successor after the window defines the respective label for the sub-series (Image 4). \n",
    "Note that we will use the terms sample sequence and sub-sequence to a certain degree interchangable. Sample is used refering to the sequential counts in a row of the data matrix used for model training. Sub-sequence also refers to such a row, but it shall emphasis the row beeing a sub-sequence of a longer time series over the whole observation period.\n",
    "<img align=\"center\" src=\"pics/Format1.png\" alt=\"NN\" style=\"width: 500px;\"/>\n",
    "<p style='text-align: center;'>Image 4: Formatting data input for recurrent neural networks.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_seqs = data.pivot(index='District', columns='Date', values='Incidents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.to_datetime(full_seqs.columns)\n",
    "\n",
    "lower = pd.to_datetime(\"2015-01-01\")\n",
    "upper = pd.to_datetime(\"2017-01-01\")\n",
    "end = pd.to_datetime(\"2018-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24, 5113), (24, 731))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_idx = full_seqs.columns[dates < lower]\n",
    "val_idx = full_seqs.columns[(dates >=lower) == (dates < upper)]\n",
    "\n",
    "full_seqs_tr = full_seqs[tr_idx].values\n",
    "full_seqs_val = full_seqs[val_idx].values\n",
    "\n",
    "full_seqs_tr.shape, full_seqs_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the sliding window by looping through the 24 full sequences and use array slicing withn a list comprehension to create the sub-sequences. The resulting sub-sequences are stacked vertically. Recurrent layers in keras and `MyRNN` above expect an input tensor of the dimensions `[batch_size, timesteps, input_dim]`. Since we work with one dimensional series, we have ignored the third dimension so far. In order to match the required input shape a thrid dimension of size one is added to the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_sequences(long_seqs, SEQ_LEN):\n",
    "  # input matrix of long sequences [sequences, timesteps]\n",
    "  # returns matrix of subsequences X [subsequences, SEQ_LEN, 1], and labels y[subsequences,1]\n",
    "  X = []\n",
    "  y = []\n",
    "\n",
    "  for long_seq in long_seqs:\n",
    "    n = long_seq.shape[0]\n",
    "    # slide window of SEQ_LEN over full sequence\n",
    "    seq_stacked = np.stack([long_seq[i:i+SEQ_LEN] for i in range(n-SEQ_LEN)])\n",
    "    labels = np.array([long_seq[i] for i in np.arange(SEQ_LEN, n)]).reshape(-1,1)\n",
    "    X.append(seq_stacked)\n",
    "    y.append(labels)\n",
    "\n",
    "  X =  np.vstack(X)\n",
    "  #add axis for number of features per time step = 1  \n",
    "  X = X[:,:,np.newaxis]\n",
    "  y =  np.vstack(y)\n",
    "  \n",
    "  return(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQaphkyk39Du",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((122472, 10, 1), (122472, 1), (17304, 10, 1), (17304, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# cut sequences in sub sequences of length SEQ_LEN\n",
    "X_tr, y_tr = cut_sequences(full_seqs_tr, SEQ_LEN)\n",
    "X_val, y_val = cut_sequences(full_seqs_val, SEQ_LEN)\n",
    "\n",
    "X_tr.shape, y_tr.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X80CGhAGkULk"
   },
   "source": [
    "### Evaluation metric and benchmark\n",
    "Before training a model one should consider how the prediction error may be evaluated w.r.t. the application objective. We use the mean squared error (MSE) as loss function, which is common for regression tasks. \n",
    "For user of the model, MSE is hard to interpret in the context of the model application. The mean absolute error (MAE) seems more interpretable: In combination with average crime counts it provides some intuition about the uncertainty included in model predictions. To get a basic feeling for model performance while building the model we establish as naive benchmark: We calculate the MAE using the last observed crime count as prediction for the succeeding crime count. This results in a MAE of 6.65.  Given the mean of 46 crimes per day and district this does not seem to bad for  a simple heuristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nJrYaadcgSNB",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average daily crime count by district: 46\n",
      "MAE predict last value: 6.652\n"
     ]
    }
   ],
   "source": [
    "# naive benchmark: predict last observed count\n",
    "AVG = y_tr.mean().round(3)\n",
    "y_true = full_seqs_val[:,1:]\n",
    "preds = full_seqs_val[:,:-1]\n",
    "\n",
    "MAE_last_val = np.abs(preds - y_true ).mean().round(3)\n",
    "\n",
    "print(\"Average daily crime count by district: \" + str(int(AVG)))\n",
    "print(\"MAE predict last value: \" + str(MAE_last_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Like other neural networks, RNNs are trained trough back propagation. To derive the gradient for an RNN one must calculate the impact on the loss considering all inputs back to the first time step. This is called backpropagation through time (bptt). We look in more detail into the derivation when introducing the vanishing gradient problem and deriving the gradient for RNNs with Long-Short-Term Memory. Here we use the keras `.compile` method to pass the loss function, the optimizer and MAE as evaluation metric to `myRNN`.  <br>\n",
    "We then train `myRNN`on the described input format. Subsequently we demonstrate a different data format using input and output sequences. We train two new models, one with and one without the `stateful` property. Subsequently we compare their training time and behaviour. Therfore we fit each model for 50 epochs, they converge relativ quickly, and save the results to a `Histroy` object. We focus on the concepts behind the models and their implementation. To keep things simple we use the same hyperparameter set for every model and briefly evaluate the MAE on the validation set over different epochs. We will look into more detail into hyperparameter tuning after introducing more complex RNNs with LSTM and GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_SZ = 30\n",
    "EP =  50\n",
    "\n",
    "myRNN.compile(loss='mean_squared_error', optimizer='adam', metrics = ['mae'])\n",
    "myRNN_hist = myRNN.fit(get_input(X_tr), y_tr, batch_size = B_SZ, epochs = EP , validation_data = (get_input(X_val), y_val), verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0mBIylkIPNJm"
   },
   "source": [
    "### Alternative formatting with prediction sequence\n",
    "\n",
    "The way we created sample sequences so far, seems quite inefficient in terms of data usage: By sliding a window one step at time through the sequences we create mostly redundant sample sequences. A more efficient way of presenting data to the model is illustrated in Image 6. Instead of changing one value in each sample sequence, they are now defined by disjunct sub-sequences of lenght `SEQ_LEN`. Instead of predicting one label after observing a complete sub-sequence, we assigne a corresponding label sequences of the same length to each sample sequence. Each label sequence starts one time step ahead of the sample sequence.\n",
    "The remaining values, i.e. the remainder of the full sequence length divided by `SEQ_LEN`, are dropped. This reduced the size of the data set by a factor of `SEQ_LEN`, without losing any information compared to the previous formatting.\n",
    "<img align=\"center\" src=\"pics/Format2.png\" alt=\"NN\" style=\"width: 500px;\"/>\n",
    "<p style='text-align: center;'> Image 5: Input format with prediction sequence.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r7bZsiAXfMv-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cut_sequence_return_state(long_seqs, SEQ_LEN, cut_seq_start = True):\n",
    "  long_seqs_X = long_seqs[:,:-1]\n",
    "  long_seqs_y = long_seqs[:,1:]\n",
    "  if(cut_seq_start):\n",
    "    start = long_seqs_X.shape[1] % SEQ_LEN\n",
    "    X = [long_seqs_X[:,i:i+SEQ_LEN] for i in np.arange(start,long_seqs_X.shape[1],SEQ_LEN)]\n",
    "    y = [long_seqs_y[:,i:i+SEQ_LEN] for i in np.arange(start,long_seqs_y.shape[1],SEQ_LEN)]\n",
    "  else:\n",
    "    start = 0\n",
    "    X = [long_seqs_X[:,i:i+SEQ_LEN] for i in np.arange(start,long_seqs_X.shape[1],SEQ_LEN)]\n",
    "    y = [long_seqs_y[:,i:i+SEQ_LEN] for i in np.arange(start,long_seqs_y.shape[1],SEQ_LEN)]\n",
    "    if X[-1].shape[1] != SEQ_LEN:\n",
    "      X.pop()\n",
    "      y.pop()\n",
    "    \n",
    "  X = np.vstack(X)[:,:,np.newaxis]\n",
    "  y = np.vstack(y)[:,:,np.newaxis]\n",
    "  return X, y\n",
    "\n",
    "X_tr, y_tr = cut_sequence_return_state(full_seqs_tr, SEQ_LEN)\n",
    "X_val, y_val = cut_sequence_return_state(full_seqs_val, SEQ_LEN, cut_seq_start = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now replace `myRNN` by the keras `SimpleRNN()` layer in combination with a `Dense()` output layer. While this implements the same behaviour as `myRNN` it allows for additional functionality. By setting the `return_sequence` parameter to true it passes all hidden states, not only the last one, to the following layer. In the `myRRN` code above, this could have been implemented, by appending all hidden states to a list instead of updating the variable `ht`. The hidden states within the list are then passed through the output layer. The outputlayer returns for every time step in this hidden state sequence a prediction for the following time step in the label sequence. To implement this above, the output layer may be included in de loop (Image 3). The predictions then need to be save in a new output list. In keras this can be done more conviniently, by wrapping the output layer in the `TimeDistributed()` Wrapper. Image 9 shows the unrolled illustration of an RNN predicting sequences.\n",
    "<img align=\"center\" src=\"pics/RNN_Ret_Seq.png\" alt=\"NN\" style=\"width: 500px;\"/>\n",
    "<p style='text-align: center;'> Image 6: Recurrent neural network with return sequence</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple RNN with return sequence\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(H_SZ, input_shape=(SEQ_LEN,FEAT_SZ), return_sequences=True) )\n",
    "model.add(TimeDistributed(Dense(FEAT_SZ)))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics = ['mae'])\n",
    "seqRNN_hist = model.fit(X_tr, y_tr, batch_size = B_SZ,  epochs = EP, validation_data = (X_val, y_val), verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stateful recurrent neural networks\n",
    "When comparing Image 6 and Image 2 you may notice a disadvatage of the return sequence formatting: In both cases, when making a prediction for $y_{t+1}$ the network has seen the inputs $x_{t-2}$, $x_{t-1}$ and $x_{t}$. When using a return sequences to make a prediction for $y_{t-1}$ in Image 6 the only network input is $x_{t-2}$. The hidden state has been initialized with zeros. Therefore we might suspect the error for the predictions on the beginning of the sequence to be higher than at the end. This problem occurs because we need to cut the full sequences into sub-sequences treated as independet samples. One way to avoid that is by saving the last hidden state of one sub-sequence and use it as inizialization of $h_{t}$ when passing the following subsequence (from the initial full sequence) through the network. In keras this can be done by setting the `stateful` parameter of the RNN to true. To memorize which subsequences belongt to the same district the batch size is used. It needs to be set equal to the number of independen full series, e.g here the number of districts. Note that the return sequence formatting is ordered by time and district: The first sub-sequence of the first district is in the first row and the first sub-sequence of the second district in the second row. The 25th row contains the second sub-sequence of the first district and so on. The samples within on batch get passed through the network in paralell. Thus the last hidden state of one row index in a batch is used to initialize the initial hidden state of the same row index in the next batch. When calling the `.fit` method keras shuffles the rows in the training data by defaul at the beginning of every training epoch. To make use of the stateful property the `shuffle` is set to false. Moreover the hidden states needs to be reset after every epoch. Otherwise the hidden state of the last time step of the full sequence would be passed on to the first time step at the next epoch. To do this we train the network in a for loop one epoch at a time and reset the state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PNEw5TKBiQj1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stateful RNN\n",
    "B_SZ = 24\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(H_SZ, batch_input_shape=(B_SZ, SEQ_LEN, FEAT_SZ), return_sequences=True, stateful=True))\n",
    "model.add(TimeDistributed(Dense(FEAT_SZ)))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateful_val_mae = []\n",
    "stateful_loss = []\n",
    "stateful_val_loss = []\n",
    "\n",
    "for i in range(EP):\n",
    "    stateRNN_hist = model.fit(X_tr, y_tr, batch_size = B_SZ, epochs = 1, validation_data = (X_val, y_val), verbose = 0, shuffle=False)\n",
    "    model.reset_states()\n",
    "    stateful_val_mae.append(stateRNN_hist.history['val_mean_absolute_error'])\n",
    "    stateful_loss.append(stateRNN_hist.history['loss'])\n",
    "    stateful_val_loss.append(stateRNN_hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing training behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a stateful RNN it is important to understand that the value of the last state is simply copied to the initial hidden state of the next subsequence. This may improve the prediction error, but when deriving the gradient, they are treated as constant. The inputs considered when updating the weights are still restricted to the sub-sequence length. Therefore, [some practitioners](https://fairyonice.github.io/Stateful-LSTM-model-training-in-Keras.html) argue that \"unstateful” RNNs achieve in many application better results. Reasons for this include the loss of randomness by stopping to shuffle the data between epochs and the fixed batch size, which is a parameter to which RNNs might be sensitive.\n",
    "[Brownlee](https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/) gives as a rule of thumb to use stateful RNNs when the output mainly depends on the occurence of a certain input. This may be the case in many natural language tasks. If the outputs represent a complex function of the previous time steps, increasing the subsequence length may be necessary instead. We will evaluate the behaviour of the MAE for the three models on the validation set over the training epochs. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.repeat(MAE_last_val,50))\n",
    "plt.plot(myRNN_hist.history['val_mean_absolute_error'])\n",
    "plt.plot(seqRNN_hist.history['val_mean_absolute_error'])\n",
    "plt.plot(stateful_val_mae)\n",
    "\n",
    "plt.title('Mean absolut error on validation set')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('epoch')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([4,20])\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 6) # (w, h)\n",
    "\n",
    "plt.legend(['predict last count', 'myRNN', 'returnSeq', 'statefulRNN'], loc='upper right')\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to notice is, that `myRRN` converges clearly faster than the two other models. It reaches already after the first epoch an MAE around six. This makes sense because the size of the training set was ten times bigger. However the `seqRNN` has not converged after ten epochs. So basically we make up for the reduced redundancy in the new data format by training for more epochs. \n",
    "All models perform better than the naive benchmark of 6.65. However, the improvment seems moderate given the additional model complexity. `seqRNN` and `statefulRNN` seem to converge against a value above five, while `seqRNN` only reaches a value close to six. The higher value of `statefulRNN` was expected due to the missing information at the start of the sample sequences. The stateful property could reduce the MAE back to the level of `myRNN` but it does not clearly improve it. Due to this result and the additional restrictions when building a stateful model we continue to work with the \"stateless\" variant and the first data format.<br>\n",
    "So far we have been using a relativ small `SEQ_LEN` of ten. A simpler way of providing more information on past observations than the stateful option would be to increase the sample sequence length. We have not considered that so far because simple RNNs are hard to train on long sequences. The reason for this is the vanishing gradient problem."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Blog_Part_Marc_clean.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
