{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sample\n",
    "x = np.array([0.2, 0.3, 0.4])\n",
    "y = 7.0\n",
    "\n",
    "# Initialize Weights\n",
    "Wc = np.array([0.2, 0.4])\n",
    "\n",
    "Wo = np.array([0.1, 3.1])\n",
    "Wf = np.array([2.3, 0.2])\n",
    "Wi = np.array([3.1, 0.1])\n",
    "\n",
    "Uc = np.array([[1.8, 3.6], [4.7, 2.9]])\n",
    "Uo = np.array([[0.1, 0.9], [0.7, 4.3]])\n",
    "Uf = np.array([[3.6, 4.1], [1.0, 0.9]])\n",
    "Ui = np.array([[1.5, 2.6], [2.1, 0.2]])\n",
    "\n",
    "w = np.array([2.0, 4.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation LSTM\n",
    "$\n",
    "% Forget-Gate\n",
    "\\newcommand{\\F}{\\color{purple}{f_t}}\n",
    "\\newcommand{\\Ffull}[1][]{\\color{purple}{\\sigma^{#1} \\left( W_f + U_f \\cdot h_{t-1} \\right)}}\n",
    "\\newcommand{\\f}[1]{\\color{purple}{f^t_{#1}}}\n",
    "\\newcommand{\\ffull}[2][]{\\color{purple}{\\sigma^{#1} \\left(w^f_{#2} x_t + u^f_{#2 1} h^{t-1}_1 + u^f_{#2 2}h^{t-1}_2 \\right)}}\n",
    "\\newcommand{\\Wf}{\\color{purple}{W_f}}\n",
    "\\newcommand{\\Uf}{\\color{purple}{U_f}}\n",
    "\\newcommand{\\wf}[1]{\\color{purple}{w_{#1}^f}}\n",
    "\\newcommand{\\uf}[2]{\\color{purple}{u_{#1 #2}^f}}\n",
    "% Input-Gate\n",
    "\\newcommand{\\I}{\\color{fuchsia}{i_t}}\n",
    "\\newcommand{\\Ifull}[1][]{\\color{fuchsia}{\\sigma^{#1} \\left( W_i + U_i \\cdot h_{t-1} \\right)}}\n",
    "\\newcommand{\\i}[1]{\\color{fuchsia}{i^t_{#1}}}\n",
    "\\newcommand{\\ifull}[2][]{\\color{fuchsia}{\\sigma^{#1} \\left( w^i_{#2} x_t + u^i_{#2 1} h^{t-1}_1 + u^i_{#2 2} h^{t-1}_2 \\right)}}\n",
    "\\newcommand{\\Wi}{\\color{fuchsia}{W_i}}\n",
    "\\newcommand{\\Ui}{\\color{fuchsia}{U_i}}\n",
    "\\newcommand{\\wi}[1]{\\color{fuchsia}{w_{#1}^i}}\n",
    "\\newcommand{\\ui}[2]{\\color{fuchsia}{u_{#1 #2}^i}}\n",
    "% Output-Gate\n",
    "\\newcommand{\\O}{\\color{turquoise}{o_t}}\n",
    "\\newcommand{\\Ofull}[1][]{\\color{turquoise}{\\sigma^{#1} \\left( W_o + U_o \\cdot h_{t-1} \\right)}}\n",
    "\\newcommand{\\o}[1]{\\color{turquoise}{o^t_{#1}}}\n",
    "\\newcommand{\\ofull}[2][]{\\color{turquoise}{\\sigma^{#1} \\left( w^o_{#2} x_t + u^o_{#2 1} h^{t-1}_1 + u^o_{#2 2} h^{t-1}_2 \\right)}}\n",
    "\\newcommand{\\Wo}{\\color{turquoise}{W_o}}\n",
    "\\newcommand{\\Uo}{\\color{turquoise}{U_o}}\n",
    "\\newcommand{\\wo}[1]{\\color{turquoise}{w_{#1}^o}}\n",
    "\\newcommand{\\uo}[2]{\\color{turquoise}{u_{#1 #2}^o}}\n",
    "% Cell (c without tilde - or however the fuck it's supposed to be called)\n",
    "\\newcommand{\\C}[1][]{\\color{lime}{c_{t #1}}}\n",
    "\\newcommand{\\Cfull}{\\F * \\C[-1] + \\I * \\Ctil}\n",
    "\\newcommand{\\c}[2][]{\\color{lime}{c^{t #1}_{#2}}}\n",
    "% Candidate (c with tilde - again: whatever, it's fine.)\n",
    "\\newcommand{\\Ctil}{\\color{green}{\\tilde{c}_t}}\n",
    "\\newcommand{\\Ctilfull}[1][]{\\color{green}{g^{#1} \\left( W_c x_t + U_c \\cdot h_{t-1} \\right)}}\n",
    "\\newcommand{\\ctil}[2][]{\\color{green}{g^{#1} \\left( w^c_{#2} x_t + u^c_{#2 1} h^{t-1}_1 + u^c_{#2 2} h^{t-1}_2 \\right)}}\n",
    "\\newcommand{\\ctils}[1]{\\color{green}{\\tilde{c}_{#1}}}\n",
    "\\newcommand{\\Wc}{\\color{green}{W_c}}\n",
    "\\newcommand{\\Uc}{\\color{green}{U_c}}\n",
    "\\newcommand{\\wc}[1]{\\color{green}{w_{#1}^c}}\n",
    "\\newcommand{\\uc}[2]{\\color{green}{u_{#1 #2}^c}}\n",
    "% Miscellaneous\n",
    "\\newcommand{\\yhat}{\\hat{y}_t}\n",
    "\\newcommand{\\error}{\\left(y - \\yhat \\right)}\n",
    "\\newcommand{\\deriv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\dxt}[1]{\\color{#1}{x_t}}\n",
    "\\DeclareMathOperator{\\diag}{diag}\n",
    "\\newcommand{\\myH}[2][-1]{\\color{#2}{h_{t #1}}}\n",
    "\\newcommand{\\myh}[2][red]{\\color{#1}{h^{t-1}_{#2}}}\n",
    "$\n",
    "In order to demonstrate how the backpropagation on the LSTM works we will further investigate our previous example. Remember, that we used a $3 \\times 1$ vector for our single sample. Additionally, we looked at a very simple neural network that has only 2 neurons. Typically, we want to find better weights in each consecutive training iteration and in order to achieve that we follow a fraction of the negative gradient of a loss function.\n",
    "\n",
    "So right now, we need two things:\n",
    "1. a loss function\n",
    "1. and its gradients with respect to each of the weight matrices.\n",
    "\n",
    "Regarding the loss function we can make our lives easier by picking one, that is fit for our regression task while also being easy to derive. One of the easiest loss functions for that matter is the sum of squared errors, which we scale by a convenient constant:\n",
    "\n",
    "$$loss_t = \\frac{1}{2}\\left( y - \\yhat \\right)^2.$$\n",
    "\n",
    "Where $\\yhat$ is the prediction at timestep $t$. When taking the derivative with respect to $\\yhat$ one can easily see:\n",
    "\n",
    "$$\\deriv{loss_t}{\\yhat} = \\left( y - \\yhat \\right) (-1)$$\n",
    "\n",
    "We define our total loss to be the sum of the losses at each timestep:\n",
    "\n",
    "$$loss = \\sum loss_t$$\n",
    "\n",
    "This is also the reason, why the LSTM architecture remedies the vanishing or exploding gradients. Instead of resulting in a huge product as in the backpropagation through time, we will simply add the gradients of each timestep's contribution \\[[Mallya 2017](http://arunmallya.github.io/writeups/nn/lstm/index.html)\\]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what function we want to derive we also need to know which weights we want to tune. We define the weight matrices as\n",
    "\n",
    "\\begin{alignat}{2}\n",
    "% Weight matrices\n",
    "\\Wo &= \\begin{bmatrix} \\wo{1} \\\\ \\wo{2} \\end{bmatrix} \\quad \\Uo &&= \\begin{bmatrix} \\uo{1}{1} & \\uo{1}{2} \\\\ \\uo{2}{1} & \\uo{2}{2} \\end{bmatrix} \\\\\n",
    "\\Wf &= \\begin{bmatrix} \\wf{1} \\\\ \\wf{2} \\end{bmatrix} \\quad \\Uf &&= \\begin{bmatrix} \\uf{1}{1} & \\uf{1}{2} \\\\ \\uf{2}{1} & \\uf{2}{2} \\end{bmatrix} \\\\\n",
    "\\Wi &= \\begin{bmatrix} \\wi{1} \\\\ \\wi{2} \\end{bmatrix} \\quad \\Ui &&= \\begin{bmatrix} \\ui{1}{1} & \\ui{1}{2} \\\\ \\ui{2}{1} & \\ui{2}{2} \\end{bmatrix} \\\\\n",
    "\\Wc &= \\begin{bmatrix} \\wc{1} \\\\ \\wc{2} \\end{bmatrix} \\quad \\Uc &&= \\begin{bmatrix} \\uc{1}{1} & \\uc{1}{2} \\\\ \\uc{2}{1} & \\uc{2}{2} \\end{bmatrix}\n",
    "\\end{alignat}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, define the LSTM-forward pass as\n",
    "\n",
    "\\begin{align}\n",
    "% Forward pass\n",
    "\\myH[]{red} &= \\O * g(\\C) \\tag{1} \\\\\n",
    "\\C &= \\F * \\C[-1] + \\I * \\Ctil \\tag{2} \\\\\n",
    "\\Ctil &= \\Ctilfull = \\begin{pmatrix} \n",
    "                         \\ctil{1} \\\\ \\ctil{2}\n",
    "                     \\end{pmatrix} \\tag{3} \\\\\n",
    "\\I &= \\begin{pmatrix} \n",
    "           \\i{1} \\\\ \\i{2} \n",
    "      \\end{pmatrix} \n",
    "   = \\Ifull\n",
    "   = \\begin{pmatrix}\n",
    "         \\ifull{1} \\\\ \\ifull{2} \n",
    "     \\end{pmatrix} \\tag{4} \\\\\n",
    "\\F &= \\begin{pmatrix} \n",
    "          \\f{1} \\\\ \\f{2} \n",
    "      \\end{pmatrix} \n",
    "   = \\Ffull \n",
    "   = \\begin{pmatrix} \n",
    "         \\ffull{1} \\\\ \\ffull{2} \n",
    "     \\end{pmatrix} \\tag{5} \\\\\n",
    "\\O &= \\begin{pmatrix} \n",
    "          \\o{1} \\\\ \\o{2} \n",
    "      \\end{pmatrix} \n",
    "   = \\Ofull\n",
    "   = \\begin{pmatrix} \n",
    "         \\ofull{1} \\\\ \\ofull{2}\n",
    "     \\end{pmatrix} \\tag{6}\n",
    "\\end{align}\n",
    "\n",
    "Where we choose the activation functions $\\sigma$ as the sigmoid function and $g$ as the hyperbolic function \\[[cp Dey, Salem 2017](https://arxiv.org/pdf/1701.05923.pdf)\\].\n",
    "\n",
    "\\begin{alignat}{3}\n",
    "\\sigma(x) &= \\frac{\\exp(-x)}{1 + \\exp(-x} &&\\quad\\text{with}\\quad \\deriv{\\sigma(x)}{x} &&&= \\sigma(x) (1-\\sigma(x)) \\\\\n",
    "g(x) &= \\tanh(x) &&\\quad\\text{with}\\quad \\deriv{g(x)}{x} &&&= 1 - \\tanh^2(x)\n",
    "\\end{alignat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation functions\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(x):\n",
    "    \"\"\"Derivative of sigmoid function.\"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "def dtanh(x):\n",
    "    \"\"\"Derivative of tanh function.\"\"\"\n",
    "    return 1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also, for our purposes define the forward pass again, this time with a small change. This time, we want to store the results of every operation that depends on the timestep $t$ in their own lists. And we want to define a function that we can call later in a more convenient fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine forward pass\n",
    "h = []\n",
    "o = []\n",
    "f = []\n",
    "i = []\n",
    "c_ = []\n",
    "c = []\n",
    "y_ = []\n",
    "\n",
    "def LSTM_forward():\n",
    "    \"\"\"Perform forward pass\"\"\"\n",
    "    \n",
    "    # First, set hidden state to zero\n",
    "    h.append(np.zeros_like(Wc))\n",
    "    c.append(np.zeros_like(Wc))\n",
    "    \n",
    "    # Set `start` so that future calls don't have to reset the above lists\n",
    "    for t, xt in enumerate(x, start=len(y_)):\n",
    "        # Calculate values of gates (equations 4, 5, 6)\n",
    "        it = sigmoid(Wi.dot(xt) + Ui.dot(h[t]))\n",
    "        ft = sigmoid(Wf.dot(xt) + Uf.dot(h[t]))\n",
    "        ot = sigmoid(Wo.dot(xt) + Uo.dot(h[t]))\n",
    "\n",
    "        # Calculate candidate update (equations 3 and 2)\n",
    "        c_t = tanh(Wc.dot(xt) + Uc.dot(h[t]))\n",
    "        ct = ft * c[t] + it * c_t\n",
    "\n",
    "        # Calculate hidden state (equation 1)\n",
    "        ht = ot * tanh(ct)\n",
    "\n",
    "        # Prediction at step t (output layer)\n",
    "        y_t = w.dot(ht)\n",
    "\n",
    "        # Save variables to container, these will\n",
    "        # persist outside of the function's scope\n",
    "        h.append(ht)\n",
    "        o.append(ot)\n",
    "        f.append(ft)\n",
    "        i.append(it)\n",
    "        c_.append(c_t)\n",
    "        c.append(ct)\n",
    "        y_.append(y_t)\n",
    "    \n",
    "    # Return final prediction\n",
    "    return y_[-1]\n",
    "\n",
    "yhat_before = LSTM_forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to derive the loss function, we need to figure out how we obtain our predictions. We know that the output layer is just a linear combination of the hidden state and a vector of weights which, in our case, we denote by $w \\in \\mathbb{R}^2$. So the next step is to take the neatly separated equations $(1)$ to $(6)$ and plug them all in.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\yhat &= w^T \\cdot \\myH[]{red}\n",
    "      = w^T \\cdot \\left[ \\O * g(\\C) \\right] \\\\\n",
    "      &= w^T \\left[ \\Ofull * g(\\Cfull) \\right] \\\\\n",
    "      &= w^T \\cdot \\Bigg[ \\begin{pmatrix} \\ofull{1} \\\\ \\ofull{2} \\end{pmatrix} \\\\ \n",
    "      &\\quad\\quad* g \n",
    "                         \\begin{pmatrix} \n",
    "                             \\ffull{1} \\c[-1]{1} + \\ifull{1} + \\ctil{1} \\\\ \n",
    "                             \\ffull{2} \\c[-1]{2} + \\ifull{2} + \\ctil{2}\n",
    "                         \\end{pmatrix} \\Bigg]\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "which, if written in its single equation form, gives\n",
    "\n",
    "\\begin{alignat}{2}\n",
    "\\yhat &= w_1 \\ofull{1} g\\left(\\ffull{1} \\c[-1]{1} + \\\\ \\ifull{1} \\ctil{1}\\right)\\\\ \n",
    "      &+ w_2 \\ofull{2} g\\left(\\ffull{2} \\c[-1]{2} + \\\\ \\ifull{2} \\ctil{2}\\right) \\tag{7}\n",
    "\\end{alignat}\n",
    "\n",
    "Now this is arguably neither neat, nor pretty. However, since we are looking at a very low dimensional example it is a good opportunity to look into the backpropagation algorithm using high-school math only. And we can do that in a straightforward manner looking at equation $(7)$.\n",
    "\n",
    "Of course, if you are familiar with matrix derivatives and know that the element-wise multiplication of two vectors can be written as\n",
    "\n",
    "\\begin{equation}\n",
    "a * b = \\diag(a) \\cdot b = \\begin{bmatrix} \n",
    "                               a_1 & \\dots & 0 \\\\\n",
    "                               \\vdots & \\ddots & \\vdots \\\\\n",
    "                               0 & \\dots & a_n\n",
    "                           \\end{bmatrix} \\cdot\n",
    "                           \\begin{pmatrix} \n",
    "                                b_1 \\\\\n",
    "                                \\vdots \\\\\n",
    "                                b_n\n",
    "                           \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "then you're all set and you don't need to do this the tedious way we are about to embark on. Because we are going to take the partial derivative of $(7)$ with respect to each weight and using these results we will piece the gradients together. Moreover, once we have done that we will try to find a set of matrix operations, that will result in the gradients we previously puzzled together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights of the output gate\n",
    "\n",
    "Let's start with the weights of the output gate. If we take the partial derivatives with respect to $\\wo{1}$ and $\\wo{2}$ respectively we obtain:\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\wo{1}} = w_1 g(\\c{1}) \\ofull[\\prime]{1} \\ctil{1} \\dxt{turquoise}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\wo{2}} = w_2 g(\\c{2}) \\ofull[\\prime]{2} \\ctil{2} \\dxt{turquoise}\n",
    "\\end{equation}\n",
    "\n",
    "Which we now only need to combine in order to get the gradient of $\\yhat$ with respect to $\\Wo$.\n",
    "\n",
    "\\begin{align}\n",
    "\\deriv{\\yhat}{\\Wo} &= \\begin{bmatrix} \\deriv{\\yhat}{\\wo{1}} \\\\ \\deriv{\\yhat}{\\wo{2}} \\end{bmatrix} \\\\\n",
    "                   &= w * g(\\C) * \\Ofull[\\prime] * \\Ctil\\ \\dxt{turquoise} \\\\\n",
    "                   &= \\diag(w) \\cdot \\diag(g(\\C)) \\cdot \\diag(\\Ofull[\\prime]) \\cdot \\Ctil \\ \\dxt{turquoise}\n",
    "\\end{align}\n",
    "\n",
    "The last line is just a way of describing the same formula, without the use of element-wise multiplication, relying just on scalar and dot products. However, while this satisfies more conventional algebraic notational standards it is computationally more efficient to implement the gradients using Hadamard products.\n",
    "\n",
    "If we instead follow the logic of the backpropagation we obtain:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial {loss}}{\\partial W_o} &= \\sum\\frac{\\partial {loss}_t}{\\partial \\hat{y}_t} \\overbrace{\\frac{\\partial \\hat{y}_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial o_t} \\frac{\\partial o_t}{\\partial \\Wo}}^{\\deriv{\\yhat}{\\Wo}}\\\\\n",
    "                                   &= \\sum -(y - \\hat{y}_t)\\ w * g(\\C) * \\Ofull[\\prime] * \\Ctil x_t.\n",
    "\\end{align}\n",
    "\n",
    "Where the rightmost part is, unsurprisingly, the same as previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdWo = np.zeros_like(Wo)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    # Note that c = (c0, c1, c2, c3), whereas y_ = (y_1, y_2, y_3)\n",
    "    # thus we need to index c differently\n",
    "    dLossdWo += -(y-y_[t]) * w *dsigmoid(Wo.dot(xt) + Uo.dot(h[t])) * tanh(c[t+1]) * c_[t] * xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we obtain the partial derivatives for each element of $\\Uo$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\uo{1}{1}} = w_1 g(\\c{1}) \\ofull[\\prime]{1} \\ctil{1} \\myh[turquoise]{1}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\uo{1}{2}} = w_1 g(\\c{1}) \\ofull[\\prime]{1} \\ctil{1} \\myh[turquoise]{2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\uo{2}{1}} = w_2 g(\\c{2}) \\ofull[\\prime]{2} \\ctil{2} \\myh[turquoise]{1}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\uo{2}{2}} = w_2 g(\\c{2}) \\ofull[\\prime]{2} \\ctil{2} \\myh[turquoise]{2}\n",
    "\\end{equation}\n",
    "\n",
    "And by combining them into the jacobian of $\\yhat$ with respect to $\\Uo$ we get\n",
    "\n",
    "\\begin{align}\n",
    "\\deriv{\\yhat}{\\Uo} &= \\begin{bmatrix}\n",
    "                         \\deriv{\\yhat}{\\uo{1}{1}} & \\deriv{\\yhat}{\\uo{1}{2}} \\\\\n",
    "                         \\deriv{\\yhat}{\\uo{2}{1}} & \\deriv{\\yhat}{\\uo{2}{2}}\n",
    "                     \\end{bmatrix} \\\\\n",
    "                   &= \\begin{bmatrix}\n",
    "                         w_1 g(\\c{1}) \\ofull[\\prime]{1} \\ctils{1} \\myh[turquoise]{1} &\n",
    "                         w_1 g(\\c{1}) \\ofull[\\prime]{1} \\ctils{1} \\myh[turquoise]{2} \\\\\n",
    "                         w_2 g(\\c{2}) \\ofull[\\prime]{2} \\ctils{2} \\myh[turquoise]{1} &\n",
    "                         w_2 g(\\c{2}) \\ofull[\\prime]{2} \\ctils{2} \\myh[turquoise]{2} \\\\\n",
    "                     \\end{bmatrix}  \\\\\n",
    "                  &= \\begin{bmatrix} w & w \\end{bmatrix} *\n",
    "                     \\begin{bmatrix} g(\\C) & g(\\C) \\end{bmatrix} *\n",
    "                     \\begin{bmatrix} \\Ofull[\\prime] & \\Ofull[\\prime] \\end{bmatrix} *\n",
    "                     \\begin{bmatrix} \\Ctil & \\Ctil \\end{bmatrix} *\n",
    "                     \\begin{bmatrix} \\myH{turquoise}^T \\\\ \\myH{turquoise}^T \\end{bmatrix} \n",
    "                     \\\\\n",
    "                  &= \\diag(w) \\cdot \\diag(g(\\C)) \\cdot \\diag(\\Ofull[\\prime]) \\cdot \\diag(\\Ctil) \\cdot \\mathbb{1} \\mathbb{1}^T \\cdot \\diag(\\myH{turquoise}).\n",
    "\\end{align}\n",
    "\n",
    "Or, by the logic of the backpropagation algorithm:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial U_o} &= \\sum\\frac{\\partial {loss}_t}{\\partial \\hat{y}_t} \\overbrace{\\frac{\\partial \\hat{y}_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial o_t} \\frac{\\partial o_t}{\\partial \\Uo}}^{\\deriv{\\yhat}{\\Uo}} \\\\\n",
    "                                   &= \\sum -(y - \\hat{y}_t)\\ \\begin{bmatrix} w & w \\end{bmatrix} *\n",
    "                     \\begin{bmatrix} g(\\C) & g(\\C) \\end{bmatrix} *\n",
    "                     \\begin{bmatrix} \\Ofull[\\prime] & \\Ofull[\\prime] \\end{bmatrix} *\n",
    "                     \\begin{bmatrix} \\Ctil & \\Ctil \\end{bmatrix} *\n",
    "                     \\begin{bmatrix} \\myH{turquoise}^T \\\\ \\myH{turquoise}^T \\end{bmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "Again, rather than relying on the notation that makes heavy use of the dot products, we opt for implementing the one using element-wise multiplication. Note, that the first four of our matrix valued factors of that multiplication are obtained by combining copies of a vector in a column-wise fashion. In contrast to that, the last factor is obtained by row-wise stacking copies of a vector. In `numpy` one can obtain this behavior by a simple broadcasting operation. We simply need to `reshape` the first four factors, the expansion will then be performed automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdUo = np.zeros_like(Uo)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dy_dot = w * tanh(c[t+1]) * dsigmoid(Wo.dot(xt) + Uo.dot(h[t])) * c_[t]\n",
    "    # Expand so that the above multiplication can be performed element-wise\n",
    "    dLossdUo += (y-y_[t]) * dy_dot.reshape(-1, 1) * h[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights of the forget gate\n",
    "\n",
    "By the same token we obtain the partial derivatives of $\\yhat$ with respect to the elements of $\\Wf$\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\wf{1}} = w_1 \\o{1} g^\\prime(\\c{1}) \\ffull[\\prime]{1} \\c[-1]{1} \\dxt{purple}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\wf{2}} = w_2 \\o{2} g^\\prime(\\c{2}) \\ffull[\\prime]{2} \\c[-1]{2} \\dxt{purple}\n",
    "\\end{equation}\n",
    "\n",
    "and combine them to retrieve the gradient\n",
    "\n",
    "\\begin{align}\n",
    "\\deriv{\\yhat}{\\Wf} &= w * \\O * g^\\prime(\\C) * \\Ffull[\\prime] * \\C[-1] \\ \\dxt{purple} \\\\\n",
    "                   &= \\diag(w) \\cdot \\diag(\\O) \\cdot \\diag\\left(g^\\prime(\\C)\\right) \\cdot \\diag(\\Ffull[\\prime]) \\cdot \\C[-1] \\ \\dxt{purple}.\n",
    "\\end{align}\n",
    "\n",
    "Or, looking at the backpropagation's reasoning, we get:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial W_f} &= \n",
    "    \\sum\\frac{\\partial loss_t}{\\partial \\hat{y}_t}\\overbrace{\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial c_t} \\frac{\\partial c_t}{\\partial f_t}\\frac{\\partial f_t}{\\partial W_f}}^{\\deriv{\\yhat}{\\Wf}} \\\\\n",
    "                                   &= \\sum -(y-\\hat{y}_t) w * \\left[\\O * g^\\prime(\\C)\\right] * \\C * \\Ffull[\\prime] \\dxt{purple}\n",
    "\\end{align}\n",
    "\n",
    "Which we, straightforwardly, can implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdWf = np.zeros_like(Wf)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dLossdWf += -(y-y_[t]) * w * (o[t] * dtanh(c[t+1])) * c[t] * dsigmoid(Wf.dot(xt) + Uf.dot(h[t])) * xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the partial derivatives of $\\yhat$ with respect to the elements of $\\Uf$ we obtain:\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\uf{1}{1}} = w_1 g^\\prime(\\c{1}) \\ffull[\\prime]{1} \\c[-1]{1} \\myh[purple]{1}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\uf{1}{2}} = w_1 g^\\prime(\\c{1}) \\ffull[\\prime]{1} \\c[-1]{1} \\myh[purple]{2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\uf{2}{1}} = w_2 g^\\prime(\\c{2}) \\ffull[\\prime]{2} \\c[-1]{2} \\myh[purple]{1}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\uf{2}{2}} = w_2 g^\\prime(\\c{2}) \\ffull[\\prime]{2} \\c[-1]{2} \\myh[purple]{2}.\n",
    "\\end{equation}\n",
    "\n",
    "Which we can combine in order to acquire the jacobian of $\\yhat$ with respect to $\\Uf$.\n",
    "\n",
    "\\begin{align}\n",
    "\\deriv{\\yhat}{\\Uf} &= \\begin{bmatrix}\n",
    "                         \\deriv{\\yhat}{\\uf{1}{1}} & \\deriv{\\yhat}{\\uf{1}{2}} \\\\\n",
    "                         \\deriv{\\yhat}{\\uf{2}{1}} & \\deriv{\\yhat}{\\uf{2}{2}}\n",
    "                     \\end{bmatrix}\n",
    "                   = \\begin{bmatrix}\n",
    "                          w_1 g^\\prime(\\c{1}) \\ffull[\\prime]{1} \\c[-1]{1} \\myh[purple]{1} &\n",
    "                          w_1 g^\\prime(\\c{1}) \\ffull[\\prime]{1} \\c[-1]{1} \\myh[purple]{2} \\\\\n",
    "                          w_2 g^\\prime(\\c{2}) \\ffull[\\prime]{2} \\c[-1]{2} \\myh[purple]{1} &\n",
    "                          w_2 g^\\prime(\\c{2}) \\ffull[\\prime]{2} \\c[-1]{2} \\myh[purple]{2}\n",
    "                     \\end{bmatrix}\n",
    "                  \\\\\n",
    "                  &= \\begin{bmatrix} w & w \\end{bmatrix} *\n",
    "                     \\begin{bmatrix} \\O & \\O \\end{bmatrix} *\n",
    "                     g^\\prime( \\begin{bmatrix} \\C & \\C \\end{bmatrix}) *\n",
    "                     \\begin{bmatrix} \\Ffull[\\prime] & \\Ffull[\\prime] \\end{bmatrix} *\n",
    "                     \\begin{bmatrix} \\C[-1] & \\C[-1] \\end{bmatrix} *\n",
    "                     \\begin{bmatrix} \\myH{purple}^T \\\\ \\myH{purple}^T \\end{bmatrix} \\\\\n",
    "                  &= \\diag(w) \\cdot \\diag(\\O) \\cdot \\diag\\left(g^\\prime(\\C)\\right) \\cdot \\diag(\\Ffull[\\prime]) \\cdot \\diag(\\C[-1]) \\cdot \\mathbb{1} \\mathbb{1}^T \\cdot \\diag(\\myH{purple})\n",
    "\\end{align}\n",
    "\n",
    "The same is of course achieved by the backpropagation's rationale.\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial U_f} &= \\sum\\frac{\\partial loss_t}{\\partial \\hat{y}_t} \\overbrace{\\frac{\\partial \\hat{y}_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial c_t} \\frac{\\partial c_t}{\\partial f_t} \\frac{\\partial f_t}{\\partial U_f}}^{\\deriv{\\yhat}{\\Uf}} \\\\\n",
    "                                   &= \\sum -(y-\\hat{y}_t)\\ \\begin{bmatrix} w & w \\end{bmatrix} *\n",
    "                     \\begin{bmatrix} \\O & \\O \\end{bmatrix} *\n",
    "                     g^\\prime( \\begin{bmatrix} \\C & \\C \\end{bmatrix}) *\n",
    "                     \\begin{bmatrix} \\Ffull[\\prime] & \\Ffull[\\prime] \\end{bmatrix} *\n",
    "                     \\begin{bmatrix} \\C[-1] & \\C[-1] \\end{bmatrix} *\n",
    "                     \\begin{bmatrix} \\myH{purple}^T \\\\ \\myH{purple}^T \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Which we can implement, making use of `numpy`'s broadcasting operations, just as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdUf = np.zeros_like(Uf)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dy_dft = w*(o[t]*dtanh(c[t+1])) * c[t] * dsigmoid(Wf.dot(xt)+Uf.dot(h[t]))\n",
    "    dLossdUf += -(y-y_[t]) * dy_dft.reshape(-1, 1) * h[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights of the input gate\n",
    "\n",
    "Looking at the weights of the input gate we can proceed in the familiar fashion. We procure the partial derivatives of $\\yhat$ with respect to $\\wi{1}$ and $\\wi{2}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\wi{1}} = w_1 \\o{1} g^\\prime(\\c{1}) \\ctil{1} \\ifull[\\prime]{1} \\dxt{fuchsia},\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\wi{2}} = w_2 \\o{2} g^\\prime(\\c{2}) \\ctil{2} \\ifull[\\prime]{2} \\dxt{fuchsia}\n",
    "\\end{equation}\n",
    "\n",
    "and proceed by collecting them in order to form the gradient of $\\yhat$ with respect to $\\Wi$\n",
    "\n",
    "\\begin{align}\n",
    "\\deriv{\\yhat}{\\Ui} &= w * \\O * g^\\prime(\\C) * \\Ctil * \\Ifull[\\prime] \\dxt{fuchsia} \\\\\n",
    "                   &= \\diag(w) \\cdot \\diag(\\O) \\cdot \\diag(g^\\prime(\\C)) \\cdot \\diag(\\Ctil) \\cdot \\Ifull[\\prime] \\dxt{fuchsia}.\n",
    "\\end{align}\n",
    "\n",
    "Again, looking at it from the backpropagation's view, we get\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial W_i} &= \\sum\\frac{\\partial loss_t}{\\partial \\hat{y}_t} \\overbrace{\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial c_t} \\frac{\\partial c_t}{\\partial i_t}\\frac{\\partial i_t}{\\partial \\Wi}}^{\\deriv{\\yhat}{\\Wi}} \\\\\n",
    "                                   &= \\sum -(y-\\hat{y}_t)\\ w * \\O * g^\\prime(\\C) * \\Ctil * \\Ifull[\\prime] \\dxt{fuchsia}.\n",
    "\\end{align}\n",
    "\n",
    "Subsequently, we can implement the computation of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdWi = np.zeros_like(Wi)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dLossdWi += -(y-y_[t]) * w * (o[t] * dtanh(c[t+1])) * c_[t] * dsigmoid(Wi.dot(xt) + Ui.dot(h[t])) * xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently we proceed with the partial derivatives of $\\yhat$ with respect to the elements of $\\Ui$, which are\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\ui{1}{1}} = w_1 \\o{1} g^\\prime(\\c{1}) \\ctil{1} \\ifull[\\prime]{1} \\myh[fuchsia]{1},\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\ui{1}{2}} = w_1 \\o{1} g^\\prime(\\c{1}) \\ctil{1} \\ifull[\\prime]{1} \\myh[fuchsia]{2},\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\ui{2}{1}} = w_2 \\o{2} g^\\prime(\\c{2}) \\ctil{2} \\ifull[\\prime]{2} \\myh[fuchsia]{1},\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\ui{2}{2}} = w_2 \\o{2} g^\\prime(\\c{2}) \\ctil{2} \\ifull[\\prime]{2} \\myh[fuchsia]{2}.\n",
    "\\end{equation}\n",
    "\n",
    "By collecting them into a matrix we obtain the jacobian of $\\yhat$ with respect to $\\Ui$\n",
    "\n",
    "\\begin{align}\n",
    "\\deriv{\\yhat}{\\Ui} &= \\begin{bmatrix}\n",
    "                          \\deriv{\\yhat}{\\ui{1}{1}} & \\deriv{\\yhat}{\\ui{1}{2}} \\\\\n",
    "                          \\deriv{\\yhat}{\\ui{2}{1}} & \\deriv{\\yhat}{\\ui{2}{2}} \n",
    "                      \\end{bmatrix} \\\\ \n",
    "                   &= \\begin{bmatrix}\n",
    "                          w_1 \\o{1} g^\\prime(\\c{1}) \\ctils{1} \\ifull[\\prime]{1} \\myh[fuchsia]{1} &\n",
    "                          w_1 \\o{1} g^\\prime(\\c{1}) \\ctils{1} \\ifull[\\prime]{1} \\myh[fuchsia]{2} \\\\\n",
    "                          w_2 \\o{2} g^\\prime(\\c{2}) \\ctils{2} \\ifull[\\prime]{2} \\myh[fuchsia]{1} &\n",
    "                          w_2 \\o{2} g^\\prime(\\c{2}) \\ctils{2} \\ifull[\\prime]{2} \\myh[fuchsia]{2} \\\\\n",
    "                      \\end{bmatrix} \\\\\n",
    "                   &= \\begin{bmatrix} w & w \\end{bmatrix} *\n",
    "                      \\begin{bmatrix} \\O & \\O \\end{bmatrix} *\n",
    "                      g^\\prime\\left(\\begin{bmatrix} \\C & \\C \\end{bmatrix}\\right) *\n",
    "                      \\begin{bmatrix} \\Ctil & \\Ctil \\end{bmatrix} *\n",
    "                      \\begin{bmatrix} \\Ifull[\\prime] & \\Ifull[\\prime] \\end{bmatrix} *\n",
    "                      \\begin{bmatrix} \\myH{fuchsia}^T \\\\ \\myH{fuchsia}^T \\end{bmatrix} \\\\\n",
    "                   &= \\diag(w) \\cdot \\diag(\\O) \\cdot \\diag\\left(g^\\prime(\\C)\\right) \\cdot \\diag(\\Ctil) \\cdot \\diag(\\Ifull[\\prime]) \\cdot \\mathbb{1} \\mathbb{1}^T \\cdot \\diag(\\myH{fuchsia})\n",
    "\\end{align}\n",
    "\n",
    "For our intents and purposes we are interested in the gradients of the loss function, which are\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial U_i} &= \\sum\\frac{\\partial loss_t}{\\partial \\hat{y}_t} \\overbrace{\\frac{\\partial \\hat{y}_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial c_t} \\frac{\\partial c_t}{\\partial i_t} \\frac{\\partial i_t}{\\partial U_i}}^{\\deriv{\\yhat}{\\Ui}} \\\\\n",
    "                                   &= \\sum(y-\\hat{y}_t)\\ \\begin{bmatrix} w & w \\end{bmatrix} *\n",
    "                      \\begin{bmatrix} \\O & \\O \\end{bmatrix} *\n",
    "                      g^\\prime\\left(\\begin{bmatrix} \\C & \\C \\end{bmatrix}\\right) *\n",
    "                      \\begin{bmatrix} \\Ctil & \\Ctil \\end{bmatrix} * \\\\\n",
    "                      &\\quad\\quad\\begin{bmatrix} \\Ifull[\\prime] & \\Ifull[\\prime] \\end{bmatrix} *\n",
    "                      \\begin{bmatrix} \\myH{fuchsia}^T \\\\ \\myH{fuchsia}^T \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Which we implement by making use of `numpy`'s broadcasting functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdUi = np.zeros_like(Ui)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dy_dit = w * (o[t]*dtanh(c[t])) * c_[t] * dsigmoid(Wi.dot(xt)+Ui.dot(h[t]))\n",
    "    dLossdUi += -(y-y_[t]) * dy_dit.reshape(-1, 1) * h[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights of the candidate update\n",
    "\n",
    "Again we begin by taking partial derivatives of $\\yhat$ with respect to $\\Wc$'s elements, such that we get\n",
    "\n",
    "\\begin{align}\n",
    "\\deriv{\\yhat}{\\wc{1}} = w_1 \\o{1} g^\\prime(\\c{1}) \\ifull{1} \\ctil[\\prime]{1} \\dxt{green}\n",
    "\\end{align}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align}\n",
    "\\deriv{\\yhat}{\\wc{2}} = w_2 \\o{2} g^\\prime(\\c{2}) \\ifull{2} \\ctil[\\prime]{2} \\dxt{green}.\n",
    "\\end{align}\n",
    "\n",
    "Arranging the partial derivatives, we obtain the gradient of $\\yhat$ with respect to $\\Wc$\n",
    "\n",
    "\\begin{align}\n",
    "\\deriv{\\yhat}{\\Wc} = w * \\O * g^\\prime(\\C) * \\I * \\Ctilfull[\\prime]\\ \\dxt{green},\n",
    "\\end{align}\n",
    "\n",
    "which, in terms of our loss function gives\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial W_c} &= \\sum\\frac{\\partial loss_t}{\\partial \\hat{y}_t}\\overbrace{\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial c_t}\\frac{\\partial c_t}{\\partial \\tilde{c}_t} \\frac{\\partial \\tilde{c}_t}{\\partial W_c}}^{\\deriv{\\yhat}{\\Wc}} \\\\\n",
    "                                   &= \\sum(y - \\hat{y}_t)\\ w * \\O * g^\\prime(\\C) * \\I * \\Ctilfull[\\prime]\\ \\dxt{green}\n",
    "\\end{align}\n",
    "\n",
    "which we implement in the same manner as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdWc = np.zeros_like(Wc)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dLossdWc += -(y-y_[t]) * w * (o[t] * dtanh(c[t+1])) * i[t] * dtanh(Wc.dot(xt) + Uc.dot(h[t])) * xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we start by obtaining the partial derivatives of $\\yhat$ with respect to each of $\\Wc$'s elements.\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\uc{1}{1}} = w_1 \\o{1} g^\\prime(\\c{1}) \\i{1} \\ctil[\\prime]{1} \\myh[green]{1},\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\uc{1}{2}} = w_1 \\o{1} g^\\prime(\\c{1}) \\i{1} \\ctil[\\prime]{1} \\myh[green]{2},\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\uc{2}{1}} = w_2 \\o{2} g^\\prime(\\c{2}) \\i{2} \\ctil[\\prime]{2} \\myh[green]{1}\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "\\deriv{\\yhat}{\\uc{2}{2}} = w_2 \\o{2} g^\\prime(\\c{2}) \\i{2} \\ctil[\\prime]{2} \\myh[green]{2}.\n",
    "\\end{equation}\n",
    "\n",
    "And by organizing these partial derivatives, we can form the jacobian of $\\yhat$ with respect to $\\Uc$\n",
    "\n",
    "\\begin{align}\n",
    "\\deriv{\\yhat}{\\Uc} &= \\begin{bmatrix} \n",
    "                          \\deriv{\\yhat}{\\uc{1}{1}} & \\deriv{\\yhat}{\\uc{1}{2}} \\\\\n",
    "                          \\deriv{\\yhat}{\\uc{2}{1}} & \\deriv{\\yhat}{\\uc{2}{2}}\n",
    "                      \\end{bmatrix} = \n",
    "                      \\begin{bmatrix}\n",
    "                          w_1 \\o{1} g^\\prime(\\c{1}) \\i{1} \\ctil[\\prime]{1} \\myh[green]{1} &\n",
    "                          w_1 \\o{1} g^\\prime(\\c{1}) \\i{1} \\ctil[\\prime]{1} \\myh[green]{2} \\\\\n",
    "                          w_2 \\o{2} g^\\prime(\\c{2}) \\i{2} \\ctil[\\prime]{2} \\myh[green]{1} &\n",
    "                          w_2 \\o{2} g^\\prime(\\c{2}) \\i{2} \\ctil[\\prime]{2} \\myh[green]{2} \n",
    "                      \\end{bmatrix} \\\\\n",
    "                   &= \\begin{bmatrix} w & w \\end{bmatrix} *\n",
    "                      \\begin{bmatrix} \\O & \\O \\end{bmatrix} *\n",
    "                      g^\\prime(\\begin{bmatrix} \\C & \\C \\end{bmatrix}) *\n",
    "                      \\begin{bmatrix} \\I & \\I \\end{bmatrix} *\n",
    "                      \\begin{bmatrix} \\Ctilfull[\\prime] & \\Ctilfull[\\prime] \\end{bmatrix} *\n",
    "                      \\begin{bmatrix} \\myH{green}^T \\\\ \\myH{green}^T \\end{bmatrix} \\\\\n",
    "                   &= \\diag(w) \\cdot \n",
    "                      \\diag(\\O) \\cdot \n",
    "                      \\diag\\left(g^\\prime(\\C)\\right) \\cdot \n",
    "                      \\diag(\\I) \\cdot \n",
    "                      \\diag(\\Ctilfull[\\prime]) \\cdot \n",
    "                      \\mathbb{1} \\mathbb{1}^T \\cdot \n",
    "                      \\diag(\\myH{green})\n",
    "\\end{align}\n",
    "\n",
    "For our problem this means, that we calculate the gradient of the loss function with respect to $\\Uc$ as\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial U_c} &= \\sum\\frac{\\partial loss_t}{\\partial \\hat{y}_t}\\overbrace{\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial c_t}\\frac{\\partial c_t}{\\partial \\tilde{c}_t}\\frac{\\partial \\tilde{c}_t}{\\partial U_c}}^{\\deriv{\\yhat}{\\Uc}} \\\\\n",
    "                          &= \\sum(y_t - \\hat{y}_t)\\ w * \\O * g^\\prime(\\C) * \\I * \\Ctilfull[\\prime]\\ * \\begin{bmatrix} h^T_{t-1} \\\\ h^T_{t-1} \\end{bmatrix},\n",
    "\\end{align}\n",
    "\n",
    "which by now should be easy to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdUc = np.zeros_like(Uc)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dc_tdf = w * (o[t] * dtanh(c[t+1])) * i[t] * dtanh(Wc.dot(xt)+Uc.dot(h[t]))\n",
    "    dLossdUc += -(y-y_[t]) * dc_tdf.reshape(-1, 1) * h[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights of the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to adjust the weights of the output layer. The gradient of the loss function is simply\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{loss}{w} = \\sum\\deriv{loss_t}{w} = \\sum\\deriv{loss_t}{\\yhat}\\deriv{\\yhat}{w} = \\sum -(y - \\yhat) \\myH[]{red}.\n",
    "\\end{equation}\n",
    "\n",
    "Which can be readily implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdw = np.zeros_like(w)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dLossdw += -(y - y_[t]) * h[t+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight update\n",
    "\n",
    "With all gradients being computed, we can use them in order to update our weights. Typically, when using gradient descent, we follow the direction of the negative gradient, multiplied by some fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.01\n",
    "\n",
    "Wc -= eta * dLossdWc\n",
    "Wo -= eta * dLossdWo\n",
    "Wf -= eta * dLossdWf\n",
    "Wi -= eta * dLossdWi\n",
    "\n",
    "Uc -= eta * dLossdUc\n",
    "Uo -= eta * dLossdUo\n",
    "Uf -= eta * dLossdUf\n",
    "Ui -= eta * dLossdUi\n",
    "\n",
    "w -= eta * dLossdw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the new weights we can compute the updated prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_after = LSTM_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$\\hat{y}_{before}$</th>\n",
       "      <th>$\\hat{y}_{after}$</th>\n",
       "      <th>$y$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.046038</td>\n",
       "      <td>4.956741</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   $\\hat{y}_{before}$  $\\hat{y}_{after}$  $y$\n",
       "1            2.046038           4.956741  7.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({r\"$\\hat{y}_{before}$\": yhat_before, r\"$\\hat{y}_{after}$\": yhat_after, r\"$y$\": y}, index=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can clearly see, we are getting close to the real $y$. In a real application one would of course evaluate the gradients at multiple samples and use an average gradient in order to update the weight matrices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infosys",
   "language": "python",
   "name": "infosys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
