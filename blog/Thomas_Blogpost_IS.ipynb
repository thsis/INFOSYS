{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sample\n",
    "x = np.array([0.2, 0.3, 0.4])\n",
    "y = 7.0\n",
    "\n",
    "# Initialize Weights\n",
    "Wc = np.array([0.2, 0.4])\n",
    "\n",
    "Wo = np.array([0.1, 3.1])\n",
    "Wf = np.array([2.3, 0.2])\n",
    "Wi = np.array([3.1, 0.1])\n",
    "\n",
    "Uc = np.array([[1.8, 3.6], [4.7, 2.9]])\n",
    "Uo = np.array([[0.1, 0.9], [0.7, 4.3]])\n",
    "Uf = np.array([[3.6, 4.1], [1.0, 0.9]])\n",
    "Ui = np.array([[1.5, 2.6], [2.1, 0.2]])\n",
    "\n",
    "w = np.array([2.0, 4.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation LSTM\n",
    "$\n",
    "% Forget-Gate\n",
    "\\newcommand{\\F}{\\color{orange}{f_t}}\n",
    "\\newcommand{\\Ffull}[1][]{\\color{orange}{\\sigma^{#1} \\left( W_f + U_f \\cdot h_{t-1} \\right)}}\n",
    "\\newcommand{\\f}[1]{\\color{orange}{f^t_{#1}}}\n",
    "\\newcommand{\\ffull}[2][]{\\color{orange}{\\sigma^{#1} \\left(w^f_{#2} x_t + u^f_{#2 1} h^{t-1}_1 + u^f_{#2 2}h^{t-1}_2 \\right)}}\n",
    "\\newcommand{\\Wf}{\\color{orange}{W_f}}\n",
    "\\newcommand{\\Uf}{\\color{orange}{U_f}}\n",
    "\\newcommand{\\wf}[1]{\\color{orange}{w_{#1}^f}}\n",
    "\\newcommand{\\uf}[2]{\\color{orange}{u_{#1 #2}^f}}\n",
    "% Input-Gate\n",
    "\\newcommand{\\I}{\\color{red}{i_t}}\n",
    "\\newcommand{\\Ifull}[1][]{\\color{red}{\\sigma^{#1} \\left( W_i + U_i \\cdot h_{t-1} \\right)}}\n",
    "\\newcommand{\\i}[1]{\\color{red}{i^t_{#1}}}\n",
    "\\newcommand{\\ifull}[2][]{\\color{red}{\\sigma^{#1} \\left( w^i_{#2} x_t + u^i_{#2 1} h^{t-1}_1 + u^i_{#2 2} h^{t-1}_2 \\right)}}\n",
    "\\newcommand{\\Wi}{\\color{red}{W_i}}\n",
    "\\newcommand{\\Ui}{\\color{red}{U_i}}\n",
    "\\newcommand{\\wi}[1]{\\color{red}{w_{#1}^i}}\n",
    "\\newcommand{\\ui}[2]{\\color{red}{u_{#1 #2}^i}}\n",
    "% Output-Gate\n",
    "\\newcommand{\\O}{\\color{blue}{o_t}}\n",
    "\\newcommand{\\Ofull}[1][]{\\color{blue}{\\sigma^{#1} \\left( W_o + U_o \\cdot h_{t-1} \\right)}}\n",
    "\\newcommand{\\o}[1]{\\color{blue}{o^t_{#1}}}\n",
    "\\newcommand{\\ofull}[2][]{\\color{blue}{\\sigma^{#1} \\left( w^o_{#2} x_t + u^o_{#2 1} h^{t-1}_1 + u^o_{#2 2} h^{t-1}_2 \\right)}}\n",
    "\\newcommand{\\Wo}{\\color{blue}{W_o}}\n",
    "\\newcommand{\\Uo}{\\color{blue}{U_o}}\n",
    "\\newcommand{\\wo}[1]{\\color{blue}{w_{#1}^o}}\n",
    "\\newcommand{\\uo}[2]{\\color{blue}{u_{#1 #2}^o}}\n",
    "% Cell (c without tilde - or however the fuck it's supposed to be called)\n",
    "\\newcommand{\\C}[1][]{\\color{lime}{c_{t #1}}}\n",
    "\\newcommand{\\Cfull}{\\F * \\C[-1] + \\I * \\Ctil}\n",
    "\\newcommand{\\c}[2][]{\\color{lime}{c^{t #1}_{#2}}}\n",
    "% Candidate (c with tilde - again: whatever, it's fine.)\n",
    "\\newcommand{\\Ctil}{\\color{green}{\\tilde{c}_t}}\n",
    "\\newcommand{\\Ctilfull}[1][]{\\color{green}{g^{#1} \\left( W_c x_t + U_c \\cdot h_{t-1} \\right)}}\n",
    "\\newcommand{\\ctil}[2][]{\\color{green}{g^{#1} \\left( w^c_{#2} x_t + u^c_{#2 1} h^{t-1}_1 + u^c_{#2 2} h^{t-1}_2 \\right)}}\n",
    "\\newcommand{\\ctils}[1]{\\color{green}{\\tilde{c}_{#1}}}\n",
    "\\newcommand{\\Wc}{\\color{green}{W_c}}\n",
    "\\newcommand{\\Uc}{\\color{green}{U_c}}\n",
    "\\newcommand{\\wc}[1]{\\color{green}{w_{#1}^c}}\n",
    "\\newcommand{\\uc}[2]{\\color{green}{u_{#1 #2}^c}}\n",
    "% Miscellaneous\n",
    "\\newcommand{\\yhat}{\\hat{y}_t}\n",
    "\\newcommand{\\error}{\\left(y - \\yhat \\right)}\n",
    "\\newcommand{\\deriv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\dxt}[1]{\\color{#1}{x_t}}\n",
    "\\DeclareMathOperator{\\diag}{diag}\n",
    "\\newcommand{\\myH}[2][-1]{\\color{#2}{h_{t #1}}}\n",
    "\\newcommand{\\myh}[2][red]{\\color{#1}{h^{t-1}_{#2}}}\n",
    "$\n",
    "In order to demonstrate how the backpropagation on the LSTM works we will further investigate our previous example. Remember, that we used a $3 \\times 1$ vector for our single sample. Additionally, we looked at a very simple neural network that has only 2 neurons. Typically, we want to find better weights in each consecutive training iteration and in order to achieve that we follow a fraction of the negative gradient of a loss function.\n",
    "\n",
    "So right now, we need two things:\n",
    "1. a loss function\n",
    "1. and its gradients with respect to each of the weight matrices.\n",
    "\n",
    "Regarding the loss function we can make our lives easier by picking one, that is fit for our regression task while also being easy to derive. One of the easiest loss functions for that matter is the sum of squared errors, which we scale by a convenient constant:\n",
    "\n",
    "$$loss_t = \\frac{1}{2}\\left( y - \\yhat \\right)^2.$$\n",
    "\n",
    "Where $\\yhat$ is the prediction at timestep $t$. When taking the derivative with respect to $\\yhat$ one can easily see:\n",
    "\n",
    "$$\\deriv{loss_t}{\\yhat} = \\left( y - \\yhat \\right) (-1)$$\n",
    "\n",
    "We define our total loss to be the sum of the losses at each timestep:\n",
    "\n",
    "$$loss = \\sum loss_t$$\n",
    "\n",
    "which of course gives\n",
    "\n",
    "$$\\deriv{loss}{\\yhat} = \\sum\\deriv{loss_t}{\\yhat}$$.\n",
    "\n",
    "This is also the reason, why the LSTM architecture remedies the vanishing or exploding gradients. Instead of resulting in a huge product as in the backpropagation through time, we will simply add the gradients of each timestep's contribution \\[[Mallya 2017](http://arunmallya.github.io/writeups/nn/lstm/index.html)\\]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what function we want to derive we also need to know which weights we want to tune. We define the weight matrices as\n",
    "\n",
    "\\begin{alignat}{2}\n",
    "% Weight matrices\n",
    "\\Wo &= \\begin{bmatrix} \\wo{1} \\\\ \\wo{2} \\end{bmatrix} \\quad \\Uo &&= \\begin{bmatrix} \\uo{1}{1} & \\uo{1}{2} \\\\ \\uo{2}{1} & \\uo{2}{2} \\end{bmatrix} \\\\\n",
    "\\Wf &= \\begin{bmatrix} \\wf{1} \\\\ \\wf{2} \\end{bmatrix} \\quad \\Uf &&= \\begin{bmatrix} \\uf{1}{1} & \\uf{1}{2} \\\\ \\uf{2}{1} & \\uf{2}{2} \\end{bmatrix} \\\\\n",
    "\\Wi &= \\begin{bmatrix} \\wi{1} \\\\ \\wi{2} \\end{bmatrix} \\quad \\Ui &&= \\begin{bmatrix} \\ui{1}{1} & \\ui{1}{2} \\\\ \\ui{2}{1} & \\ui{2}{2} \\end{bmatrix} \\\\\n",
    "\\Wc &= \\begin{bmatrix} \\wc{1} \\\\ \\wc{2} \\end{bmatrix} \\quad \\Uc &&= \\begin{bmatrix} \\uc{1}{1} & \\uc{1}{2} \\\\ \\uc{2}{1} & \\uc{2}{2} \\end{bmatrix}\n",
    "\\end{alignat}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, define the LSTM-forward pass as\n",
    "\n",
    "\\begin{align}\n",
    "% Forward pass\n",
    "\\myH[]{red} &= \\O * g(\\C) \\tag{1} \\\\\n",
    "\\C &= \\F * \\C[-1] + \\I * \\Ctil \\tag{2} \\\\\n",
    "\\Ctil &= \\Ctilfull\\tag{3} \\\\\n",
    "\\I &= \\begin{pmatrix} \n",
    "           \\i{1} \\\\ \\i{2} \n",
    "      \\end{pmatrix} \n",
    "   = \\Ifull \\tag{4} \\\\\n",
    "\\F &= \\begin{pmatrix} \n",
    "          \\f{1} \\\\ \\f{2} \n",
    "      \\end{pmatrix} \n",
    "   = \\Ffull  \\tag{5} \\\\\n",
    "\\O &= \\begin{pmatrix} \n",
    "          \\o{1} \\\\ \\o{2} \n",
    "      \\end{pmatrix} \n",
    "   = \\Ofull \\tag{6}\n",
    "\\end{align}\n",
    "\n",
    "Where we choose the activation functions $\\sigma$ as the sigmoid function and $g$ as the hyperbolic function \\[[cp Dey, Salem 2017](https://arxiv.org/pdf/1701.05923.pdf)\\].\n",
    "\n",
    "\\begin{alignat}{3}\n",
    "\\sigma(x) &= \\frac{\\exp(-x)}{1 + \\exp(-x} &&\\quad\\text{with}\\quad \\deriv{\\sigma(x)}{x} &&&= \\sigma(x) (1-\\sigma(x)) \\\\\n",
    "g(x) &= \\tanh(x) &&\\quad\\text{with}\\quad \\deriv{g(x)}{x} &&&= 1 - \\tanh^2(x)\n",
    "\\end{alignat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation functions\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(x):\n",
    "    \"\"\"Derivative of sigmoid function.\"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "def dtanh(x):\n",
    "    \"\"\"Derivative of tanh function.\"\"\"\n",
    "    return 1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also, for our purposes define the forward pass again, this time with a small change. This time, we want to store the results of every operation that depends on the timestep $t$ in their own lists. And we want to define a function that we can call later in a more convenient fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine forward pass\n",
    "h = []\n",
    "o = []\n",
    "f = []\n",
    "i = []\n",
    "c_ = []\n",
    "c = []\n",
    "y_ = []\n",
    "\n",
    "def LSTM_forward():\n",
    "    \"\"\"Perform forward pass\"\"\"\n",
    "    \n",
    "    # First, set hidden state to zero\n",
    "    h.append(np.zeros_like(Wc))\n",
    "    c.append(np.zeros_like(Wc))\n",
    "    \n",
    "    # Set `start` so that future calls don't have to reset the above lists\n",
    "    for t, xt in enumerate(x, start=len(y_)):\n",
    "        # Calculate values of gates (equations 4, 5, 6)\n",
    "        it = sigmoid(Wi.dot(xt) + Ui.dot(h[t]))\n",
    "        ft = sigmoid(Wf.dot(xt) + Uf.dot(h[t]))\n",
    "        ot = sigmoid(Wo.dot(xt) + Uo.dot(h[t]))\n",
    "\n",
    "        # Calculate candidate update (equations 3 and 2)\n",
    "        c_t = tanh(Wc.dot(xt) + Uc.dot(h[t]))\n",
    "        ct = ft * c[t] + it * c_t\n",
    "\n",
    "        # Calculate hidden state (equation 1)\n",
    "        ht = ot * tanh(ct)\n",
    "\n",
    "        # Prediction at step t (output layer)\n",
    "        y_t = w.dot(ht)\n",
    "\n",
    "        # Save variables to container, these will\n",
    "        # persist outside of the function's scope\n",
    "        h.append(ht)\n",
    "        o.append(ot)\n",
    "        f.append(ft)\n",
    "        i.append(it)\n",
    "        c_.append(c_t)\n",
    "        c.append(ct)\n",
    "        y_.append(y_t)\n",
    "    \n",
    "    # Return final prediction\n",
    "    return y_[-1]\n",
    "\n",
    "yhat_before = LSTM_forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to derive the loss function, we need to figure out how we obtain our predictions. We know that the output layer is just a linear combination of the hidden state and a vector of weights which, in our case, we denote by $w \\in \\mathbb{R}^2$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\yhat = w^T \\cdot \\myH[]{red}\n",
    "\\end{equation}\n",
    "\n",
    "Of course, if you are familiar with matrix derivatives and know that the element-wise multiplication of two vectors can be written as\n",
    "\n",
    "\\begin{equation}\n",
    "a * b = \\diag(a) \\cdot b = \\begin{bmatrix} \n",
    "                               a_1 & \\dots & 0 \\\\\n",
    "                               \\vdots & \\ddots & \\vdots \\\\\n",
    "                               0 & \\dots & a_n\n",
    "                           \\end{bmatrix} \\cdot\n",
    "                           \\begin{pmatrix} \n",
    "                                b_1 \\\\\n",
    "                                \\vdots \\\\\n",
    "                                b_n\n",
    "                           \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "then you're all set. For a derivation of all gradients using high-school-level math only, consider the appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights of the output gate\n",
    "\n",
    "By following the logic of the backpropagation we obtain:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial {loss}}{\\partial W_o} &= \\sum\\frac{\\partial {loss}_t}{\\partial \\hat{y}_t} \\overbrace{\\frac{\\partial \\hat{y}_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial o_t} \\frac{\\partial o_t}{\\partial \\Wo}}^{\\deriv{\\yhat}{\\Wo}}\\\\\n",
    "                                   &= \\sum -(y - \\hat{y}_t)\\ w * g(\\C) * \\Ofull[\\prime] * \\Ctil x_t.\n",
    "\\end{align}\n",
    "\n",
    "Which we can implement in a straightforward manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdWo = np.zeros_like(Wo)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    # Note that c = (c0, c1, c2, c3), whereas y_ = (y_1, y_2, y_3)\n",
    "    # thus we need to index c differently\n",
    "    dLossdWo += -(y-y_[t]) * w *dsigmoid(Wo.dot(xt) + Uo.dot(h[t])) * tanh(c[t+1]) * c_[t] * xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we obtain the gradient with respect to $\\Uo$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial U_o} &= \\sum\\frac{\\partial {loss}_t}{\\partial \\hat{y}_t} \\overbrace{\\frac{\\partial \\hat{y}_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial o_t} \\frac{\\partial o_t}{\\partial \\Uo}}^{\\deriv{\\yhat}{\\Uo}} \\\\\n",
    "                                   &= \\sum -(y - \\hat{y}_t)\\ \\begin{bmatrix} w & w \\end{bmatrix} *\n",
    "                     \\begin{bmatrix} g(\\C) & g(\\C) \\end{bmatrix} *\n",
    "                     \\begin{bmatrix} \\Ofull[\\prime] & \\Ofull[\\prime] \\end{bmatrix} *\n",
    "                     \\begin{bmatrix} \\Ctil & \\Ctil \\end{bmatrix} *\n",
    "                     \\begin{bmatrix} \\myH{blue}^T \\\\ \\myH{blue}^T \\end{bmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "Note, that the first four of our matrix valued factors of that multiplication are obtained by combining copies of a vector in a column-wise fashion. In contrast to that, the last factor is obtained by row-wise stacking copies of a vector. In `numpy` one can obtain this behavior by a simple broadcasting operation. We simply need to `reshape` the first four factors, the expansion will then be performed automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdUo = np.zeros_like(Uo)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dy_dot = w * tanh(c[t+1]) * dsigmoid(Wo.dot(xt) + Uo.dot(h[t])) * c_[t]\n",
    "    # Expand so that the above multiplication can be performed element-wise\n",
    "    dLossdUo += (y-y_[t]) * dy_dot.reshape(-1, 1) * h[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights of the forget gate\n",
    "\n",
    "By the same token we obtain the partial gradient of $loss_t$ with respect to the elements of $\\Wf$\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial W_f} &= \n",
    "    \\sum\\frac{\\partial loss_t}{\\partial \\hat{y}_t}\\overbrace{\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial c_t} \\frac{\\partial c_t}{\\partial f_t}\\frac{\\partial f_t}{\\partial W_f}}^{\\deriv{\\yhat}{\\Wf}} \\\\\n",
    "                                   &= \\sum -(y-\\hat{y}_t) w * \\left[\\O * g^\\prime(\\C)\\right] * \\C * \\Ffull[\\prime] \\dxt{orange}\n",
    "\\end{align}\n",
    "\n",
    "Which we, can implement as candidly as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdWf = np.zeros_like(Wf)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dLossdWf += -(y-y_[t]) * w * (o[t] * dtanh(c[t+1])) * c[t] * dsigmoid(Wf.dot(xt) + Uf.dot(h[t])) * xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the gradient of $loss_t$ with respect to $\\Uf$ we obtain:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial U_f} &= \\sum\\frac{\\partial loss_t}{\\partial \\hat{y}_t} \\overbrace{\\frac{\\partial \\hat{y}_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial c_t} \\frac{\\partial c_t}{\\partial f_t} \\frac{\\partial f_t}{\\partial U_f}}^{\\deriv{\\yhat}{\\Uf}} \\\\\n",
    "                                   &= \\sum -(y-\\hat{y}_t)\\ \\begin{bmatrix} w & w \\end{bmatrix} *\n",
    "                     \\begin{bmatrix} \\O & \\O \\end{bmatrix} *\n",
    "                     g^\\prime( \\begin{bmatrix} \\C & \\C \\end{bmatrix}) *\n",
    "                     \\begin{bmatrix} \\Ffull[\\prime] & \\Ffull[\\prime] \\end{bmatrix} *\n",
    "                     \\begin{bmatrix} \\C[-1] & \\C[-1] \\end{bmatrix} *\n",
    "                     \\begin{bmatrix} \\myH{orange}^T \\\\ \\myH{orange}^T \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Which we can implement, making use of `numpy`'s broadcasting operations, just as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdUf = np.zeros_like(Uf)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dy_dft = w*(o[t]*dtanh(c[t+1])) * c[t] * dsigmoid(Wf.dot(xt)+Uf.dot(h[t]))\n",
    "    dLossdUf += -(y-y_[t]) * dy_dft.reshape(-1, 1) * h[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights of the input gate\n",
    "\n",
    "Looking at the weights of the input gate we can proceed in the familiar fashion. We procure the gradients of $loss_t$ with respect to $\\Wi$ as\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial W_i} &= \\sum\\frac{\\partial loss_t}{\\partial \\hat{y}_t} \\overbrace{\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial c_t} \\frac{\\partial c_t}{\\partial i_t}\\frac{\\partial i_t}{\\partial \\Wi}}^{\\deriv{\\yhat}{\\Wi}} \\\\\n",
    "                                   &= \\sum -(y-\\hat{y}_t)\\ w * \\O * g^\\prime(\\C) * \\Ctil * \\Ifull[\\prime] \\dxt{red}.\n",
    "\\end{align}\n",
    "\n",
    "Subsequently, we can implement the computation of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdWi = np.zeros_like(Wi)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dLossdWi += -(y-y_[t]) * w * (o[t] * dtanh(c[t+1])) * c_[t] * dsigmoid(Wi.dot(xt) + Ui.dot(h[t])) * xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently we proceed with the gradient of $loss_t$ with respect to $\\Ui$, which are given by\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial U_i} &= \\sum\\frac{\\partial loss_t}{\\partial \\hat{y}_t} \\overbrace{\\frac{\\partial \\hat{y}_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial c_t} \\frac{\\partial c_t}{\\partial i_t} \\frac{\\partial i_t}{\\partial U_i}}^{\\deriv{\\yhat}{\\Ui}} \\\\\n",
    "                                   &= \\sum(y-\\hat{y}_t)\\ \\begin{bmatrix} w & w \\end{bmatrix} *\n",
    "                      \\begin{bmatrix} \\O & \\O \\end{bmatrix} *\n",
    "                      g^\\prime\\left(\\begin{bmatrix} \\C & \\C \\end{bmatrix}\\right) *\n",
    "                      \\begin{bmatrix} \\Ctil & \\Ctil \\end{bmatrix} * \\\\\n",
    "                      &\\quad\\quad\\begin{bmatrix} \\Ifull[\\prime] & \\Ifull[\\prime] \\end{bmatrix} *\n",
    "                      \\begin{bmatrix} \\myH{red}^T \\\\ \\myH{red}^T \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Which we implement by making use of `numpy`'s broadcasting functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdUi = np.zeros_like(Ui)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dy_dit = w * (o[t]*dtanh(c[t])) * c_[t] * dsigmoid(Wi.dot(xt)+Ui.dot(h[t]))\n",
    "    dLossdUi += -(y-y_[t]) * dy_dit.reshape(-1, 1) * h[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights of the candidate update\n",
    "\n",
    "Again we begin by computint the gradient of $loss_t$ with respect to $\\Wc$, such that we get\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial W_c} &= \\sum\\frac{\\partial loss_t}{\\partial \\hat{y}_t}\\overbrace{\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial c_t}\\frac{\\partial c_t}{\\partial \\tilde{c}_t} \\frac{\\partial \\tilde{c}_t}{\\partial W_c}}^{\\deriv{\\yhat}{\\Wc}} \\\\\n",
    "                                   &= \\sum(y - \\hat{y}_t)\\ w * \\O * g^\\prime(\\C) * \\I * \\Ctilfull[\\prime]\\ \\dxt{green}\n",
    "\\end{align}\n",
    "\n",
    "which we implement in the same unequivocal manner as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdWc = np.zeros_like(Wc)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dLossdWc += -(y-y_[t]) * w * (o[t] * dtanh(c[t+1])) * i[t] * dtanh(Wc.dot(xt) + Uc.dot(h[t])) * xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we obtain the gradient of the loss function with respect to $\\Uc$ as\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial loss}{\\partial U_c} &= \\sum\\frac{\\partial loss_t}{\\partial \\hat{y}_t}\\overbrace{\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial c_t}\\frac{\\partial c_t}{\\partial \\tilde{c}_t}\\frac{\\partial \\tilde{c}_t}{\\partial U_c}}^{\\deriv{\\yhat}{\\Uc}} \\\\\n",
    "                          &= \\sum(y_t - \\hat{y}_t)\\ w * \\O * g^\\prime(\\C) * \\I * \\Ctilfull[\\prime]\\ * \\begin{bmatrix} h^T_{t-1} \\\\ h^T_{t-1} \\end{bmatrix},\n",
    "\\end{align}\n",
    "\n",
    "which by now should be easy to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdUc = np.zeros_like(Uc)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dc_tdf = w * (o[t] * dtanh(c[t+1])) * i[t] * dtanh(Wc.dot(xt)+Uc.dot(h[t]))\n",
    "    dLossdUc += -(y-y_[t]) * dc_tdf.reshape(-1, 1) * h[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights of the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to adjust the weights of the output layer. The gradient of the loss function is simply\n",
    "\n",
    "\\begin{equation}\n",
    "\\deriv{loss}{w} = \\sum\\deriv{loss_t}{w} = \\sum\\deriv{loss_t}{\\yhat}\\deriv{\\yhat}{w} = \\sum -(y - \\yhat) \\myH[]{red}.\n",
    "\\end{equation}\n",
    "\n",
    "Which can be readily implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdw = np.zeros_like(w)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dLossdw += -(y - y_[t]) * h[t+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight update\n",
    "\n",
    "With all gradients being computed, we can use them in order to update our weights. Typically, when using gradient descent, we follow the direction of the negative gradient, multiplied by some fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.01\n",
    "\n",
    "Wc -= eta * dLossdWc\n",
    "Wo -= eta * dLossdWo\n",
    "Wf -= eta * dLossdWf\n",
    "Wi -= eta * dLossdWi\n",
    "\n",
    "Uc -= eta * dLossdUc\n",
    "Uo -= eta * dLossdUo\n",
    "Uf -= eta * dLossdUf\n",
    "Ui -= eta * dLossdUi\n",
    "\n",
    "w -= eta * dLossdw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the new weights we can compute the updated prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_after = LSTM_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$\\hat{y}_{before}$</th>\n",
       "      <th>$\\hat{y}_{after}$</th>\n",
       "      <th>$y$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.046038</td>\n",
       "      <td>4.956741</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   $\\hat{y}_{before}$  $\\hat{y}_{after}$  $y$\n",
       "1            2.046038           4.956741  7.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({r\"$\\hat{y}_{before}$\": yhat_before, r\"$\\hat{y}_{after}$\": yhat_after, r\"$y$\": y}, index=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can clearly see, we are getting close to the real $y$. In a real application one would of course evaluate the gradients at multiple samples and use an average gradient in order to update the weight matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation GRU\n",
    "$\n",
    "% Vector shortcuts\n",
    "\\newcommand{\\Htil}{\\color{green}{\\tilde{h}_t}}\n",
    "\\newcommand{\\H}{\\color{green}{h_{t-1}}}\n",
    "\\newcommand{\\R}{\\color{blue}{r_t}}\n",
    "\\newcommand{\\Z}{\\color{red}{z_t}}\n",
    "% Gates Matrix Notation\n",
    "\\newcommand{\\Gfull}[1][]{\\color{green}{g^{#1}\\left( W_h x_t + U_h \\cdot ( \\R * \\H)\\right)}}\n",
    "\\newcommand{\\Zfull}[1][]{\\color{red}{\\sigma^{#1} \\left( W_z x_t + U_z \\cdot h_{t-1} \\right)}}\n",
    "\\newcommand{\\Rfull}[1][]{\\color{blue}{\\sigma^{#1} \\left( W_r x_t + U_r \\cdot h_{t-1} \\right)}}\n",
    "% Vector Elements\n",
    "\\newcommand{\\htil}[1]{\\color{green}{\\tilde{h}^t_{#1}}}\n",
    "\\newcommand{\\h}[1]{\\color{green}{h_{#1}^{t-1}}}\n",
    "\\newcommand{\\z}[1]{\\color{red}{z_{#1}^{t}}}\n",
    "\\newcommand{\\r}[1]{\\color{blue}{r_{#1}^{t}}}\n",
    "% Explicit Vector Elements\n",
    "\\newcommand{\\gfull}[2][]{\\color{green}{g^{#1} \\left( w^h_{#2} x_t + u^h_{#2 1} \\r{1} \\h{1} + u^h_{#2 2} \\r{2} \\h{2} \\right)}}\n",
    "\\newcommand{\\zfull}[2][]{\\color{red}{\\sigma^{#1} \\left(w^z_{#2} x_t + u^z_{#2 1}h_1^{t-1}+u^z_{#2 2} h_2^{t-1}\\right)}}\n",
    "\\newcommand{\\rfull}[2][]{\\color{blue}{\\sigma^{#1} \\left(w^r_{#2} x_t + u^r_{#2 1}h_1^{t-1}+u^r_{#2 2} h_2^{t-1}\\right)}}\n",
    "\\newcommand{\\gfullfull}[2][]{\\color{green}{g^{#1} \\left( w^h_{#2} x_t + u^h_{#2 1} \\rfull{1} \\h{1} + u^h_{#2 2} \\rfull{2} \\h{2} \\right)}}\n",
    "% Weight Matrices\n",
    "\\newcommand{\\Wh}{\\color{green}{W_h}}\n",
    "\\newcommand{\\Wz}{\\color{red}{W_z}}\n",
    "\\newcommand{\\Wr}{\\color{blue}{W_r}}\n",
    "\\newcommand{\\Uh}{\\color{green}{U_h}}\n",
    "\\newcommand{\\Uz}{\\color{red}{U_z}}\n",
    "\\newcommand{\\Ur}{\\color{blue}{U_r}}\n",
    "% Weight Matrix Elements\n",
    "\\newcommand{\\wh}[1]{\\color{green}{w^h_{#1}}}\n",
    "\\newcommand{\\wz}[1]{\\color{red}{w^z_{#1}}}\n",
    "\\newcommand{\\wr}[1]{\\color{blue}{w^r_{#1}}}\n",
    "\\newcommand{\\uh}[2]{\\color{green}{u^h_{#1 #2}}}\n",
    "\\newcommand{\\uz}[2]{\\color{red}{u^z_{#1 #2}}}\n",
    "\\newcommand{\\ur}[2]{\\color{blue}{u^r_{#1 #2}}}\n",
    "% Miscellaneous\n",
    "\\newcommand{\\dxt}[1]{\\color{#1}{x_t}}\n",
    "\\newcommand{\\yhat}{\\hat{y}_t}\n",
    "\\newcommand{\\deriv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\DeclareMathOperator{\\diag}{diag}\n",
    "$\n",
    "Consider again, our low-dimensional numerical example with only one case with features $x = \\begin{pmatrix}x_1, & x_2, & x_3 \\end{pmatrix}^T$ and label $y$. Additionally let's again assume we only have two neurons in our network. This means for the weight matrices \n",
    "\n",
    "\\begin{alignat}{2}\n",
    "\\color{green}{W_h} &= \\begin{bmatrix} \\wh{1} \\\\ \\wh{2} \\end{bmatrix}  \\quad\n",
    "\\color{green}{U_h} &&= \\begin{bmatrix} \\uh{1}{1} & \\uh{1}{2} \\\\ \\uh{2}{1} & \\uh{2}{2}\\end{bmatrix}\\\\ \n",
    "\\color{red}{W_z} &= \\begin{bmatrix} \\wz{1} \\\\ \\wz{2} \\end{bmatrix}  \\quad\n",
    "\\color{red}{U_z} &&= \\begin{bmatrix} \\uz{1}{1} & \\uz{1}{2} \\\\ \\uz{2}{1} & \\uz{2}{2} \\end{bmatrix}\\\\\n",
    "\\color{blue}{W_r} &= \\begin{bmatrix} \\wr{1} \\\\ \\wr{2} \\end{bmatrix} \\quad\n",
    "\\color{blue}{U_r} &&= \\begin{bmatrix} \\ur{1}{1} & \\ur{1}{2} \\\\ \\ur{2}{1} & \\ur{2}{2} \\end{bmatrix} \n",
    "\\end{alignat}\n",
    "\n",
    "We define the potential candidate update as\n",
    "\\begin{equation}\n",
    "\\Htil = \\Gfull,\n",
    "\\end{equation}\n",
    "\n",
    "the gates as\n",
    "\n",
    "\\begin{align}\n",
    "\\Z &= \\Zfull \\quad\\text{and}\\\\\n",
    "\\R &= \\Rfull,\n",
    "\\end{align}\n",
    "\n",
    "where as before, $g$ is the $tanh$-function and $\\sigma$ the sigmoid function.\n",
    "\n",
    "Finally, we define the update of the hidden state as\n",
    "\\begin{equation}\n",
    "h_t = \\color{red}{1-\\Z} * h_{t-1} + \\Z * \\Htil\n",
    "\\end{equation} \\[[cp Rey, Salem, 2017](https://arxiv.org/pdf/1701.05923.pdf)\\].\n",
    "\n",
    "For the actual prediction at a step $t$, we connect every output of our hidden state in a dense layer. Which means that we are taking a weighted sum of all two of them.\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y}_t = W^T h_t,\n",
    "\\end{equation}\n",
    "\n",
    "Where $W = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}$ is the matrix that contains the weights for the outer layer. In our case this is just a $2\\times 1$ vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a simple example containing just one sample $x \\in \\mathbb{R}^2$, suppose that the the weights at a certain point look like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sample\n",
    "x = np.array([0.2, 0.3, 0.4])\n",
    "y = 7.0\n",
    "\n",
    "# Initialize Weights\n",
    "Wh = np.array([0.2, 0.9])\n",
    "\n",
    "Wz = np.array([0.1, 3.1])\n",
    "Wr = np.array([2.3, 0.5])\n",
    "\n",
    "Uh = np.array([[1.5, 2.6], [1.8, 3.6]])\n",
    "Uz = np.array([[0.1, 4.1], [0.2, 1.0]])\n",
    "Ur = np.array([[1.3, 7.1], [9.1, 4.5]])\n",
    "\n",
    "w = np.array([2.0, 4.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement the forward pass like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = []\n",
    "h_ = []\n",
    "z = []\n",
    "r = []\n",
    "y_ = []\n",
    "\n",
    "def GRU_forward():\n",
    "    \"\"\"Perform forward pass.\"\"\"\n",
    "    h.append(np.zeros_like(Wh))\n",
    "    \n",
    "    for t, xt in enumerate(x, start=len(y_)):\n",
    "        # Calculate values of the gates\n",
    "        zt = sigmoid(Wz.dot(xt) + Uz.dot(h[t]))\n",
    "        rt = sigmoid(Wr.dot(xt) + Ur.dot(h[t]))\n",
    "\n",
    "        # Calculate candidate update\n",
    "        h_t = tanh(Wh.dot(xt) + Uh.dot(rt * h[t]))\n",
    "\n",
    "        # Calculate hidden state\n",
    "        ht = (1-zt) * h[t] + zt * h_t\n",
    "\n",
    "        # Calculate prediction at step t\n",
    "        y_t = w.dot(ht)\n",
    "\n",
    "        # Save variables to container\n",
    "        h.append(ht)\n",
    "        h_.append(h_t)\n",
    "        z.append(zt)\n",
    "        r.append(rt)\n",
    "        y_.append(y_t)\n",
    "    \n",
    "    return y_[-1]\n",
    "\n",
    "yhat_before = GRU_forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we want to keep track of the prediction before updating the weights, such that we can compare it with the prediction we obtain after we update our weight matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights for the candidate $\\tilde{h}_t$\n",
    "\n",
    "Let's start with investigating the gradient of $loss_t$ with respect to $\\Wh$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial {loss}_t}{\\partial W_h} &= \\frac{\\partial {loss}_t}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial \\tilde{h}_t}\\frac{\\partial \\tilde{h}_t}{\\partial \\Wh} \\\\\n",
    "                                   &= -(y-\\hat{y}_t) W * \\Z * \\Gfull[\\prime] \\color{green}{x_t}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdWh = np.zeros_like(Wh)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    # Note that `h` has an entry at start, so indexing at t accesses h_{t-1}\n",
    "    dh_tdWh = dtanh(Wh.dot(xt) + Uh.dot(r[t] * h[t])) * xt\n",
    "    dLossdWh += -(y-y_[t]) * w * z[t] * dh_tdWh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, following the logic of the backpropagation algorithm for the gradient of $loss_t$ with respect to $\\Uh$, we obtain\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial {loss}_t}{\\partial \\Uh} &= \\frac{\\partial {loss}_t}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial \\tilde{h}_t}\\frac{\\partial \\tilde{h}_t}{\\partial \\Uh} \\\\\n",
    "                                    &= -(y-\\hat{y}_t)\\ \\diag \\left[ w * \\Z * \\Gfull[\\prime]\\right] \\cdot  \\begin{bmatrix} (\\R * \\H)^T \\\\ (\\R * \\H)^T \\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdUh = np.zeros_like(Uh)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dy_dh_t = w * z[t] * dtanh(Wh.dot(xt) + Uh.dot(r[t] * h[t]))\n",
    "    dLossdUh += -(y-y_[t]) * dy_dh_t.reshape(-1, 1) * h[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights for the update gate $z_t$\n",
    "\n",
    "Using the same logic, we calculate the gradient of $loss_t$ with respect to $\\Wz$, which gives \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial {loss}_t}{\\partial W_z} &= \\frac{\\partial {loss}_t}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial z_t}\\frac{\\partial z_t}{\\partial W_z} \\\\\n",
    "                                   &= -(y-\\hat{y}_t)\\ w * \\begin{bmatrix} -\\H + \\Htil \\end{bmatrix} * \\Zfull[\\prime] \\dxt{red}.\n",
    "\\end{align}\n",
    "\n",
    "Which in turn we can translate into `Python` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdWz = np.zeros_like(Wz)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dztdWz = dsigmoid(Wz.dot(xt) + Uz.dot(h[t])) * xt\n",
    "    dLossdWz += -(y-y_[t]) * w * (-h[t+1] - h_[t]) * dztdWz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now this should be getting old. We use the previous derivations in order to find the gradient of the loss function with respect to $\\Uz$, where now one can easily see that:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial {loss}_t}{\\partial \\Uz} &= \\frac{\\partial loss}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial z_t}\\frac{\\partial z_t}{\\partial \\Uz} \\\\\n",
    "                                   &= -(y-\\hat{y}_t)\\ \\diag \\left[ w * [-\\H + \\Htil] * \\Zfull[\\prime]\\right] \\cdot \\begin{bmatrix} \\H^T \\\\ \\H^T \\end{bmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "Having done that, we can implement it in the now too familiar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdUz = np.zeros_like(Uz)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    dy_dzt = w * [-h[t+1] + h_[t] * dsigmoid(Wz.dot(xt) + Uz.dot(h[t]))]\n",
    "    dLossdUz += -(y-y_[t]) * dy_dzt.reshape(-1, 1) * h[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights for the reset gate $r_t$\n",
    "\n",
    "Similarly, we can proceed with obtaining the gradients of $loss_t$ with respect to the elements of the reset gate.\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial {loss}_t}{\\partial \\Wr} &= \\frac{\\partial {loss}_t}{\\partial \\hat{y}_t}\\frac{\\partial \\hat{y}_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial \\tilde{h}_t}\\frac{\\partial \\tilde{h}_t}{\\partial \\Wr} \\\\\n",
    "                                   &= -(y-\\hat{y}_t)\\ w^T \\cdot \\left(\n",
    "\\begin{bmatrix} \\Z & \\Z \\end{bmatrix} * \n",
    "\\begin{bmatrix} \\Gfull[\\prime] & \\Gfull[\\prime] \\end{bmatrix} * \n",
    "\\Uh *\n",
    "\\begin{bmatrix} \\H^T \\\\ \\H^T \\end{bmatrix} *\n",
    "\\begin{bmatrix} \\Rfull[\\prime]^T \\\\ \\Rfull[\\prime]^T \\end{bmatrix}\n",
    "\\right) \\dxt{blue}.\n",
    "\\end{align}\n",
    "\n",
    "Which we also implement in `Python`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdWr = np.zeros_like(Wr)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    z_dg = z[t] * dtanh(Wh.dot(xt) + Uh.dot(r[t] * h[t]))\n",
    "    d_rt = dsigmoid(Wr.dot(xt) + Ur.dot(h[t]))\n",
    "    dLossdWr += -(y-y_[t]) * w.dot(z_dg.reshape(-1, 1) * Uh * h[t] * d_rt) * xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the gradient of $loss_t$ with respect to $\\Ur$\n",
    "\n",
    "\\begin{align}\n",
    "\\deriv{loss_t}{\\Ur} &= -(y-\\yhat) \\begin{bmatrix}(w * \\Z * \\Gfull[\\prime])^T \\\\ (w * \\Z * \\Gfull[\\prime])^T \\end{bmatrix} \\cdot \\Uh^T  * \n",
    "\\begin{bmatrix} \\H \\cdot \\H^T \\end{bmatrix} *\n",
    "\\begin{bmatrix} \\Rfull[\\prime] & \\Rfull[\\prime] \\end{bmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "We implement this gradient in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLossdUr = np.zeros_like(Ur)\n",
    "\n",
    "for t, xt in enumerate(x):\n",
    "    w_z_dg = w * z[t] * dtanh(Wh.dot(xt) + Uh.dot(r[t] * h[t]))\n",
    "    outer_h = np.outer(h[t], h[t])\n",
    "    dr = dsigmoid(Wr.dot(xt)+Ur.dot(h[t])).reshape(-1, 1)\n",
    "\n",
    "    dLossdUr += -(y - y_[t]) * w_z_dg.dot(Uh.T)*outer_h * dr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update the weights\n",
    "\n",
    "Now that we have computed the gradients of the loss function with respect to any of the weight matrices, we can update the weights and then see if this did indeed improve the prediction of our toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1\n",
    "Wr -= eta * dLossdWr\n",
    "Wz -= eta * dLossdWz\n",
    "Wh -= eta * dLossdWh\n",
    "\n",
    "Ur -= eta * dLossdUr\n",
    "Uz -= eta * dLossdUz\n",
    "Uh -= eta * dLossdUh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform the forward pass again with the updated weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_after = GRU_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$\\hat{y}_{before}$</th>\n",
       "      <th>$\\hat{y}_{after}$</th>\n",
       "      <th>$y$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.14414</td>\n",
       "      <td>5.986334</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   $\\hat{y}_{before}$  $\\hat{y}_{after}$  $y$\n",
       "1             5.14414           5.986334  7.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({r\"$\\hat{y}_{before}$\": yhat_before, r\"$\\hat{y}_{after}$\": yhat_after, r\"$y$\": y}, index=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed, we are better than before. If we were to reiterate these steps multiple times we could get arbitrarily close to the true value of $7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37464,
     "status": "ok",
     "timestamp": 1545031948062,
     "user": {
      "displayName": "Thomas Siskos",
      "photoUrl": "",
      "userId": "07314453599490335660"
     },
     "user_tz": -60
    },
    "id": "brnADKcIMsmj",
    "outputId": "3d54f8c1-81e0-4786-d202-782d6c296ea7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "\n",
    "from keras.layers import SimpleRNN, LSTM, GRU\n",
    "from models.recurrent import Recurrent\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from hyperopt import tpe, hp, fmin, STATUS_OK, Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UIZeKG-hMsmT"
   },
   "source": [
    "# Parameter Tuning using `hyperopt`\n",
    "\n",
    "Since we now know what we are doing and how our models work it's time to turn our attention back to our initial problem at hand and to fine tune our neural nets. Since we want to be able to make a fair comparison among the three different types of neural networks we need to tune them in a similar fashion, and to be precise we should also tune the same hyperparameters for them. A nice way to do that is to use the `hyperopt` module. With it, we will use a Tree of Parzen Estimators to find the optimal set of hyperparameters (for more information regarting Tree of Parzen Estimators consider the Appendix).\n",
    "\n",
    "Additionally, to facilitate the fitting of numerous models we opted to automate the data generation process and to gather other assorted functionality in a custom built subclass of the `keras.Sequential` class. We will explain selected features of this handcrafted `models.Recurrent` class along the way. There is also a full [documentation](https://github.com/thsis/INFOSYS/blob/master/README.md) available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q46OZEhvMsmz"
   },
   "source": [
    "### Load the series by district:\n",
    "\n",
    "First we want to load the data and here we want to explicitly parse the dates inside the `Date` column and specify a `MultiIndex` where the levels are `Date` and then `District`. This allows our `models.Recurrent`-class to distinguish between the series for the whole of Chicago and the Chicago crime series by each district. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1358,
     "status": "ok",
     "timestamp": 1545032014547,
     "user": {
      "displayName": "Thomas Siskos",
      "photoUrl": "",
      "userId": "07314453599490335660"
     },
     "user_tz": -60
    },
    "id": "reOGQNIHMsm0",
    "outputId": "95d06ada-8c47-49e1-a0df-63ce8d71f7b1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th colspan=\"21\" halign=\"left\">2001-01-01</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>District</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>5.0</th>\n",
       "      <th>6.0</th>\n",
       "      <th>7.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>9.0</th>\n",
       "      <th>10.0</th>\n",
       "      <th>...</th>\n",
       "      <th>16.0</th>\n",
       "      <th>17.0</th>\n",
       "      <th>18.0</th>\n",
       "      <th>19.0</th>\n",
       "      <th>20.0</th>\n",
       "      <th>21.0</th>\n",
       "      <th>22.0</th>\n",
       "      <th>24.0</th>\n",
       "      <th>25.0</th>\n",
       "      <th>31.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Incidents</th>\n",
       "      <td>37.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>...</td>\n",
       "      <td>67.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Date      2001-01-01                                                      \\\n",
       "District        1.0    2.0    3.0   4.0   5.0   6.0   7.0    8.0    9.0    \n",
       "Incidents       37.0  110.0  103.0  96.0  95.0  84.0  83.0  111.0  109.0   \n",
       "\n",
       "Date             ...                                                         \\\n",
       "District    10.0 ...   16.0  17.0  18.0  19.0  20.0 21.0  22.0  24.0   25.0   \n",
       "Incidents  104.0 ...   67.0  67.0  72.0  72.0  40.0  0.0  61.0  59.0  120.0   \n",
       "\n",
       "Date            \n",
       "District  31.0  \n",
       "Incidents  0.0  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath_district = os.path.join(\"..\", \"data\", \"crimes_district.csv\")\n",
    "district = pd.read_csv(datapath_district, index_col=[\"Date\", \"District\"],\n",
    "                       dtype={\"District\": object,\n",
    "                              \"Incidents\": np.float32},\n",
    "                       parse_dates=[\"Date\"])\n",
    "district.sort_index().head(24).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is just a series and not exactly a matrix of features, as we know it from traditional statistics or even other architectures for neural networks. Therefore we take the series as it is to be our **target** variable and we construct multiple columns by lagging the series by up to $l$ days, where $l \\in \\mathbb{N}$. If the maximum number of lags is, for example, $l = 3$ this means that we try to predict the number of cases today, just by knowing the number of cases that occured the previous day, the day before that and the day before that day. This is handily included in the `models.Recurrent` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Recurrent(Sequential):\n",
    "    def __init__(self, ...):\n",
    "        # Lots of code...\n",
    "    \n",
    "    def __get_features(self):\n",
    "        cols = [\"lag_\" + str(i) for i in range(1, self.maxlag+1)]\n",
    "\n",
    "        for i, colname in enumerate(cols, 1):\n",
    "            if isinstance(self.data.index, pd.core.index.MultiIndex):\n",
    "                lag = self.data.groupby(self.cross_label)[\"y\"].shift(i)\n",
    "            else:\n",
    "                lag = self.data[\"y\"].shift(i)\n",
    "\n",
    "            self.data[colname] = lag\n",
    "\n",
    "        self.data = self.data.dropna(axis=0)\n",
    "        # Even more code...        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include the maximum number of lags as another hyperparameter, in order to cut down on training time. Besides, we have no reason to believe that the number of crime incidents in the far past, say more than a year ago, would be a good indicator for the number of occuring acts of crime tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JhYEx224Msm6"
   },
   "source": [
    "### Separate data and a holdout set.\n",
    "\n",
    "Because we use a library that tunes parameters with regards to the test set, we need to separate a holdout set. On this set, which we will not expose to our models we can evaluate the performance of our tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q4-q0b1DMsm6"
   },
   "outputs": [],
   "source": [
    "# Define breaking points for train and holdout set.\n",
    "lower = pd.to_datetime(\"2015-01-01\")\n",
    "upper = pd.to_datetime(\"2017-01-01\")\n",
    "end = pd.to_datetime(\"2018-01-01\")\n",
    "# Divide dataset.\n",
    "district_data = copy.deepcopy(district.loc[district.index.map(lambda x: lower <= x[0] < upper), :])\n",
    "district_holdout = copy.deepcopy(district.loc[district.index.map(lambda x: upper <= x[0] < end), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uHf-qUidMsnE"
   },
   "source": [
    "## Hyperopt parameter spaces\n",
    "\n",
    "Most of our models have similar parameters which we want to tune. For the sampling of the subspace we use mostly uniform distributions, since they impose the least a-priori assumptions on where the optimal parameters lie. Additionally, mostly due to hardware limitations, we restrict the number of maximum lags to be no higher than $100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PgJDORp9MsnF"
   },
   "outputs": [],
   "source": [
    "# add optimizer, learn-rate, \n",
    "paramspace = {\"maxlag\": scope.int(hp.quniform(\"maxlag\", 1, 100, 1)),\n",
    "              \"cell_neurons\": scope.int(hp.quniform(\"cell_neurons\", 1, 30, 1)),\n",
    "              \"batch_size\": scope.int(hp.quniform(\"batch_size\", 1, 100, 1)),\n",
    "              \"optimizer\": hp.choice(\"optimizer\", [\"adam\", \"sgd\"])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T9XaIyMxMsnK"
   },
   "source": [
    "We have defined a base parameter space dictionary which we update with the parameters which are more specific to each cell. This can get out of hand pretty fast, therefore we generate a dictionary through a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1545032025243,
     "user": {
      "displayName": "Thomas Siskos",
      "photoUrl": "",
      "userId": "07314453599490335660"
     },
     "user_tz": -60
    },
    "id": "1zohqnNLMsnL",
    "outputId": "3a090bda-192b-4a23-da4c-dcd4a22d1628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GRU': {'batch_size': <hyperopt.pyll.base.Apply object at 0x7fae81b2f9b0>,\n",
      "         'cell': <class 'keras.layers.recurrent.GRU'>,\n",
      "         'cell_neurons': <hyperopt.pyll.base.Apply object at 0x7fae81b2f7f0>,\n",
      "         'maxlag': <hyperopt.pyll.base.Apply object at 0x7fae81b28e10>,\n",
      "         'optimizer': <hyperopt.pyll.base.Apply object at 0x7fae81b2fa90>},\n",
      " 'LSTM': {'batch_size': <hyperopt.pyll.base.Apply object at 0x7fae81b2f9b0>,\n",
      "          'cell': <class 'keras.layers.recurrent.LSTM'>,\n",
      "          'cell_neurons': <hyperopt.pyll.base.Apply object at 0x7fae81b2f7f0>,\n",
      "          'maxlag': <hyperopt.pyll.base.Apply object at 0x7fae81b28e10>,\n",
      "          'optimizer': <hyperopt.pyll.base.Apply object at 0x7fae81b2fa90>},\n",
      " 'SimpleRNN': {'batch_size': <hyperopt.pyll.base.Apply object at 0x7fae81b2f9b0>,\n",
      "               'cell': <class 'keras.layers.recurrent.SimpleRNN'>,\n",
      "               'cell_neurons': <hyperopt.pyll.base.Apply object at 0x7fae81b2f7f0>,\n",
      "               'maxlag': <hyperopt.pyll.base.Apply object at 0x7fae81b28e10>,\n",
      "               'optimizer': <hyperopt.pyll.base.Apply object at 0x7fae81b2fa90>}}\n"
     ]
    }
   ],
   "source": [
    "spacesdict ={}\n",
    "for cell in (SimpleRNN, LSTM, GRU):\n",
    "    spacesdict[cell.__name__] = {\"cell\": cell, **paramspace}\n",
    "\n",
    "pprint(spacesdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OUrdVXZKMsnR"
   },
   "source": [
    "In order to store the results of each `hyperopt`-search we can create a `hyperopt.Trials`-object, which is very similar to a native Python `dictionary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1075,
     "status": "ok",
     "timestamp": 1545032028697,
     "user": {
      "displayName": "Thomas Siskos",
      "photoUrl": "",
      "userId": "07314453599490335660"
     },
     "user_tz": -60
    },
    "id": "CSGL-pKDMsnS",
    "outputId": "d17860c2-bb79-45a3-893f-c212672904c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GRU': <hyperopt.base.Trials object at 0x7fae9e4e7978>,\n",
      " 'LSTM': <hyperopt.base.Trials object at 0x7fae9e4e77b8>,\n",
      " 'SimpleRNN': <hyperopt.base.Trials object at 0x7fae9e4e7908>}\n"
     ]
    }
   ],
   "source": [
    "trialsdict = {model: Trials() for model in (\"SimpleRNN\", \"LSTM\", \"GRU\")}\n",
    "pprint(trialsdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HKjvtKXRMsnZ"
   },
   "source": [
    "## Optimizing over the parameter space\n",
    "\n",
    "The design of the `hyperopt`-module demands of us to define an objective function, which we then optimize by calling the `hyperopt.fmin` function. And of course, we could do that. However, the objective function will be extremely costly to evaluate - - and there is nothing to be done about that - we only add a progress bar, since there is no visual representation of the algorithms' progress. That's why we will use the `Python`-decorator syntax, where we decorate our model call with the progress bar from the `tqdm` module and the `fmin`-function from `hyperopt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x8StfQyQMsnc"
   },
   "outputs": [],
   "source": [
    "def minimizer(objective):\n",
    "    def outer(paramspace, trials, max_evals=100):\n",
    "        \"\"\"Generate an inner objective-function and optimize it.\"\"\"\n",
    "        pbar = tqdm(total=max_evals, desc=paramspace[\"cell\"].__name__)\n",
    "        def inner(*args, **kwargs):\n",
    "            \"\"\"Update the progress bar and call the objective function.\"\"\"\n",
    "            pbar.update()\n",
    "            return objective(*args, **kwargs)\n",
    "\n",
    "        best = fmin(fn=inner,\n",
    "                    space=paramspace,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=max_evals,\n",
    "                    trials=trials)\n",
    "        pbar.close()\n",
    "        return best\n",
    "    return outer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, each time the nested function `inner` gets called, it will update the progress bar. Giving us a rough idea on how long it will take until we get our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uyUt7nxxMsnm"
   },
   "source": [
    "And now to the function we wish to decorate. It contains 3 steps:\n",
    "1. Create a model with a certain set of (yet undefined) parameters.\n",
    "2. Train the model.\n",
    "3. Calculate the loss on a test set and return a dictionary, that contains the loss and a flag indicating that everything went okay.\n",
    "\n",
    "We choose a very simplistic architecture, where we have only the input layer, a hidden layer which we call - in accordance to the `keras` documentation - a *cell*. This *cell* can be a `keras.SimpleRNN`, a `keras.LSTM` or a `keras.GRU` object. Finally, we add a densely connected output layer, that we provide with a Rectified Linear Unit (ReLu) activation function, this step is just a precaution, since we do not want our model to generate predictions which are lower than zero.\n",
    "\n",
    "```python\n",
    "class Recurrent(Sequential):\n",
    "    def __init__(self, ...):\n",
    "        # Lots of code...\n",
    "       \n",
    "    def train(self):\n",
    "        \"\"\"Train the model based on parameters passed to `__init__`.\"\"\"\n",
    "        X_train = self.__transform_shape(self.X_train)\n",
    "\n",
    "        self.add(self.cell(self.cell_neurons,\n",
    "                           input_shape=(1, self.maxlag),\n",
    "                           **self.cellkwargs))\n",
    "        self.add(Dense(1))\n",
    "        self.add(Activation('relu'))\n",
    "\n",
    "        self.compile(loss=self.lossfunc,\n",
    "                     optimizer=self.optimizer,\n",
    "                     metrics=self.metrics)\n",
    "        self.fit(X_train, self.y_train,\n",
    "                 epochs=self.epochs,\n",
    "                 batch_size=self.batch_size,\n",
    "                 verbose=self.verbose,\n",
    "                 **self.fitkwargs)\n",
    "        \n",
    "    # Further code...\n",
    "```\n",
    "\n",
    "And while defining the function, we can pass it directly to our decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RBmT4lysMsn3"
   },
   "outputs": [],
   "source": [
    "@minimizer\n",
    "def district_get_loss(params):\n",
    "    \"\"\"Return loss on test set.\"\"\"\n",
    "    model = Recurrent(district_data, epochs=3, verbose=False, **params)\n",
    "    model.train()\n",
    "    predictions = model.forecast(model.X_test)\n",
    "    loss = mean_squared_error(y_true=model.y_test, y_pred=predictions)\n",
    "    return {'loss': loss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O5mNxCx_MsoA"
   },
   "source": [
    "The above syntax is just a shortcut for\n",
    "```python\n",
    "district_get_loss = minimizer(district_get_loss)\n",
    "```\n",
    "\n",
    "And now to tuning the hyperparameters. Again we pass through all our models and store the best hyperparameters in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2568
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12589980,
     "status": "error",
     "timestamp": 1544988819688,
     "user": {
      "displayName": "Thomas Siskos",
      "photoUrl": "",
      "userId": "07314453599490335660"
     },
     "user_tz": -60
    },
    "id": "A44sibRMMsoB",
    "outputId": "61e6aab6-87e0-4226-c925-e6ec52477ee3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855ba9167bb64727bea92e98c8159619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='SimpleRNN', style=ProgressStyle(description_width='initial'))â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eae0e52b965416795ca91eb2c0c9f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='LSTM', style=ProgressStyle(description_width='initial')), HTMâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4aaccebcf7e4d56bccbc2ae6954cc11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='GRU', style=ProgressStyle(description_width='initial')), HTMLâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best = {}\n",
    "\n",
    "for model in \"SimpleRNN\", \"LSTM\", \"GRU\":\n",
    "    best[model] = district_get_loss(spacesdict[model], \n",
    "                                    trialsdict[model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NYu-PTXYMsoL"
   },
   "source": [
    "The reason why we use dictionaries so much is, because we can easily save them as a `json`-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jqEriUrWMsoN"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(\"..\", 'analysis', \"models\", 'best_params.json'), 'w') as outfile:\n",
    "    json.dump(best, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 626,
     "status": "ok",
     "timestamp": 1544988825300,
     "user": {
      "displayName": "Thomas Siskos",
      "photoUrl": "",
      "userId": "07314453599490335660"
     },
     "user_tz": -60
    },
    "id": "51tPczCBMsoE",
    "outputId": "a49c8c84-b708-4d22-8a1b-01ffd0045896"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GRU': {'batch_size': 2.0,\n",
      "         'cell_neurons': 26.0,\n",
      "         'maxlag': 44.0,\n",
      "         'optimizer': 1},\n",
      " 'LSTM': {'batch_size': 5.0,\n",
      "          'cell_neurons': 17.0,\n",
      "          'maxlag': 84.0,\n",
      "          'optimizer': 1},\n",
      " 'SimpleRNN': {'batch_size': 39.0,\n",
      "               'cell_neurons': 24.0,\n",
      "               'maxlag': 22.0,\n",
      "               'optimizer': 1}}\n"
     ]
    }
   ],
   "source": [
    "pprint(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the optimal hyperparameters for our different models do not differ too much. Especially the number of neurons stays roughly the same accross architectures. However, that does **not** mean that the number of weights is similar, by construction the *LSTM* has the most weights, followed by the *GRU*, the *RNN* has the least amount of weights that need to be trained. The most remarkable difference lies in the batch size, where both *LSTM* and *GRU* needed a smaller batch size to be trained effectively, which subsequently increases the time it needs to train them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DgR2IpNCHtVn"
   },
   "source": [
    "## Evaluate on Holdout set\n",
    "\n",
    "In order to properly evaluate our models we retrain them, using the optimal parameters as well as the data which until now has been left untouched. We can easily do that from within our custom `models.Recurrent` class. We just need to find the size of our new train set as a percentage of the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N2wFC7Jq0XUm"
   },
   "outputs": [],
   "source": [
    "train_size = len(district_data) / (len(district_data) + len(district_holdout))\n",
    "data = district.loc[district.index.map(lambda x: lower <= x[0] <= end)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can train the final models and calculate the mean absolute and the mean squared error on the holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Im1peOq9tzK"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc7c14d541c4615a0c96992dfce1ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def typecast(params):\n",
    "    \"\"\"Fix types of hyperopt.Trials\"\"\"\n",
    "    out = {}\n",
    "    optimizers = [\"adam\", \"sgd\"]\n",
    "    for key, val in params.items():\n",
    "        if key == \"optimizer\":\n",
    "            # val is 0 or 1, transform into string\n",
    "            out[key] = optimizers[val]\n",
    "        else:\n",
    "            try:\n",
    "                # take a float, turn it into an int\n",
    "                out[key] = int(val)\n",
    "            except TypeError:\n",
    "                out[key] = val\n",
    "    return out\n",
    "\n",
    "name2cell = {\"RNN\": SimpleRNN,\n",
    "             \"LSTM\": LSTM,\n",
    "             \"GRU\": GRU}\n",
    "\n",
    "series_col = []\n",
    "cell_col = []\n",
    "loss_L1_col = []\n",
    "loss_L2_col = []\n",
    "\n",
    "for name, cell in tqdm(name2cell.items()):\n",
    "    bestparams = typecast({\"cell\": cell,\n",
    "                          **best[cell.__name__]})\n",
    "        \n",
    "    model = Recurrent(data, epochs=3, train_size=train_size,\n",
    "                      **bestparams, verbose=False)\n",
    "    model.train()\n",
    "        \n",
    "    y_pred = model.forecast(model.X_test)\n",
    "    \n",
    "    loss_L1 = mean_absolute_error(y_true=model.y_test,\n",
    "                                  y_pred=y_pred)\n",
    "    loss_L2 = mean_squared_error(y_true=model.y_test,\n",
    "                                 y_pred=y_pred)\n",
    "\n",
    "    cell_col.append(name)\n",
    "    loss_L2_col.append(loss_L2)\n",
    "    loss_L1_col.append(loss_L1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we compare the models to a naive model, that simply outputs the previous days' number of reported crimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Model: prediction equal to previous day.\n",
    "district_holdout[\"Baseline\"] = district_holdout.groupby(\"District\").shift(1)\n",
    "district_holdout.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Calculate baseline model performance.\n",
    "cell_col.insert(0, \"Baseline\")\n",
    "baseline_L1 = mean_absolute_error(y_true=district_holdout.Incidents,\n",
    "                                  y_pred=district_holdout.Baseline)\n",
    "loss_L1_col.insert(0, baseline_L1)\n",
    "baseline_L2 = mean_squared_error(y_true=district_holdout.Incidents,\n",
    "                                 y_pred=district_holdout.Baseline)\n",
    "loss_L2_col.insert(0, baseline_L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we save everything to a `pd.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ksXPv6NB0dFb"
   },
   "outputs": [],
   "source": [
    "validation = pd.DataFrame({\"Model\": cell_col,\n",
    "                           \"Validation L1 Loss\": loss_L1_col,\n",
    "                           \"Validation L2 Loss\": loss_L2_col})\n",
    "validation.to_csv(os.path.join(\"..\", \"analysis\", \"models\", \"loss.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Baseline', 6.565590858459473, 78.38884735107422], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.iloc[0, :].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Incidents</th>\n",
       "      <th>Baseline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>District</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <th>1.0</th>\n",
       "      <td>21.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-03</th>\n",
       "      <th>1.0</th>\n",
       "      <td>44.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-04</th>\n",
       "      <th>1.0</th>\n",
       "      <td>31.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-05</th>\n",
       "      <th>1.0</th>\n",
       "      <td>43.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-06</th>\n",
       "      <th>1.0</th>\n",
       "      <td>25.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-07</th>\n",
       "      <th>1.0</th>\n",
       "      <td>32.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-08</th>\n",
       "      <th>1.0</th>\n",
       "      <td>19.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-09</th>\n",
       "      <th>1.0</th>\n",
       "      <td>25.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-10</th>\n",
       "      <th>1.0</th>\n",
       "      <td>38.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-11</th>\n",
       "      <th>1.0</th>\n",
       "      <td>33.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-12</th>\n",
       "      <th>1.0</th>\n",
       "      <td>32.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-13</th>\n",
       "      <th>1.0</th>\n",
       "      <td>34.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-14</th>\n",
       "      <th>1.0</th>\n",
       "      <td>28.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-15</th>\n",
       "      <th>1.0</th>\n",
       "      <td>23.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-16</th>\n",
       "      <th>1.0</th>\n",
       "      <td>40.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-17</th>\n",
       "      <th>1.0</th>\n",
       "      <td>37.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-18</th>\n",
       "      <th>1.0</th>\n",
       "      <td>26.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-19</th>\n",
       "      <th>1.0</th>\n",
       "      <td>37.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-20</th>\n",
       "      <th>1.0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-21</th>\n",
       "      <th>1.0</th>\n",
       "      <td>40.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-22</th>\n",
       "      <th>1.0</th>\n",
       "      <td>28.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-23</th>\n",
       "      <th>1.0</th>\n",
       "      <td>40.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-24</th>\n",
       "      <th>1.0</th>\n",
       "      <td>45.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-25</th>\n",
       "      <th>1.0</th>\n",
       "      <td>34.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-26</th>\n",
       "      <th>1.0</th>\n",
       "      <td>42.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-27</th>\n",
       "      <th>1.0</th>\n",
       "      <td>53.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-28</th>\n",
       "      <th>1.0</th>\n",
       "      <td>33.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-29</th>\n",
       "      <th>1.0</th>\n",
       "      <td>27.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-30</th>\n",
       "      <th>1.0</th>\n",
       "      <td>45.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-31</th>\n",
       "      <th>1.0</th>\n",
       "      <td>36.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-02</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-03</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-04</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-05</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-06</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-07</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-08</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-09</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-10</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-11</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-12</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-13</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-14</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-15</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-17</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-18</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-19</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-20</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-21</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-22</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-23</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-24</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-25</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-26</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-27</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-28</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-30</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8736 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Incidents  Baseline\n",
       "Date       District                     \n",
       "2017-01-02 1.0            21.0      32.0\n",
       "2017-01-03 1.0            44.0      21.0\n",
       "2017-01-04 1.0            31.0      44.0\n",
       "2017-01-05 1.0            43.0      31.0\n",
       "2017-01-06 1.0            25.0      43.0\n",
       "2017-01-07 1.0            32.0      25.0\n",
       "2017-01-08 1.0            19.0      32.0\n",
       "2017-01-09 1.0            25.0      19.0\n",
       "2017-01-10 1.0            38.0      25.0\n",
       "2017-01-11 1.0            33.0      38.0\n",
       "2017-01-12 1.0            32.0      33.0\n",
       "2017-01-13 1.0            34.0      32.0\n",
       "2017-01-14 1.0            28.0      34.0\n",
       "2017-01-15 1.0            23.0      28.0\n",
       "2017-01-16 1.0            40.0      23.0\n",
       "2017-01-17 1.0            37.0      40.0\n",
       "2017-01-18 1.0            26.0      37.0\n",
       "2017-01-19 1.0            37.0      26.0\n",
       "2017-01-20 1.0            41.0      37.0\n",
       "2017-01-21 1.0            40.0      41.0\n",
       "2017-01-22 1.0            28.0      40.0\n",
       "2017-01-23 1.0            40.0      28.0\n",
       "2017-01-24 1.0            45.0      40.0\n",
       "2017-01-25 1.0            34.0      45.0\n",
       "2017-01-26 1.0            42.0      34.0\n",
       "2017-01-27 1.0            53.0      42.0\n",
       "2017-01-28 1.0            33.0      53.0\n",
       "2017-01-29 1.0            27.0      33.0\n",
       "2017-01-30 1.0            45.0      27.0\n",
       "2017-01-31 1.0            36.0      45.0\n",
       "...                        ...       ...\n",
       "2017-12-02 31.0            0.0       0.0\n",
       "2017-12-03 31.0            0.0       0.0\n",
       "2017-12-04 31.0            0.0       0.0\n",
       "2017-12-05 31.0            0.0       0.0\n",
       "2017-12-06 31.0            0.0       0.0\n",
       "2017-12-07 31.0            0.0       0.0\n",
       "2017-12-08 31.0            0.0       0.0\n",
       "2017-12-09 31.0            0.0       0.0\n",
       "2017-12-10 31.0            0.0       0.0\n",
       "2017-12-11 31.0            0.0       0.0\n",
       "2017-12-12 31.0            0.0       0.0\n",
       "2017-12-13 31.0            0.0       0.0\n",
       "2017-12-14 31.0            0.0       0.0\n",
       "2017-12-15 31.0            0.0       0.0\n",
       "2017-12-16 31.0            0.0       0.0\n",
       "2017-12-17 31.0            0.0       0.0\n",
       "2017-12-18 31.0            0.0       0.0\n",
       "2017-12-19 31.0            0.0       0.0\n",
       "2017-12-20 31.0            0.0       0.0\n",
       "2017-12-21 31.0            0.0       0.0\n",
       "2017-12-22 31.0            0.0       0.0\n",
       "2017-12-23 31.0            0.0       0.0\n",
       "2017-12-24 31.0            0.0       0.0\n",
       "2017-12-25 31.0            0.0       0.0\n",
       "2017-12-26 31.0            0.0       0.0\n",
       "2017-12-27 31.0            0.0       0.0\n",
       "2017-12-28 31.0            0.0       0.0\n",
       "2017-12-29 31.0            0.0       0.0\n",
       "2017-12-30 31.0            0.0       0.0\n",
       "2017-12-31 31.0            0.0       0.0\n",
       "\n",
       "[8736 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "district_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siskos/INFOSYS/infosys/lib/python3.6/site-packages/pandas/core/indexing.py:630: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "district_holdout.loc[:, \"Baseline\"] = district_holdout.groupby(\"District\").shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Incidents</th>\n",
       "      <th>Baseline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>District</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-01</th>\n",
       "      <th>1.0</th>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <th>1.0</th>\n",
       "      <td>21.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-03</th>\n",
       "      <th>1.0</th>\n",
       "      <td>44.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-04</th>\n",
       "      <th>1.0</th>\n",
       "      <td>31.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-05</th>\n",
       "      <th>1.0</th>\n",
       "      <td>43.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-06</th>\n",
       "      <th>1.0</th>\n",
       "      <td>25.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-07</th>\n",
       "      <th>1.0</th>\n",
       "      <td>32.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-08</th>\n",
       "      <th>1.0</th>\n",
       "      <td>19.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-09</th>\n",
       "      <th>1.0</th>\n",
       "      <td>25.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-10</th>\n",
       "      <th>1.0</th>\n",
       "      <td>38.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-11</th>\n",
       "      <th>1.0</th>\n",
       "      <td>33.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-12</th>\n",
       "      <th>1.0</th>\n",
       "      <td>32.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-13</th>\n",
       "      <th>1.0</th>\n",
       "      <td>34.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-14</th>\n",
       "      <th>1.0</th>\n",
       "      <td>28.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-15</th>\n",
       "      <th>1.0</th>\n",
       "      <td>23.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-16</th>\n",
       "      <th>1.0</th>\n",
       "      <td>40.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-17</th>\n",
       "      <th>1.0</th>\n",
       "      <td>37.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-18</th>\n",
       "      <th>1.0</th>\n",
       "      <td>26.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-19</th>\n",
       "      <th>1.0</th>\n",
       "      <td>37.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-20</th>\n",
       "      <th>1.0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-21</th>\n",
       "      <th>1.0</th>\n",
       "      <td>40.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-22</th>\n",
       "      <th>1.0</th>\n",
       "      <td>28.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-23</th>\n",
       "      <th>1.0</th>\n",
       "      <td>40.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-24</th>\n",
       "      <th>1.0</th>\n",
       "      <td>45.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-25</th>\n",
       "      <th>1.0</th>\n",
       "      <td>34.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-26</th>\n",
       "      <th>1.0</th>\n",
       "      <td>42.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-27</th>\n",
       "      <th>1.0</th>\n",
       "      <td>53.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-28</th>\n",
       "      <th>1.0</th>\n",
       "      <td>33.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-29</th>\n",
       "      <th>1.0</th>\n",
       "      <td>27.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-30</th>\n",
       "      <th>1.0</th>\n",
       "      <td>45.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-02</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-03</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-04</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-05</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-06</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-07</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-08</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-09</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-10</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-11</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-12</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-13</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-14</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-15</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-16</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-17</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-18</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-19</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-20</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-21</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-22</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-23</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-24</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-25</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-26</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-27</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-28</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-30</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <th>31.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8760 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Incidents  Baseline\n",
       "Date       District                     \n",
       "2017-01-01 1.0            32.0       NaN\n",
       "2017-01-02 1.0            21.0      32.0\n",
       "2017-01-03 1.0            44.0      21.0\n",
       "2017-01-04 1.0            31.0      44.0\n",
       "2017-01-05 1.0            43.0      31.0\n",
       "2017-01-06 1.0            25.0      43.0\n",
       "2017-01-07 1.0            32.0      25.0\n",
       "2017-01-08 1.0            19.0      32.0\n",
       "2017-01-09 1.0            25.0      19.0\n",
       "2017-01-10 1.0            38.0      25.0\n",
       "2017-01-11 1.0            33.0      38.0\n",
       "2017-01-12 1.0            32.0      33.0\n",
       "2017-01-13 1.0            34.0      32.0\n",
       "2017-01-14 1.0            28.0      34.0\n",
       "2017-01-15 1.0            23.0      28.0\n",
       "2017-01-16 1.0            40.0      23.0\n",
       "2017-01-17 1.0            37.0      40.0\n",
       "2017-01-18 1.0            26.0      37.0\n",
       "2017-01-19 1.0            37.0      26.0\n",
       "2017-01-20 1.0            41.0      37.0\n",
       "2017-01-21 1.0            40.0      41.0\n",
       "2017-01-22 1.0            28.0      40.0\n",
       "2017-01-23 1.0            40.0      28.0\n",
       "2017-01-24 1.0            45.0      40.0\n",
       "2017-01-25 1.0            34.0      45.0\n",
       "2017-01-26 1.0            42.0      34.0\n",
       "2017-01-27 1.0            53.0      42.0\n",
       "2017-01-28 1.0            33.0      53.0\n",
       "2017-01-29 1.0            27.0      33.0\n",
       "2017-01-30 1.0            45.0      27.0\n",
       "...                        ...       ...\n",
       "2017-12-02 31.0            0.0       0.0\n",
       "2017-12-03 31.0            0.0       0.0\n",
       "2017-12-04 31.0            0.0       0.0\n",
       "2017-12-05 31.0            0.0       0.0\n",
       "2017-12-06 31.0            0.0       0.0\n",
       "2017-12-07 31.0            0.0       0.0\n",
       "2017-12-08 31.0            0.0       0.0\n",
       "2017-12-09 31.0            0.0       0.0\n",
       "2017-12-10 31.0            0.0       0.0\n",
       "2017-12-11 31.0            0.0       0.0\n",
       "2017-12-12 31.0            0.0       0.0\n",
       "2017-12-13 31.0            0.0       0.0\n",
       "2017-12-14 31.0            0.0       0.0\n",
       "2017-12-15 31.0            0.0       0.0\n",
       "2017-12-16 31.0            0.0       0.0\n",
       "2017-12-17 31.0            0.0       0.0\n",
       "2017-12-18 31.0            0.0       0.0\n",
       "2017-12-19 31.0            0.0       0.0\n",
       "2017-12-20 31.0            0.0       0.0\n",
       "2017-12-21 31.0            0.0       0.0\n",
       "2017-12-22 31.0            0.0       0.0\n",
       "2017-12-23 31.0            0.0       0.0\n",
       "2017-12-24 31.0            0.0       0.0\n",
       "2017-12-25 31.0            0.0       0.0\n",
       "2017-12-26 31.0            0.0       0.0\n",
       "2017-12-27 31.0            0.0       0.0\n",
       "2017-12-28 31.0            0.0       0.0\n",
       "2017-12-29 31.0            0.0       0.0\n",
       "2017-12-30 31.0            0.0       0.0\n",
       "2017-12-31 31.0            0.0       0.0\n",
       "\n",
       "[8760 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "district_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "parameter-tuning.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "infosys",
   "language": "python",
   "name": "infosys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
